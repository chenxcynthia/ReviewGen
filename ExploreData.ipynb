{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4036e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T04:22:06.802939Z",
     "start_time": "2023-03-08T04:21:57.612043Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "307aa482",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T05:26:27.255471Z",
     "start_time": "2023-03-08T05:26:09.626412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (2.10.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from datasets) (8.0.0)\n",
      "Requirement already satisfied: aiohttp in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from datasets) (3.7.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from datasets) (1.22.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from datasets) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from datasets) (2023.3.0)\n",
      "Requirement already satisfied: responses<0.19 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pandas in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from datasets) (1.2.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: packaging in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from datasets) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: xxhash in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from aiohttp->datasets) (3.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from aiohttp->datasets) (4.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: filelock in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (0.1.97)\n",
      "Requirement already satisfied: rouge_score in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from rouge_score) (0.15.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from rouge_score) (1.15.0)\n",
      "Requirement already satisfied: numpy in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from rouge_score) (1.22.1)\n",
      "Requirement already satisfied: nltk in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from rouge_score) (3.6.1)\n",
      "Requirement already satisfied: tqdm in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from nltk->rouge_score) (4.65.0)\n",
      "Requirement already satisfied: regex in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from nltk->rouge_score) (2021.4.4)\n",
      "Requirement already satisfied: click in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from nltk->rouge_score) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from nltk->rouge_score) (1.0.1)\n",
      "Requirement already satisfied: wandb in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (0.10.18)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from wandb) (3.20.1)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from wandb) (2.8.1)\n",
      "Requirement already satisfied: pathtools in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from wandb) (1.0.1)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from wandb) (3.1.13)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from wandb) (0.19.5)\n",
      "Requirement already satisfied: six>=1.13.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from wandb) (1.15.0)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from wandb) (5.0.1)\n",
      "Requirement already satisfied: Click>=7.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: PyYAML in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from wandb) (5.9.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-score in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (0.3.13)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from bert-score) (4.65.0)\n",
      "Requirement already satisfied: requests in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from bert-score) (2.25.1)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from bert-score) (4.26.1)\n",
      "Requirement already satisfied: numpy in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from bert-score) (1.22.1)\n",
      "Requirement already satisfied: torch>=1.0.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from bert-score) (1.12.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from bert-score) (20.9)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from bert-score) (1.2.4)\n",
      "Requirement already satisfied: matplotlib in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from bert-score) (3.3.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from packaging>=20.9->bert-score) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas>=1.0.1->bert-score) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from torch>=1.0.0->bert-score) (4.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (0.12.1)\n",
      "Requirement already satisfied: filelock in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (3.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (5.4.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from transformers>=3.0.0->bert-score) (2021.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from matplotlib->bert-score) (8.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from matplotlib->bert-score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from matplotlib->bert-score) (0.10.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from requests->bert-score) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from requests->bert-score) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from requests->bert-score) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from requests->bert-score) (2022.12.7)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 3.3 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from evaluate) (4.65.0)\n",
      "Requirement already satisfied: dill in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from evaluate) (1.22.1)\n",
      "Requirement already satisfied: xxhash in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from evaluate) (2.10.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from evaluate) (2023.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from evaluate) (0.12.1)\n",
      "Requirement already satisfied: packaging in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from evaluate) (20.9)\n",
      "Requirement already satisfied: responses<0.19 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from evaluate) (2.25.1)\n",
      "Requirement already satisfied: pandas in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from evaluate) (1.2.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (5.4.1)\n",
      "Requirement already satisfied: aiohttp in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.7.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (8.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.1.0)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (3.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (20.3.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (3.0.1)\n",
      "Requirement already satisfied: filelock in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from packaging->evaluate) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from pandas->evaluate) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from pandas->evaluate) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.15.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets\n",
    "! pip install sentencepiece\n",
    "! pip install rouge_score\n",
    "! pip install wandb\n",
    "! pip install bert-score\n",
    "! pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b94c729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T04:22:37.617855Z",
     "start_time": "2023-03-08T04:22:25.831778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (4.3.2)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from transformers) (3.0.12)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-macosx_10_11_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from transformers) (1.22.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: requests in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages (from requests->transformers) (1.26.4)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.1\n",
      "    Uninstalling tokenizers-0.10.1:\n",
      "      Successfully uninstalled tokenizers-0.10.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.3.2\n",
      "    Uninstalling transformers-4.3.2:\n",
      "      Successfully uninstalled transformers-4.3.2\n",
      "Successfully installed tokenizers-0.13.2 transformers-4.26.1\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e8df6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T04:22:41.201745Z",
     "start_time": "2023-03-08T04:22:41.199061Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bart Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d544aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T04:22:41.503461Z",
     "start_time": "2023-03-08T04:22:41.500210Z"
    }
   },
   "outputs": [],
   "source": [
    "sublibraries = [f'ICLR{year}' for year in range(2017, 2021)] + [f'NIPS{year}' for year in range(2017, 2021)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c7f831",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34174567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a02a70f",
   "metadata": {},
   "source": [
    "# DataSet Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d9b322",
   "metadata": {},
   "source": [
    "## Load DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e7d163",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-08T06:33:25.681Z"
    }
   },
   "outputs": [],
   "source": [
    "workdir = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e062c46a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T05:54:54.444745Z",
     "start_time": "2023-03-08T05:54:54.165369Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset json (/Users/erichansen/.cache/huggingface/datasets/json/default-31803ec862b813f4/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad289bf629a480a8ffa7ea064531bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, DatasetDict, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_json(workdir + \"paper_data.json\")\n",
    "\n",
    "# train_df, test_df = train_test_split(df, test_size=0.25, random_state=100)\n",
    "\n",
    "# paper_reviews_dataset = DatasetDict({\n",
    "#     \"train\": train_df,\n",
    "#     \"test\": test_df\n",
    "# })\n",
    "\n",
    "paper_reviews_dataset = load_dataset(\"json\", data_files=\"/Users/erichansen/Downloads/paper_data.json\")\n",
    "paper_reviews_dataset = paper_reviews_dataset['train'].train_test_split(test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7d94fb72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T05:54:57.323983Z",
     "start_time": "2023-03-08T05:54:57.284115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>decision</th>\n",
       "      <th>conference</th>\n",
       "      <th>text</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wider and Deeper, Cheaper and Faster: Tensoriz...</td>\n",
       "      <td>Accept</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>We consider the time-series prediction task of...</td>\n",
       "      <td>[The paper explores a new way to share paramet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Concentration of Multilinear Functions of the ...</td>\n",
       "      <td>Accept</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>Statistical Physics, the Ising model has found...</td>\n",
       "      <td>[This paper discusses concentration of bilinea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deep Subspace Clustering Networks</td>\n",
       "      <td>Accept</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>Subspace clustering has become an important pr...</td>\n",
       "      <td>[\\nThe paper propose to enforce self-expressiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Attentional Pooling for Action Recognition</td>\n",
       "      <td>Accept</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>Interestingly, even video based action recogni...</td>\n",
       "      <td>[This paper presents an attention-based model ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title decision conference  \\\n",
       "0  Wider and Deeper, Cheaper and Faster: Tensoriz...   Accept       NIPS   \n",
       "1  Concentration of Multilinear Functions of the ...   Accept       NIPS   \n",
       "2                  Deep Subspace Clustering Networks   Accept       NIPS   \n",
       "3         Attentional Pooling for Action Recognition   Accept       NIPS   \n",
       "\n",
       "                                                text  \\\n",
       "0  We consider the time-series prediction task of...   \n",
       "1  Statistical Physics, the Ising model has found...   \n",
       "2  Subspace clustering has become an important pr...   \n",
       "3  Interestingly, even video based action recogni...   \n",
       "\n",
       "                                             reviews  \n",
       "0  [The paper explores a new way to share paramet...  \n",
       "1  [This paper discusses concentration of bilinea...  \n",
       "2  [\\nThe paper propose to enforce self-expressiv...  \n",
       "3  [This paper presents an attention-based model ...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4cddda",
   "metadata": {},
   "source": [
    "## Tokenize DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dc060eb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T05:59:55.391279Z",
     "start_time": "2023-03-08T05:59:55.386871Z"
    }
   },
   "outputs": [],
   "source": [
    "max_paper_extract_length = 512\n",
    "max_review_length = 512\n",
    "pre_trained_model_checkpoint = \"facebook/bart-large-cnn\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9c8395dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T05:59:56.288707Z",
     "start_time": "2023-03-08T05:59:55.780946Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large-cnn\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /Users/erichansen/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/vocab.json\n",
      "loading file merges.txt from cache at /Users/erichansen/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/merges.txt\n",
      "loading file tokenizer.json from cache at /Users/erichansen/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large-cnn\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "67c1bdcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T05:59:56.653289Z",
     "start_time": "2023-03-08T05:59:56.648648Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(paper_json):\n",
    "    print('reached')\n",
    "    model_inputs = tokenizer(\n",
    "        paper_json[\"text\"],\n",
    "        max_length=max_paper_extract_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "    paper_json['review'] = paper_json['reviews'][0]\n",
    "    labels = tokenizer(\n",
    "        paper_json[\"review\"], max_length=max_review_length, truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    print(len(model_inputs['labels']))\n",
    "    print(len(paper_json['text']))\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac9efc1",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed669e0",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Bart Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3a9e3765",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:00:07.705335Z",
     "start_time": "2023-03-08T06:00:02.021902Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/config.json\n",
      "Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/erichansen/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"min_length\": 56,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"min_length\": 56,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "model = BartForConditionalGeneration.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f8a749",
   "metadata": {},
   "source": [
    "## Fine-Tune Bart Model using Tokenized, Extracted Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af2dd1",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5f9f4a31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:19:59.960370Z",
     "start_time": "2023-03-08T06:19:55.553971Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
      "loading file merges.txt from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b366afaf8748a6ba2eea74c517bfc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73bf6797ffd45d4b09a8e80f3306a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.15 seconds, 12.92 sentences/sec\n",
      "(tensor([0.9222, 0.8905]), tensor([0.9714, 0.8988]), tensor([0.9462, 0.8946]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.48571428571428577,\n",
       " 'rouge2': 0.2,\n",
       " 'rougeL': 0.48571428571428577,\n",
       " 'rougeLsum': 0.48571428571428577}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "from evaluate import load\n",
    "import bert_score\n",
    "\n",
    "# bertscore = load(\"bertscore\")\n",
    "# predictions = [\"hello there\", \"general kenobi\"]\n",
    "# references = [\"hello there\", \"general kenobi\"]\n",
    "\n",
    "predictions = [\"hello there young padawan\", \"general obi wan \"]\n",
    "references = [\"hello there youngster\", \"general kenobi\"]\n",
    "\n",
    "results = bert_score.score(predictions, references, lang='en', verbose=True)\n",
    "print(results)\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\")\n",
    "scores = rouge_score.compute(\n",
    "    predictions=predictions, references=references\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b004fef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:32:00.924364Z",
     "start_time": "2023-03-08T06:32:00.916493Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "import bert_score\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    rouge_results = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Compute BERT Score, rescaling with baseline as specified in the Ti\n",
    "    # https://github.com/Tiiiger/bert_score/blob/master/example/Demo.ipynb\n",
    "    # https://arxiv.org/pdf/1904.09675.pdf\n",
    "    bert_scores = bert_score.score(decoded_preds, decoded_labels, lang='en', \n",
    "                                   verbose=False, rescale_with_baseline=True)\n",
    "    bs_prec, bs_recall, bs_f1 = bert_scores[0].mean(), bert_scores[1].mean(), bert_scores[2].mean()\n",
    "\n",
    "#     bert_score_results = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang='en')\n",
    "    # Extract the median scores\n",
    "    results = {k: round(v, 4) for k, v in rouge_results.items()}\n",
    "    \n",
    "    # Only report F1 Score, similar to the Yuan et. al \n",
    "    results.update({'BertScore': bs_f1})\n",
    "#     result.update({})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b005e6",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "629fd8ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:00:08.311874Z",
     "start_time": "2023-03-08T06:00:08.292652Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "batch_size = 1\n",
    "num_train_epochs = 8\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_dataset[\"train\"]) // batch_size \n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-review-advisor\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "#     push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ade0e",
   "metadata": {},
   "source": [
    "### Data Collator\n",
    "Necessary for HuggingFace Batching of Feature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5cbfdc4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:00:10.827699Z",
     "start_time": "2023-03-08T06:00:10.824785Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5cc1e1",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a46a46",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-08T06:32:02.813Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: conference, decision, review, title, reviews, text. If conference, decision, review, title, reviews, text are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 24\n",
      "  Number of trainable parameters = 406290432\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/24 01:21 < 02:22, 0.10 it/s, Epoch 3/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Bertscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.367100</td>\n",
       "      <td>6.438890</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>0.044200</td>\n",
       "      <td>0.153800</td>\n",
       "      <td>0.274700</td>\n",
       "      <td>0.010323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.359800</td>\n",
       "      <td>6.768425</td>\n",
       "      <td>0.342400</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.157600</td>\n",
       "      <td>0.315200</td>\n",
       "      <td>0.049732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: conference, decision, review, title, reviews, text. If conference, decision, review, title, reviews, text are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"min_length\": 56,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
      "loading file merges.txt from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: conference, decision, review, title, reviews, text. If conference, decision, review, title, reviews, text are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"min_length\": 56,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
      "loading file merges.txt from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: conference, decision, review, title, reviews, text. If conference, decision, review, title, reviews, text are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"min_length\": 56,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7553a",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab00c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33564052",
   "metadata": {},
   "source": [
    "# Reproduce Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da4849",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
