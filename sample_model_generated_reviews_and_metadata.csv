,id,extraction_method,extraction_text,model_name,title,abstract,review,model_review_output,model_output_length
0,6299,hybrid,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines. The problem can also be understood as constructing a function f(G, nS , q) to predict nT , where the functional form of f(·) is generally unknown and has to be learned from a training dataset consisting of samples like (nS , q, nT ). Since nT is unknown, the problem cannot be solved by conventional search algorithms such as A⇤-search [11], which seeks to find paths between the given source and target nodes. For the KBC example in Figure 1(a), having access to the current node nt = Hawaii alone is not sufficient to know that the best action is moving to nt+1 = USA. Second, to address the challenge of sparse rewards, M-Walk exploits the fact that the MDP transition model is known and deterministic.2 Specifically, it combines Monte Carlo Tree Search (MCTS) with the RNN to generate trajectories that obtain significantly more positive rewards than using the RNN policy alone. The rest of the paper is organized as follows: Section 3 develops the M-Walk agent, including the model architecture, the training and testing algorithms.3 Experimental results are presented in Section 4. To see this, we observe from Figure 1(b) that once an action at (i.e., an edge in Ent or “STOP”) is selected, the next node nt+1 and its associated Ent+1 and Nnt+1 are known. The objective is to learn a policy that maximizes the terminal rewards, i.e., correctly identifies the target node with high probability. Recall from Section 2 (e.g., (1)) that one challenge in applying RL to the graph-walking problem is that the state st nominally includes the entire history of observations. We will explain in Section 3.2 how such parameter sharing enables indirect updates to the policy ⇡✓ via Q-learning from off-policy data. To address this issue, we apply the same FCN f✓(·) to 5There could be alternative ways to compute the score, such as Score(n) = maxsT!n Q✓(sT , STOP). Inspired by this recursion, we propose using the GRU-RNN [4] to encode qt into a vector representation6: qt+1 = f✓q (qt, [hA,t, hat,t, nt+1]) with initialization q0 = f✓q (q, [0, 0, nS ]), where ✓q is the model parameter, and hat,t denotes the vector hn0,t at n0 = at. Three Glass Puzzle The Three Glass Puzzle [20] is a problem studied in math puzzles and graph theory. WN18RR [6] is created from the original WN18 [2] by removing various sources of test leakage, making the dataset more challenging. We use HITS@1,3 and mean reciprocal rank (MRR) as the evaluation metrics for WN18RR, and use mean average precision (MAP) for NELL995,7 where HITS@K computes the percentage of the desired entities being ranked among the top-K list, and MRR computes an average of the reciprocal rank of the desired entities. We compare against RL-based methods [38, 5], embedding-based models (including DistMult [39], ComplEx [32] and ConvE [6]) and recent work in logical rules (NeuralLP) [40]. For all the baseline methods, we used the implementation released by the corresponding authors with their best-reported hyperparameter settings.8 The details of the hyperparameters for M-Walk are described in Appendix B.2.2 of the supplementary material. We ran the experiments three times and report the means and standard deviations (except for PRA, TransE, and TransR on NELL995, whose results are directly quoted from [38]). We observed that the novel neural architecture of M-Walk contributes an overall 1% gain relative to MINERVA on NELL995, and it is still 1% worse than M-Walk, which uses MCTS for training and testing. Third, we analyze the performance of M-Walk under different numbers of MCTS rollout simulations and different search horizons on WN18RR dataset, with results shown in Figure 5(a). We observe that M-Walk outperforms the strong baseline ConvE by 4.6–10.9% in samples that require 2 or 3 steps, while it is nearly on par for paths of length one. Therefore, M-Walk does better at reasoning over longer paths than ConvE. To examine this effect, we show in Figure 5(c)-top the HITS@K accuracies when the ground truth is in the candidate set.9 It shows that M-Walk has very high accuracy in this case, which is significantly higher than ConvE (80% vs 39.6% in HITS@1). These observations point to an important direction for improving M-Walk in future work: increasing the chance of covering the target by the candidate set. Finally, in Table 4, we show examples of reasoning paths found by M-Walk.10 Reinforcement Learning Recently, deep reinforcement learning has achieved great success in many artificial intelligence problems [17, 24, 25]. Note that the former is constructed from the visit counts of all the edges connected to the MCTS root node; it only uses information near the root node to improve the policy. Recent approaches have demonstrated limitations of these prior approaches: they suffer from cascading errors when dealing with compositional (multi-step) relationships [10]. Hence, recent works [8, 18, 10, 15, 30] have proposed approaches for injecting multi-step paths such as random walks through sequences of triples during training, further improving performance on KBC tasks. IRN [23] and Neural LP [40] explore multi-step relations by using an RNN controller with attention over an external memory. Empirically, our proposed algorithm outperforms both DeepPath and MINERVA in the KBC benchmarks.11 We developed an RL-agent (M-Walk) that learns to walk over a graph towards a desired target node for given input query and source nodes. Furthermore, we also performed extensive experimental analysis to understand M-Walk. 11A preliminary version of M-Walk with limited experiments was reported in the workshop paper [22].",review_only,M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines.","Summary of the paper:  - This paper proposes a walk-based method for knowledge graph completion (KGC).  - For triples (s, q, o), an agent is trained so that given s and q, it traverses a path from the initial node s to the   terminal node t in a knowledge graph.  - This problem is formulated as that of Q-learning using RNN to model policy and Q functions, but the issue is the   sparseness of rewards.  - Monte Carlo tree search is used to overcome this issue.  Pros:  - The paper is clearly written.  - The approach is natural and sound.  - Very good performance compared with existing KGC methods.  - Ablation study with PG-Walk and Q-Walk is nice.  Cons:  - The work is mostly empirical, without much theoretical justification (admittedly, as in many deep learning papers).  - Not much novelty or surprise, given the success of AlphaGo.  This is a type of paper in which a successful technique   in other fields is applied to a different field.  I could not see exactly what is the novelty/creativity in terms of   how MCTS is applied for this specific problem of KGC.  - No analysis on running time is provided.   General Comments:  This paper is well written (but I would like the content of Section 4.2 to be more organized --- Page 7 consists of a single paragraph now!).  The novelty of the proposal is somewhat questionable, because reinforcement learning has already been introduced for KGC, and the application of MCTS naturally comes to mind.  Although this work is mostly empirical and little theoretical contribution is provided, it still achieves the state-of-the-art performance, and that could be a merit for its publication.  Questions:  * Related work  MCTS has been introduced in walk-based KGC by Sheng et al. in ICLR 2018 Workshop (ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search), and this work must be cited.  Please elaborate on how M-Walk differs from ReinforceWalk as much as you can tell from the extended abstract.  * Experiments:  - Dataset:  Is there any reason the comparison is done with NELL995 and WN18RR?  FB15k is also the standard dataset.  - Baselines  Why do the set of compared methods differ between datasets?  In particular, we know that TransE and TransR are somewhat weaker compared with more recent bilinear models, such as ComplEx, ANALOGY, and ConvE.  - Evaluation metrics  Also, why is the MAP score used on NELL995 whereas HITS@/MRR are used on WN18RR?  I have nothing against MAP scores, but typically HITS@/MRR are reported in literature on KGC even on the NELL-995 dataset (e.g., the MINERVA paper); using different metrics makes it difficult to validate the experimental results.  - Figure 6(a)  Is there any explanation as to why accuracy drops as the number of roll outs are increased to 256?  * Application to classification task  The method is only concerned with prediction of knowledge triple in which only one entity is missing. Can the proposed method be applied to the triple classification task where the truth of a given triple must be predicted? This is more or less straightforward in score-function-based models (provided that a suitable threshold can be determined), but I cannot see how this can be addressed with walk-based methods in general.  * Running time  Training and inference both involves roll-out, which raises concern about the running time of the algorithms.  How does M-walk fare against scoring-based methods (e.g., ComplEx) or MINERVA in terms of running time?  Please provide the time required for training and inference.  I am curious about inference time in particular. ","This paper proposes a graph-walking agent that combines a deep neural network with Monte Carlo Tree Search (MCTS) and a neural policy. The proposed agent is able to learn a policy that maximizes the terminal rewards by using MCTS and the RNN. The paper is well-written and well-motivated.   The proposed method is interesting and the experimental results are convincing. However, I have two major concerns. First, the paper is not well-suited for ICLR. The main contribution of this paper is the use of Monte Carlo tree search to train the M-Walk agent. This is an important problem in graph learning and the proposed method does not address the problem well enough. Second, the proposed algorithm is not sufficiently novel. It is not clear to me what is the difference between this paper and the previous papers on this topic.  In particular, I do not understand why the authors do not compare their method against the state-of-the-art methods on the three-glass-joint task. ",165
1,837,hybrid,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art. Since the introduction of the “AlexNet” architecture [14] in the 2012 ImageNet competition, ConvNets have dominated state-of-the-art performance across a variety of computer vision tasks, including object-detection, image segmentation, image classification, face recognition, human pose estimation and tracking. In conjunction with these advances as well as the evolution of network architectures, several design best practices have emerged [8, 21, 23, 24]. This functionality can be achieved by stacking many small filters or using large filters in the network; notably, the first choice can be implemented with fewer operations (faster, fewer parameters) and also allows inclusion of more nonlinearities. Third, dimensionality reduction (1×1 convolutions) before spatially aggregating filters (e.g.3×3) is supported by the fact that outputs of neighbouring filters are highly correlated and therefore these activations can be reduced before aggregation [23]. Fourth, spatial factorization into asymmetric filters can even further reduce computational cost and 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. Fifth, it is important to normalize the responses of each feature channel within a batch to reduce internal covariate shift [11]. To demonstrate the benefits of our proposed spatiotemporal ResNet architecture, it has been evaluated on two standard action recognition benchmarks where it greatly boosts the state-of-the-art. Yet another study compared several approaches to extending ConvNets into the temporal domain, but with rather disappointing results [13]: The architectures were not particularly sensitive to temporal modelling, with a slow fusion model performing slightly better than early and late fusion alternatives; moreover, similar levels of performance were achieved by a purely spatial network. Another research branch has investigated combining image information in network architectures across longer time periods. That approach takes advantage of the local spatial similarity in images; however, it only yields a minor increase over their baseline, which is a two-stream VGG-16 ConvNet [21] used as the input to their convolutional RNN. Batch normalization [11] and ReLU [14] are applied after each convolution; the network does not use hidden fc, dropout, or max-pooling (except immediately after the first layer). In preliminary experiments we found that direct connections between identical layers of the two streams led to an increase in validation error. We conjecture that these results are due to the large change that the signal of one network stream undergoes after injecting a fusion signal from the other stream. Therefore, we developed a more subtle alternative solution based on additive interactions, as follows. To enable learning of spatiotemporal features at all possible scales, this modification is applied before the second residual unit at each spatial resolution of the network (indicated by “skip-stream” in Table 1), as exemplified by the connection at the conv5_x layers in Fig. Based on the above observations, we developed a novel approach to temporal residual connections that builds on the ConvNet design guidelines of chaining small [21] asymmetric [10, 23] filters, noted in Sec. This allows the straightforward use of standard two-stream ConvNets that have been pre-trained on large-scale datasets e.g.to leverage the massive amounts of training data from the ImageNet challenge. From left to right, top to bottom, the first row shows the convolutional and pooling building blocks, with the filter and pooling size shown as (W ×H × T,C), denoting width, height, temporal extent and number of feature channels, resp. In the last two rows we show the output size of these metalayers as well as the receptive field on which they operate. We also apply random cropping and scale augmentations: We randomly jitter the width and height of the 224× 224 input frame by ±25% and also randomly crop it from a maximum of 25% distance from the image borders. We found that this strategy significantly reduces the training duration with the drawback that each loss does not capture all available information. For our final model, we equip the spatiotemporal ResNet with a temporal max-pooling layer after pool5 (see Table 1, temporal average pooling led to inferior results) and continue training as above with the learning rate starting from 10−4 for 2K iterations followed by 10−5. For fair comparison, we follow the evaluation procedure of the original two-stream work [20] by sampling 25 frames (and their horizontal flips). However, rather than using 10 spatial 224× 224 crops from each of the frames, we apply fully convolutional testing both spatially (smallest side rescaled to 256) and temporally (the 25 frame-chunks) by classifying the video in a single forward pass, which takes ≈250ms on a Titan X GPU. Second, we consider HMDB51 [15], which has 6766 videos that show 51 different actions and generally is considered more challenging than UCF0101 due to the even wider variations in which actions occur. For both datasets, we use the provided evaluation protocol and report mean average accuracy over three splits into training and test sets. Interestingly, research in neuroscience also suggests that the human visual cortex is equipped with connections between the dorsal and the ventral stream to distribute motion information to separate visual areas [3, 27]. We achieve this by simply averaging the L2-normalized SVM scores of the FV-encoded IDT descriptors (i.e.HOG, HOF, MBH) [29] with the L2-normalized video predictions of our ST-ResNet*, again without softmax normalization. These relatively larger performance decrements again underline that our approach is better able to capture the available dynamic information, as there is less to be gained by augmenting it with IDT. Still, there is a benefit from the hand-crafted IDT features even with our approach, which could be attributed to its explicit compensation of camera motion.",review_only,Spatiotemporal Residual Networks for Video Action Recognition,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art.","The paper presents a novel architecture that 1) combines residual networks with two-stream convolutional networks, and 2) injects connections from the motion stream to the appearance stream, to be able to capture spatio-temporal features. The paper shows experiments in both of the main action recognition datasets, achieving state-of-the-art accuracy in both. Overall I think the paper is great: good idea, careful experimentation, great results and clearly written.   Although the basic components of the architecture are pre-existing, I think that the high performance and careful experimentation and description make it a very useful contribution.   I only miss some experiments to visualize what kinds of spatio-temporal features are being learned.   Since large temporal windows are important (278-280) I would add a relevant reference: Long-term Temporal Convolutions for Action Recognition, Gul Varol, Ivan Laptev, Cordelia Schmid  Small typos:  (L 187)  ""resp.""  (L 255) ""UCF0101""  ","This paper proposes to generalize the two-stream ConvNets architecture to the spatiotemporal domain by introducing residual connections between the appearance and motion pathways of the two streams. The paper is well-written and well-motivated. The proposed method is evaluated on two standard action recognition benchmarks, where it greatly outperforms the state-of-the-art.    I have the following comments:  1. In the experiments, it is not clear to me what is the difference between the performance of the proposed method and the original ResNet architecture.  2. In Figure 1, the results are not particularly strong.  3. In Fig. 2, the authors do not compare with the state of the art.  4. In Table 1, I do not understand why the Spatio-ResNet performs so much better than the baseline.  5. The experiments are not convincing. ",139
2,22112,hybrid,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values. While a large body of the machine learning literature is built upon the assumption of having access to complete datasets, in many real-world problems only incomplete datasets are available. Single imputation methods try to replace each missing value with a plausible value that is the best fit given the value of other correlated features and knowledge extracted from the dataset (Hastie et al., 1999; Anderson, 1957). While MI offers interesting statistical insights about the reliability of analysis on incomplete data, the insight is imprecise as it is mainly concerned about the population of data samples rather than individual instances. While these methods are easy to implement and analyze, they often fail to capture the complex feature dependencies as well as structures present in many problems. For instance, Mattei & Frellsen (2018) suggested a method based on deep latent variable models and importance sampling that offers a tighter likelihood bound compared to the standard VAE bound. Also, these methods are often complicated to be applied in practical setups by practitioners. A few exceptions exist such as Bayesian models and decision trees that permit direct analysis on incomplete data (Nielsen & Jensen, 2009; Zhang et al., 2005). However, given an incomplete training dataset and without any known causal structure as a priori, learning such models is a very challenging problem with the complexity of at least NP-complete to learn the network architecture in addition to an iterative EM optimization to learn model parameters (Darwiche, 2009; Neapolitan et al., 2004). Similarly, for the second part, we are interested in obtaining a distribution over the possible target assignments and the confidence of each class rather than maximum likelihood class assignments. Instead, we use recent advances in GAN stabilization and training to improve the training process (see Section 3.4). Additionally, in our experiments, we provide supporting evidence that this simple loss function enables us to sample from the conditional distribution and prevents biased inclinations toward distribution modes. For instance, for a specific test sample at hand, if a certain missing feature is a strong indicator of the target class, we would like to observe the impact of different imputations for that feature on the final hypothesis. Input: G (trained imputer), D (dataset) Output: Fθ (trained predictor) foreach Training Epoch do foreach (xi, ki, yi) in D do z ∼ N(0, I) x̂i ← ki xi+(1−ki) G(xi,ki, z) ypredi ← Fθ(x̂i) loss← L(yi, ypredi ) Backpropagate loss Update Fθ Algorithm 2: Estimating target distributions. The first assumption is consistent with the theoretical analysis of generative adversarial networks that they can converge to the true underlying distribution (Arora et al., 2018; Liu et al., 2017). It should be noted that, while Zhang et al.(2018) suggests using a single self-attention layer in the middle of the network, we observed consistent improvements by inserting multiple self-attention layers before each residual block within the network. We explored best TTUR learning-rate settings from the set of {0.001, 0.0005, 0.0001, 0.00005}. To evaluate the proposed method we use CIFAR-10 (Krizhevsky & Hinton, 2009) as an image classification dataset as well as three non-image datasets: UCI Landsat (Dua & Graff, 2017)2, MIT-BIH arrhythmia (Moody & Mark, 2001), and Diabetes classification (Kachuee et al., 2019) 3. Also, while different encoding and representation methods are suggested in the literature to handle categorical features (Jang et al., 2016; Nazabal et al., 2018), in this paper, we take the simple approach of encoding categorical variables using one-hot representation and smoothing them by adding Gaussian noise with zero mean and variance equal to 5% of feature variances. We would like to note that while the suggested solution in this paper is readily compatible with MAR structures, in our experiments, to simplify the presentation of results and to have a fair comparison with other work that does not support the MAR assumption, we limited the scope of our experiments to MCAR. Furthermore, to simulate incomplete datasets and to make sure the same features are missing without explicitly storing masks, we use hashed feature vectors to seed random number generators used to sample missing features. More detail is provided in Appendix C. Fréchet inception distance (FID) (Heusel et al., 2017) score is used to measure the quality of missing data imputation in experiments with images5. We also considered using root means squared error (RMSE); however, we decided not to use this measure as we observed an inconsistent behavior using RMSE in our comparisons as RMSE favors methods that show less variance rather than realistic and sharp samples from the distribution. We compare our results with MisGAN (Li et al., 2019) and GAIN (Yoon et al., 2018) as the state of the art imputation algorithms based on GANs as well as basic denoising autoencoder (DAE) (Vincent et al., 2008) and multiple imputation by chained equations (MICE) (Buuren & Groothuis-Oudshoorn, 2010) as baselines. Due to scalability issues, we were only able to use MICE for the smaller non-image datasets. One possible explanation for this behavior might be the fact that GAIN has an MSE loss term acting similar to an autoencoder loss smoothing noisy missing pixels. Regarding the MIT-BIH experiemts, GI outperforms other work for missing rates more than 30% while achieving similar accuracies to GAIN for lower missing rates. As it can be seen from the plots, GI provides results closest to the ideal case of having average confidence values equal to average accuracies. In order to provide further insight into the operation of GI and how imputations can potentially influence the outcomes of predictions, we conduct experiments on a synthesized dataset. One possible explanation could be the fact that imputing missing data with a uniform structure can be done by processing local regions and does not require attending to different distant regions across the image. From Table 6 it can be inferred that as the rate of missingness increases, the benefits of the suggested predictor algorithm increase significantly.",review_only,Generative Imputation and Stochastic Prediction,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values.","This paper proposes a method to impute missing features using a generative model and train a predictive model on top of imputed dataset to improve classification results. They first train a GAN model where the generator outputs an imputed representation of the input and discriminator is trained to predict if an individual features (such as a pixel) is imputed or not. Given the generator and incomplete sample, they train a predictor using the output of the generator, imputed sample, as input. Their main contribution is using a MC averaging to compute the prediction by repetitively sampling from the noise variable, z, and generating different imputations from generator. They show that the proposed model improves upon the previous SOTA on final classification performance.

Overall the paper is clearly written. But I do feel it is a bit incremental over the GAIN approach. The overall GAN architecture is very similar to GAIN's and although stochastic prediction shows clear improvements it is a bit straightforward. However, I think the uncertainty of the imputations and its effect on the final prediction is interesting. I suggest the authors to extend this part with more detailed analysis.

There are several parts that are confusing/missing in the paper:

- In GAIN, they use a hint vector as an input to the discriminator. They show that without the hint vector, there is no unique solution (this is shown without the MSE loss). The authors do not use this vector in their approach (as in Figure 1) and it is not clear to me if it causes any instabilities or if multiple experiments yield similar results or if the stochastic prediction benefits from this.
- On what type of examples GI is more accurate than other models? Since stochastic prediction is the main difference from GAIN, is this related to the multi-modality of the noisy examples?
- Can you explain the difference between the results in Figure-7 and Table-2? Results between the two mismatch.
- I think the statement in the first paragraph in Section 4.4 that ""MSE loss term would act as a denoising loss smoothing noisy missing pixels"" could be misleading. MSE is used with mask in GAIN, hence it only applies to the observed features during training. Its effect on smoothing noisy missing pixels is not clear.


I think the paper would benefit if the authors could explain/show:
- Increasing the missing rate would also increase the possibility that the ground truth be a more multi-modal distribution. Especially in rectangular generation part where it can remove a complete object. Does stochastic averaging benefit more in this case?","This paper proposes a method for imputing missing features and estimating the distribution of target assignments given incomplete data. The method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures.

The paper is well-written and well-motivated. The proposed method is interesting and the experimental results are convincing. However, the paper is not well-suited for ICLR due to the following reasons:

1. The paper is mainly about imputing features and it is not clear whether the proposed method can be applied to other datasets. For example, in Table 1, the method is applied only to Cifar-10 and Table 2, it does not work for other datasets, e.g., UCI Landsimage dataset or Diabetes classification dataset. 

2. There is no comparison with other imputation methods in the literature. For instance, in Section 3.2, it is stated that: “The authors propose a method based on deep latent variable models and importance sampling that offers a tighter likelihood bound compared to the standard VAE bound.” This is a strong statement, but the authors do not compare with other methods in terms of performance. ",188
3,18864,hybrid,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting. Recent works have used this framework to achieve impressive feats such as learning to classify using one labeled image per class (Snell et al., 2017; Finn et al., 2017), learning unsupervised update rules that generalize to different domains (Metz et al., 2018), and accelerating training procedures that are millions of steps long (Flennerhag et al., 2018). DIMCO generalizes well to novel datasets because its learning objective encourages the model to compactly use all of its degrees of freedom, thus enabling it to effectively compare datapoints while only requiring a small number of bits per datapoint. Derive a generalization bound for meta-learning that makes the tradeoff between taskspecific and task-wise overfitting concrete. Notably, we suggest that certain previous meta-learning methods may have benefitted from implicit regularization. We present experiments in Section 5 and conclude the paper with a discussion about limitations and future directions of our approach in Section 6. This can be problematic since in many works, the task-specific learning algorithm is usually just a byproduct of whatever clever meta-learning loss was proposed. We propose a factorized discrete representation scheme which enables us to represent discrete distributions with exponentially fewer parameters compared to listing the probability of each event. We similarly perform image retrival by mapping each support image to its most likely code (10) and for each query image retrieving the support image that has highest (11). Regularizing Meta-Learners The ability to generalize to novel datasets is critical in meta-learning benchmarks, and even more so in benchmarks such as Meta-Dataset (Triantafillou et al., 2019), where a model is tested on datasets from an unseen domain. The following works have reported benefits from reducing the number of such task-specific parameters: Lee & Choi (2018) learns a subset of the full network to alter during task-specific learning, Rusu et al.(2018) explicitly represents each task with a low-dimensional latent space, and Zintgraf et al.(2018) alters only a pre-specified subset of the full network during task-specific learning. We showed through Theorem 1 that restricting inner-loop expressivity reduces the generalization gap; this provides theoretical understanding to this consensus that meta-learning models with simple task-specific learners generalize to new tasks more easily. Also related is the deterministic information bottleneck (Strouse & Schwab, 2017) which extends the information bottleneck by minimizing H(X̃) rather than I(X̃;X). These three approaches to generalization are related via the chain of inequalities I(X̃;X) ≤ H(X̃) ≤ log |X̃|, which is tight when X̃ is an efficient code. Recent deep learning methods directly learn discrete representations, by learning variational autoencoders with discrete latent variables (Rolfe, 2016; van den Oord et al., 2017; Razavi et al., 2019) or maximizing the mutual information between representation and 1 In practice, we add log probabilities instead of multiplying probabilities for numerical stability. Additionally, their method solves a minimum cost flow problem within each batch to find the locally optimal code, whereas DIMCO is able to directly compute its loss function. We randomly initialize weights for the 4-layer convnet and use pretrained weights for the Inception network This experiment empirically verifies whether mutual information I(X̃;Y ) is a reasonable metric for quality of representation. We observed similar trends when training with with previously proposed loss functions: we visualize these results in Figure 5 of the appendix due to space constraints. Results in Figure 3 show that the compact code of DIMCO takes roughly an order of magnitude less memory for similar performance to N-pair loss, and requires less query time as well. We additionally note that DIMCO is able to train using large backbones without significantly overfitting, whereas experiments reported in Mishra et al.(2017) indicate that MAML (Finn et al., 2017) overfits tremendously when using a deeper backbone. This challenging experimental setup measures how much generalizable information the model can extract from a limited set of datasets; it can be seen as the meta-learning analogue of measuring the performance of a classifier trained with a small dataset. We report the average and standard deviation of the top 5 results of a random hyperparameter search (see appendix for details). Towards Explicit Meta-Regularization In Section 4, we have suggested with analogy to Theorem 1 that the benefits of some previous meta-learning methods can be attributed to implicitly being regularized by reducing the expressivity of their task-specific learners. In future work, we would like to explore explicit meta-regularization schemes that can be applied to other problems (regression, reinforcement learning etc.) Along with showing that the traditional support/query split is not strictly necessary, we demonstrated in Section 5.4 that removing it has the benefit of enabling meta-learning in datasets having one image per class. (12) Assuming that the approximate distribution q(·) is sufficiently close to p(y|x̃), minimizing (12) can be seen as arg min xent(Y, X̃) ≈ arg minEy∼Y,x̃∼X̃ [ − log p(y|x̃) ] (13) = arg minH(Y |X̃) = arg max I(X̃;Y ), (14) where the last equality uses the fact that H(Y ) is independent of model parameters. (24) We similarly bound the error caused by estimating L with a finite number of tasks sampled from τ . Combining equations (26, 24), we have with high probability∣∣∣∣L(τ, θ)− − 1 n n∑ i=1 Î(X̃(XT i , θ);YT i) ∣∣∣∣ (27) ≤ ∣∣∣L(τ, θ)− L̂(τ, θ)∣∣∣+O( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (28) ≤O (√ dΘ n log n dΘ ) +O ( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (29) Hardware Every experiment was conducted on a single Nvidia V100 GPU with CUDA 9.2. Each experiment was performed with different fixed initial seeds; we manually fix seeds with manual_seed() for python, pytorch, and numpy. We additionally show in Figure 5 the correlation between 1-shot accuracies, Recall@1, and NMI using three previously proposed losses (triplet, npair, protonet).",review_only,Discrete InfoMax Codes for Meta-Learning,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting.","This paper presents DIMCO, a meta-learner that is trained by maximizing mutual information between a discrete data representation and class labels across tasks. DIMCO is inspired by an information theoretic lower bound on the generalisation gap for meta-learning, which the authors argue identifies overfitting in the task learner as the bottleneck. 

This work proposes to constrain a learner to output discrete codes that are learned to capture the mutual information with class labels. While the idea of using discrete codes is interesting, its presentation in the manuscript is not well motivated and at times hard to follow. This makes it challenging to evaluate the novelty, validity, and generality of the proposed approach. Meanwhile, the empirical evaluation is somewhat lacking. Thus, I do not believe this work is ready for publication in its current form.

Detailed comments

My main concern is with respect to the primary contribution of this paper, a generalisation bound on meta-learning. The bound appears to be on a multi-task loss without task adaptation, and thus the claims made with respect to the theorem seem somewhat over-reaching. I also believe the VC-dimensionality of the encoder is missing in Eq. 4? If so, this changes the interpretation since the length of the code and the expressivity of the encoder are interrelated. Further, I would welcome a deeper analysis of the theorem and its implications. The current interpretation states that the number of tasks is independent of the size of each task, hence given many tasks, using minimal representations is an effective approach to meta-generalisation. Yet minimal representations is a well-known idea and has features in several works that use mutual information as a regularizer, most notably works on the Information Bottleneck. 

Another reservation I have is the use of mutual information between encoder representations and class labels as a loss function (Eq. 1). It lacks context and a proper motivation, especially since the analysis of [1] shows that the loss function in Eq. 1 is the cross-entropy objective. The authors make a similar analysis in Appendix A and argue that Eq. 1 differs in that cross-entropy is an approximation because it adds a parametrized linear layer on top of \tilde{X}. Thus, in the absence of that layer they collapse to the same objective. As DIMCO itself directly extract class label predictions from \tilde{X}, I fail to see a difference between the loss in Eq. 1 and a cross-entropy objective. 

The main motivation behind their loss objective is that it does not require a support / query set. This does not seem to be a feature of the mutual information objective itself, but rather a choice made by the authors. I would have liked a deeper discussion of this seeing as the authors make it a central tenet of DIMCO. Prior works use a support set as a principled means of doing meta-learning: meta-training explicitly takes into account that at test time, the learner will be given a small support set from which to learn how to query points. As far as I understand, DIMCO does not take this into account during meta-training.  At meta-test time however, DIMCO does use a support set to map query points (Eqs. 10 and 11). Why should we break protocols between meta training and testing? Are there any downsides to doing so?  

Empirically, I find the CUB experiment compelling but would welcome some ablations. What are the trade-offs between p and d? Can DIMCO outperform N-pair when number of bits are unconstrained?

miniImagenet is a standard benchmark in few-shot learning, but I am unable to find a results table - could the authors please provide results on the standard setup so that the method can be compared against known baselines? Further, would the results currently presented hold in a N-way-5-shot setup?

As for the constrained version of miniImagenet that the authors propose, I am not convinced this is an interesting protocol. In general, the miniImagenet task distribution is created by N-way permutations of the classes in the meta-set (e.g. meta-training tasks are combinations of the 64 classes in the meta-training set). By keeping the number of classes constant but reducing the number of images per class, this protocol is not reducing samples per task: a task is always defined as 5/20-way-1-shot (Fig. 4). Instead, the effect should be that tasks are in (greater) violation of the task i.i.d. assumption. Thus, I question whether this setup demonstrates the trade-offs the authors present in Theorem 1 and whether the results can be interpreted in light of it. 

Finally, that both experiments are image-based raises questions as to the generality of the method. The paper could be considerably strengthened by evaluating DIMCO on a non-image task, or if not discuss the method’s limitations.  

The idea of discrete codes for few-shot classification is interesting and sufficiently novel, I am likely to increase my score if my concerns are addressed and the experimental section is strengthened.

Further questions and comments:

- I am unable to parse Eq. 11 - what does the notation \prod_i p_{\tilde{x}_i, i} mean? 
- It is unnecessarily hard to follow the proof of theorem 1. It would help the reader if you restated relevant definitions, such as Eq. 1, since the difference with Eq. 23 is very subtle. It would also be helpful to explain how the summand in Eq. 25 differs from either, and Eq. 26 could be expanded or briefly explained after the derivation. 
-  Because DIMCO is trained with backpropagation, the interpretation of Eq. 5 as d independent events seems invalid. How does it affect the method if they are not independent?
- Overloading X and Y as both random variables and mini-batch samples creates unnecessary confusion. I believe the objective in Eq. 1 is approximated, not calculated exactly? For instance, the mutual information in Eq. 8 is with respect to a mini-batch, so should it not be \hat{I}? 
- p^j_{ik} in Eq. 9 is undefined.
- Eq. 9 is interpreted as an exact entropy, however it appears to be a mini-batch approximation to the true entropy? 

References 
[1] Achille and Soatto. Emergence of Invariance and Disentanglement in Deep Representations. JMLR. 2018.","This paper studies the problem of meta-learning with task-specific learners and proposes a new generalization bound for this problem. The paper is well-written and well-motivated. 

The paper is clearly written and easy to follow. The main contribution of the paper is a generalisation bound for task specific learners. This is an interesting and novel contribution.

However, I have a few questions:

1. What is the expressivity of the task specific learner?

2. How does the bound compare to other generalization bounds in terms of the number of tasks and the amount of parameters?
	3. How do the authors compare the generalization of DIMCO with respect to MAML et al. (2017) and Zintgraf et al (2018)?

4. In the experiments, it is not clear to me what is the computational complexity of the proposed method. For example, in Table 1, how much memory is required for training the model?
",143
4,13063,hybrid,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all. But in each case, the network was designed and tuned specifically for the problem at hand. Natural language processing models have been shown to benefit from a multi-task approach a long time ago (Collobert & Weston, 2008), and recently multi-task machine translation models (MinhThang Luong, 2015) have even been shown to exhibit zero-shot learning when trained on multiple languages (Melvin Johnson, 2016). Speech recognition has also been shown to benefit from multi-task training (Seltzer & Droppo, 2013), as have some vision problems, such as facial landmark detection (Zhang Z., 2014). But no competitive multi-task multi-modal model has been proposed, so the above question remains unanswered. (3) COCO image captioning dataset (Lin et al., 2014), used for image captioning. (4) WSJ parsing dataset (Marcus et al., 1999), used for constituency parsing. While the MultiModel is only a first step and will be improved in the future, two key insights are crucial to making it work at all and are our main contributions. We design modality nets to be computationally minimal, promoting heavy feature extraction and ensuring that the majority of computation is performed within the domain-agnostic body of the model. Two design decisions were important: • The unified representation is variable-size. While a fixed-size representation is tempting and easier to implement, it creates a bottleneck and limits the performance of the model. For example, separable convolutions were introduced in the Xception architecture (Chollet, 2016) and were not applied to text or speech processing before. We find that each of these mechanisms is indeed crucial for the domain it was introduced, e.g., attention is far more important for languagerelated tasks than for image-related ones. As already said above, the encoder and decoder are constructed using 3 key computational blocks to get good performance across different problems: (1) Convolutions allow the model to detect local patterns and generalize across space. We refer the reader to (Chollet, 2016) for a complete definition; here we will denote a depthwise separable convolution with weights Wh×w corresponding to f kernels of size h × w applied to an input tensor x with stride s and dilated by a factor d (see (Yu & Koltun, 2015)) as SepConvd,s,f (W,x). The source tensor is finally passed through two different pointwise convolutions to generate the memory keys K and values V and the query keys, memory keys and memory values are used to apply the attention mechanism between the self-attended target and the source (see Figure 3). On the output side, the language modality takes the decoded output of the body and performs a learned linear mapping, followed by a Softmax, resulting in a probability distribution over the token vocabulary. The spectral modality does not perform any striding along the frequency bin dimension, preserving full resolution in the spectral domain. Convolutional architectures yielded good results on word-level neural machine translation starting from (Kalchbrenner & Blunsom, 2013) and later in (Meng et al., 2015). These early models used a standard RNN on top of the convolution to generate the output and had a bottleneck there that hurt performance, especially on longer sentences, similarly to the limitations of RNN sequence-to-sequence models without attention (Sutskever et al., 2014; Cho et al., 2014). This idea, introduced in WaveNet (van den Oord et al., 2016) and also used in MultiModel (see above) significantly improves efficiency. Depthwise separable convolutions were first studied by Sifre (Sifre & Mallat, 2013) and later they were used to get good results on large-scale image classification with Xception (Chollet, 2016). We focused our experiments so as to answer the following questions: (1) How far is the MultiModel trained on 8 tasks simultaneously from state-of-the-art results? Especially the 4 translation problems behave very similarly, so we decided to not include them all in each comparison but we focused on the more varied problems instead. The results we achieve are similar to the ones task-specific models get without heavy tuning, e.g., on English-French translation we improve on the recent Extended Neural GPU results (Kaiser & Bengio, 2016). Since we are comparing different instantiations of the same model, we report two internal metrics: the negative log-perplexity and per-token accuracy (measured on the development set). As can be seen from the results in Table 2, the joint 8-problem model performs similarly to single-model on large tasks, and better, sometimes significantly, on tasks where less data is available, such as parsing. The difference in performance is significant, and since we use both dropout and early stopping, we conjecture that it is not related to over-fitting. In fact, one could expect that removing these blocks will improve performance on ImageNet alone if they were truly useless for this task. In contrast, we see in Table 4 that these blocks either don’t affect or slightly improve performance. This leads us to conclude that mixing different computation blocks is in fact a good way to improve performance on many various tasks. We believe that this treads a path towards interesting future work on more general deep learning architectures, especially since our model shows transfer learning from tasks with a large amount of available data to ones where the data is limited.",review_only,Large Scale Multi-Domain Multi-Task Learning with MultiModel,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.","The paper presents a multi-task, multi-domain model based on deep neural networks. The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition. The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic. The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task.

The paper is well written and easy to follow.

The contributions of the paper are novel and significant. The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring. The experiments clearly show the viability of the approach and give interesting insights. This is surely an important step towards more general deep learning models. 

Comments:

* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database. Moreover, some databases could be used for different tasks, such as WSJ or ImageNet.

* The training procedure of the model is not explained in the paper. What is the cost function and what is the strategy to train on multiple tasks ? The paper should at least outline the strategy.

* The experiments are sufficient to demonstrate the viability of the approach, but the experimental setup is not clear. Specifically, there is an issue about the speech recognition part of the experiment. It is not clear what the task exactly is: continuous speech recognition, isolated word recognition ? The metrics used in Table 1 are also not clear, they should be explained in the text. Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used. Information about the detailed setup is also lacking, specifically which test and development sets are used (the WSJ corpus has several sets).

* Using raw waveforms as audio modality is very interesting, but this approach is not standard for speech recognition, some references should be provided, such as:
P. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26–30.
D. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE.
T. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals. Learning the Speech Front-end With Raw Waveform CLDNNs. Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015.

Revised Review:
The main idea of the paper is very interesting and the work presented is impressive. However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks. The experiments are interesting, except for the WSJ speech task, which is almost meaningless. Indeed, it is not clear what the network has learned given the metrics presented, as the WER on WSJ should be around 5% for speech recognition.
I thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition. A simpler speech task such as Keyword Spotting could also be investigated.
","This paper proposes a multi-task multi-modal model for multi-language translation tasks. The model consists of a convolutional layer, an attention mechanism, and a sparsely-gated layers. It is trained on ImageNet, multiple translation tasks, a speech recognition corpus, a COCO dataset, and an English parsing task.

The paper is well-written and well-motivated. The proposed model achieves good results on all the tasks, and the proposed architecture is simple and effective. The paper is clearly written and easy to follow. The experimental results are convincing. 

However, I have a few questions:

1. Why did you choose to use the spectral domain for the multi-ilingual tasks? It is not clear to me what is the advantage of using it compared to the fixed-size representation of the input tensor?

2. What is the performance of the model on large-scale image classification with Oord den Oord et al. and WaveNet? It seems to me that the main advantage of the proposed model is that it does not suffer from the limitations of RNNs. It would be interesting to see how this model performs on large scale image classification tasks.
",181
5,6299,hybrid,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines. The problem can also be understood as constructing a function f(G, nS , q) to predict nT , where the functional form of f(·) is generally unknown and has to be learned from a training dataset consisting of samples like (nS , q, nT ). Since nT is unknown, the problem cannot be solved by conventional search algorithms such as A⇤-search [11], which seeks to find paths between the given source and target nodes. For the KBC example in Figure 1(a), having access to the current node nt = Hawaii alone is not sufficient to know that the best action is moving to nt+1 = USA. Second, to address the challenge of sparse rewards, M-Walk exploits the fact that the MDP transition model is known and deterministic.2 Specifically, it combines Monte Carlo Tree Search (MCTS) with the RNN to generate trajectories that obtain significantly more positive rewards than using the RNN policy alone. The rest of the paper is organized as follows: Section 3 develops the M-Walk agent, including the model architecture, the training and testing algorithms.3 Experimental results are presented in Section 4. To see this, we observe from Figure 1(b) that once an action at (i.e., an edge in Ent or “STOP”) is selected, the next node nt+1 and its associated Ent+1 and Nnt+1 are known. The objective is to learn a policy that maximizes the terminal rewards, i.e., correctly identifies the target node with high probability. Recall from Section 2 (e.g., (1)) that one challenge in applying RL to the graph-walking problem is that the state st nominally includes the entire history of observations. We will explain in Section 3.2 how such parameter sharing enables indirect updates to the policy ⇡✓ via Q-learning from off-policy data. To address this issue, we apply the same FCN f✓(·) to 5There could be alternative ways to compute the score, such as Score(n) = maxsT!n Q✓(sT , STOP). Inspired by this recursion, we propose using the GRU-RNN [4] to encode qt into a vector representation6: qt+1 = f✓q (qt, [hA,t, hat,t, nt+1]) with initialization q0 = f✓q (q, [0, 0, nS ]), where ✓q is the model parameter, and hat,t denotes the vector hn0,t at n0 = at. Three Glass Puzzle The Three Glass Puzzle [20] is a problem studied in math puzzles and graph theory. WN18RR [6] is created from the original WN18 [2] by removing various sources of test leakage, making the dataset more challenging. We use HITS@1,3 and mean reciprocal rank (MRR) as the evaluation metrics for WN18RR, and use mean average precision (MAP) for NELL995,7 where HITS@K computes the percentage of the desired entities being ranked among the top-K list, and MRR computes an average of the reciprocal rank of the desired entities. We compare against RL-based methods [38, 5], embedding-based models (including DistMult [39], ComplEx [32] and ConvE [6]) and recent work in logical rules (NeuralLP) [40]. For all the baseline methods, we used the implementation released by the corresponding authors with their best-reported hyperparameter settings.8 The details of the hyperparameters for M-Walk are described in Appendix B.2.2 of the supplementary material. We ran the experiments three times and report the means and standard deviations (except for PRA, TransE, and TransR on NELL995, whose results are directly quoted from [38]). We observed that the novel neural architecture of M-Walk contributes an overall 1% gain relative to MINERVA on NELL995, and it is still 1% worse than M-Walk, which uses MCTS for training and testing. Third, we analyze the performance of M-Walk under different numbers of MCTS rollout simulations and different search horizons on WN18RR dataset, with results shown in Figure 5(a). We observe that M-Walk outperforms the strong baseline ConvE by 4.6–10.9% in samples that require 2 or 3 steps, while it is nearly on par for paths of length one. Therefore, M-Walk does better at reasoning over longer paths than ConvE. To examine this effect, we show in Figure 5(c)-top the HITS@K accuracies when the ground truth is in the candidate set.9 It shows that M-Walk has very high accuracy in this case, which is significantly higher than ConvE (80% vs 39.6% in HITS@1). These observations point to an important direction for improving M-Walk in future work: increasing the chance of covering the target by the candidate set. Finally, in Table 4, we show examples of reasoning paths found by M-Walk.10 Reinforcement Learning Recently, deep reinforcement learning has achieved great success in many artificial intelligence problems [17, 24, 25]. Note that the former is constructed from the visit counts of all the edges connected to the MCTS root node; it only uses information near the root node to improve the policy. Recent approaches have demonstrated limitations of these prior approaches: they suffer from cascading errors when dealing with compositional (multi-step) relationships [10]. Hence, recent works [8, 18, 10, 15, 30] have proposed approaches for injecting multi-step paths such as random walks through sequences of triples during training, further improving performance on KBC tasks. IRN [23] and Neural LP [40] explore multi-step relations by using an RNN controller with attention over an external memory. Empirically, our proposed algorithm outperforms both DeepPath and MINERVA in the KBC benchmarks.11 We developed an RL-agent (M-Walk) that learns to walk over a graph towards a desired target node for given input query and source nodes. Furthermore, we also performed extensive experimental analysis to understand M-Walk. 11A preliminary version of M-Walk with limited experiments was reported in the workshop paper [22].",ner1_review2_citation4_metadata2_1k_15_epoch_v1,M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines.","Summary of the paper:  - This paper proposes a walk-based method for knowledge graph completion (KGC).  - For triples (s, q, o), an agent is trained so that given s and q, it traverses a path from the initial node s to the   terminal node t in a knowledge graph.  - This problem is formulated as that of Q-learning using RNN to model policy and Q functions, but the issue is the   sparseness of rewards.  - Monte Carlo tree search is used to overcome this issue.  Pros:  - The paper is clearly written.  - The approach is natural and sound.  - Very good performance compared with existing KGC methods.  - Ablation study with PG-Walk and Q-Walk is nice.  Cons:  - The work is mostly empirical, without much theoretical justification (admittedly, as in many deep learning papers).  - Not much novelty or surprise, given the success of AlphaGo.  This is a type of paper in which a successful technique   in other fields is applied to a different field.  I could not see exactly what is the novelty/creativity in terms of   how MCTS is applied for this specific problem of KGC.  - No analysis on running time is provided.   General Comments:  This paper is well written (but I would like the content of Section 4.2 to be more organized --- Page 7 consists of a single paragraph now!).  The novelty of the proposal is somewhat questionable, because reinforcement learning has already been introduced for KGC, and the application of MCTS naturally comes to mind.  Although this work is mostly empirical and little theoretical contribution is provided, it still achieves the state-of-the-art performance, and that could be a merit for its publication.  Questions:  * Related work  MCTS has been introduced in walk-based KGC by Sheng et al. in ICLR 2018 Workshop (ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search), and this work must be cited.  Please elaborate on how M-Walk differs from ReinforceWalk as much as you can tell from the extended abstract.  * Experiments:  - Dataset:  Is there any reason the comparison is done with NELL995 and WN18RR?  FB15k is also the standard dataset.  - Baselines  Why do the set of compared methods differ between datasets?  In particular, we know that TransE and TransR are somewhat weaker compared with more recent bilinear models, such as ComplEx, ANALOGY, and ConvE.  - Evaluation metrics  Also, why is the MAP score used on NELL995 whereas HITS@/MRR are used on WN18RR?  I have nothing against MAP scores, but typically HITS@/MRR are reported in literature on KGC even on the NELL-995 dataset (e.g., the MINERVA paper); using different metrics makes it difficult to validate the experimental results.  - Figure 6(a)  Is there any explanation as to why accuracy drops as the number of roll outs are increased to 256?  * Application to classification task  The method is only concerned with prediction of knowledge triple in which only one entity is missing. Can the proposed method be applied to the triple classification task where the truth of a given triple must be predicted? This is more or less straightforward in score-function-based models (provided that a suitable threshold can be determined), but I cannot see how this can be addressed with walk-based methods in general.  * Running time  Training and inference both involves roll-out, which raises concern about the running time of the algorithms.  How does M-walk fare against scoring-based methods (e.g., ComplEx) or MINERVA in terms of running time?  Please provide the time required for training and inference.  I am curious about inference time in particular. "," This paper proposes a method for learning to walk over a graph with a known state transition model from sparse rewards. The proposed method is based on a combination of a deep recurrent neural network ( RNN ) and Monte Carlo Tree Tree Search ( MCTS ). The RNN encodes the state ( i.e.the state of the graph ) and a policy and Q-values, which are then combined with the neural policy to generate trajectories that obtain significantly more positive rewards than using the RNN policy alone. The objective is to learn a policy that maximizes the terminal rewards of the target node with high probability. The paper is well-written and the paper is easy to follow. I would recommend the paper to all of the authors in the literature. I think the paper would have been better if the paper had been written a bit more clearly. For example, in Section 2.2, the authors proposed to combine MCTs and MCTRNN to generate a policy-improvement step to train the agent from sparse reward. In Section 3.3, they also proposed to use the M-Walk agent to learn better policies than other RL-based methods like A⇤-search [ 11 ]. In section 3.4, they propose to use M-walk as a model for learning a policy for learning the policy from off-policy data. In the section 4.1, they show that M- walk outperforms the KBC baselines in terms of performance on several graph-walking benchmarks. However, the paper does not address the problem of how to learn the policy gradients from off policy data. For instance, it is not clear to me that the authors are using off policy to learn policies. For this reason, I would like to see a more thorough evaluation of the performance of the paper.2. I am not convinced that the paper can be improved by adding a policy gradient to the model. For the paper, I think it would be better to use a policy gradient instead of an off policy.3.4. I do not think that the proposed method would be much better than the proposed policy gradient.5. I also don't think that this is a good idea. It would be nice to see the results of the experiments on the paper in Section 4.6. I have a few questions :1 ) The paper doesn't seem to address the issue of sparse rewards, which is a problem that needs to be addressed by the authors.2 ) It is unclear to me why the authors didn't address this issue in the first place. It seems like the paper should be addressed in the second and third and fourth sections.3 ) The authors should also address the question of how the authors learned the policy gradient in Section 5.4 ) I would have liked to see some of the results on the first and second sections. It is interesting to see how well the authors addressed the problem in Section 6.1. The first section is well written, but the second section is a bit confusing. The second section seems to be a bit short on details, and the third section is not very well written. It does not seem to be clear what the main contribution of this paper is, and it is hard to understand what the authors were trying to achieve. The third section does not provide any explanation of the motivation for this paper. The authors do not mention any details.",574
6,837,hybrid,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art. Since the introduction of the “AlexNet” architecture [14] in the 2012 ImageNet competition, ConvNets have dominated state-of-the-art performance across a variety of computer vision tasks, including object-detection, image segmentation, image classification, face recognition, human pose estimation and tracking. In conjunction with these advances as well as the evolution of network architectures, several design best practices have emerged [8, 21, 23, 24]. This functionality can be achieved by stacking many small filters or using large filters in the network; notably, the first choice can be implemented with fewer operations (faster, fewer parameters) and also allows inclusion of more nonlinearities. Third, dimensionality reduction (1×1 convolutions) before spatially aggregating filters (e.g.3×3) is supported by the fact that outputs of neighbouring filters are highly correlated and therefore these activations can be reduced before aggregation [23]. Fourth, spatial factorization into asymmetric filters can even further reduce computational cost and 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. Fifth, it is important to normalize the responses of each feature channel within a batch to reduce internal covariate shift [11]. To demonstrate the benefits of our proposed spatiotemporal ResNet architecture, it has been evaluated on two standard action recognition benchmarks where it greatly boosts the state-of-the-art. Yet another study compared several approaches to extending ConvNets into the temporal domain, but with rather disappointing results [13]: The architectures were not particularly sensitive to temporal modelling, with a slow fusion model performing slightly better than early and late fusion alternatives; moreover, similar levels of performance were achieved by a purely spatial network. Another research branch has investigated combining image information in network architectures across longer time periods. That approach takes advantage of the local spatial similarity in images; however, it only yields a minor increase over their baseline, which is a two-stream VGG-16 ConvNet [21] used as the input to their convolutional RNN. Batch normalization [11] and ReLU [14] are applied after each convolution; the network does not use hidden fc, dropout, or max-pooling (except immediately after the first layer). In preliminary experiments we found that direct connections between identical layers of the two streams led to an increase in validation error. We conjecture that these results are due to the large change that the signal of one network stream undergoes after injecting a fusion signal from the other stream. Therefore, we developed a more subtle alternative solution based on additive interactions, as follows. To enable learning of spatiotemporal features at all possible scales, this modification is applied before the second residual unit at each spatial resolution of the network (indicated by “skip-stream” in Table 1), as exemplified by the connection at the conv5_x layers in Fig. Based on the above observations, we developed a novel approach to temporal residual connections that builds on the ConvNet design guidelines of chaining small [21] asymmetric [10, 23] filters, noted in Sec. This allows the straightforward use of standard two-stream ConvNets that have been pre-trained on large-scale datasets e.g.to leverage the massive amounts of training data from the ImageNet challenge. From left to right, top to bottom, the first row shows the convolutional and pooling building blocks, with the filter and pooling size shown as (W ×H × T,C), denoting width, height, temporal extent and number of feature channels, resp. In the last two rows we show the output size of these metalayers as well as the receptive field on which they operate. We also apply random cropping and scale augmentations: We randomly jitter the width and height of the 224× 224 input frame by ±25% and also randomly crop it from a maximum of 25% distance from the image borders. We found that this strategy significantly reduces the training duration with the drawback that each loss does not capture all available information. For our final model, we equip the spatiotemporal ResNet with a temporal max-pooling layer after pool5 (see Table 1, temporal average pooling led to inferior results) and continue training as above with the learning rate starting from 10−4 for 2K iterations followed by 10−5. For fair comparison, we follow the evaluation procedure of the original two-stream work [20] by sampling 25 frames (and their horizontal flips). However, rather than using 10 spatial 224× 224 crops from each of the frames, we apply fully convolutional testing both spatially (smallest side rescaled to 256) and temporally (the 25 frame-chunks) by classifying the video in a single forward pass, which takes ≈250ms on a Titan X GPU. Second, we consider HMDB51 [15], which has 6766 videos that show 51 different actions and generally is considered more challenging than UCF0101 due to the even wider variations in which actions occur. For both datasets, we use the provided evaluation protocol and report mean average accuracy over three splits into training and test sets. Interestingly, research in neuroscience also suggests that the human visual cortex is equipped with connections between the dorsal and the ventral stream to distribute motion information to separate visual areas [3, 27]. We achieve this by simply averaging the L2-normalized SVM scores of the FV-encoded IDT descriptors (i.e.HOG, HOF, MBH) [29] with the L2-normalized video predictions of our ST-ResNet*, again without softmax normalization. These relatively larger performance decrements again underline that our approach is better able to capture the available dynamic information, as there is less to be gained by augmenting it with IDT. Still, there is a benefit from the hand-crafted IDT features even with our approach, which could be attributed to its explicit compensation of camera motion.",ner1_review2_citation4_metadata2_1k_15_epoch_v1,Spatiotemporal Residual Networks for Video Action Recognition,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art.","The paper presents a novel architecture that 1) combines residual networks with two-stream convolutional networks, and 2) injects connections from the motion stream to the appearance stream, to be able to capture spatio-temporal features. The paper shows experiments in both of the main action recognition datasets, achieving state-of-the-art accuracy in both. Overall I think the paper is great: good idea, careful experimentation, great results and clearly written.   Although the basic components of the architecture are pre-existing, I think that the high performance and careful experimentation and description make it a very useful contribution.   I only miss some experiments to visualize what kinds of spatio-temporal features are being learned.   Since large temporal windows are important (278-280) I would add a relevant reference: Long-term Temporal Convolutions for Action Recognition, Gul Varol, Ivan Laptev, Cordelia Schmid  Small typos:  (L 187)  ""resp.""  (L 255) ""UCF0101""  "," This paper introduces a new method for training in the spatiotemporal domain for two-stream Convolutional Networks ( Residual Networks ) by introducing residual connections in two ways. First, the authors introduce residual connections between the appearance and motion pathways of a two- stream architecture to allow spatioteporal interaction between the two streams. Second, they use residual connections to allow the network to train in the spatial residual connections of the two stream architecture. Finally, they train the network in the temporal residual connections before the second residual unit at each spatial resolution of the network. The whole model is trained end-to-end to allow hierarchical learning of complex features. The paper is well-written and the paper is easy to follow. I would recommend the paper to all of my readers. I think it would be nice to see the results of the paper in a paper, and I would like to see how well the proposed method works in practice. It would be interesting to me to see if the proposed approach can be applied to other tasks. For example, the paper would be good to compare it to the current state-of-the-art in the context of human action recognition, but I would also like to know if it could be used in other domains. I am not sure if this is the case, but the paper seems to be well written and the authors seem to have a good idea of the potential benefits of this approach. I have a few comments about the paper. First of all, it is not clear that the paper does not only apply residual connections, but not the residual connections. This is not the case with respect to the previous work, which used residual connections as a basis for training.Second, I would have liked to see a more thoroughness in the application of residual connections.The paper is very well written, but it would have been nice to be able to show the results in a more comprehensive way. For instance, I think the paper could have been better if the authors could have shown that the results were better than the existing work. Third, I don't think that the proposed model is well written. The authors should also consider using residual connections with residual connections for other tasks, as well. The proposed model would be better to show that the model is able to learn from the current work of the authors. I also think that this is a very interesting idea. However, I am still not convinced that the authors have done a thorough analysis of the results, and it is hard to see why this is important. The main contribution of this paper is that it is a novel approach to training in a spatial residual connection. It is also interesting to see what the authors are doing in terms of the performance of the model, and how it is compared to other approaches. For the most part, this paper doesn't seem to be a very novel idea, but there is a lot of work to be done in this area.",519
7,22112,hybrid,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values. While a large body of the machine learning literature is built upon the assumption of having access to complete datasets, in many real-world problems only incomplete datasets are available. Single imputation methods try to replace each missing value with a plausible value that is the best fit given the value of other correlated features and knowledge extracted from the dataset (Hastie et al., 1999; Anderson, 1957). While MI offers interesting statistical insights about the reliability of analysis on incomplete data, the insight is imprecise as it is mainly concerned about the population of data samples rather than individual instances. While these methods are easy to implement and analyze, they often fail to capture the complex feature dependencies as well as structures present in many problems. For instance, Mattei & Frellsen (2018) suggested a method based on deep latent variable models and importance sampling that offers a tighter likelihood bound compared to the standard VAE bound. Also, these methods are often complicated to be applied in practical setups by practitioners. A few exceptions exist such as Bayesian models and decision trees that permit direct analysis on incomplete data (Nielsen & Jensen, 2009; Zhang et al., 2005). However, given an incomplete training dataset and without any known causal structure as a priori, learning such models is a very challenging problem with the complexity of at least NP-complete to learn the network architecture in addition to an iterative EM optimization to learn model parameters (Darwiche, 2009; Neapolitan et al., 2004). Similarly, for the second part, we are interested in obtaining a distribution over the possible target assignments and the confidence of each class rather than maximum likelihood class assignments. Instead, we use recent advances in GAN stabilization and training to improve the training process (see Section 3.4). Additionally, in our experiments, we provide supporting evidence that this simple loss function enables us to sample from the conditional distribution and prevents biased inclinations toward distribution modes. For instance, for a specific test sample at hand, if a certain missing feature is a strong indicator of the target class, we would like to observe the impact of different imputations for that feature on the final hypothesis. Input: G (trained imputer), D (dataset) Output: Fθ (trained predictor) foreach Training Epoch do foreach (xi, ki, yi) in D do z ∼ N(0, I) x̂i ← ki xi+(1−ki) G(xi,ki, z) ypredi ← Fθ(x̂i) loss← L(yi, ypredi ) Backpropagate loss Update Fθ Algorithm 2: Estimating target distributions. The first assumption is consistent with the theoretical analysis of generative adversarial networks that they can converge to the true underlying distribution (Arora et al., 2018; Liu et al., 2017). It should be noted that, while Zhang et al.(2018) suggests using a single self-attention layer in the middle of the network, we observed consistent improvements by inserting multiple self-attention layers before each residual block within the network. We explored best TTUR learning-rate settings from the set of {0.001, 0.0005, 0.0001, 0.00005}. To evaluate the proposed method we use CIFAR-10 (Krizhevsky & Hinton, 2009) as an image classification dataset as well as three non-image datasets: UCI Landsat (Dua & Graff, 2017)2, MIT-BIH arrhythmia (Moody & Mark, 2001), and Diabetes classification (Kachuee et al., 2019) 3. Also, while different encoding and representation methods are suggested in the literature to handle categorical features (Jang et al., 2016; Nazabal et al., 2018), in this paper, we take the simple approach of encoding categorical variables using one-hot representation and smoothing them by adding Gaussian noise with zero mean and variance equal to 5% of feature variances. We would like to note that while the suggested solution in this paper is readily compatible with MAR structures, in our experiments, to simplify the presentation of results and to have a fair comparison with other work that does not support the MAR assumption, we limited the scope of our experiments to MCAR. Furthermore, to simulate incomplete datasets and to make sure the same features are missing without explicitly storing masks, we use hashed feature vectors to seed random number generators used to sample missing features. More detail is provided in Appendix C. Fréchet inception distance (FID) (Heusel et al., 2017) score is used to measure the quality of missing data imputation in experiments with images5. We also considered using root means squared error (RMSE); however, we decided not to use this measure as we observed an inconsistent behavior using RMSE in our comparisons as RMSE favors methods that show less variance rather than realistic and sharp samples from the distribution. We compare our results with MisGAN (Li et al., 2019) and GAIN (Yoon et al., 2018) as the state of the art imputation algorithms based on GANs as well as basic denoising autoencoder (DAE) (Vincent et al., 2008) and multiple imputation by chained equations (MICE) (Buuren & Groothuis-Oudshoorn, 2010) as baselines. Due to scalability issues, we were only able to use MICE for the smaller non-image datasets. One possible explanation for this behavior might be the fact that GAIN has an MSE loss term acting similar to an autoencoder loss smoothing noisy missing pixels. Regarding the MIT-BIH experiemts, GI outperforms other work for missing rates more than 30% while achieving similar accuracies to GAIN for lower missing rates. As it can be seen from the plots, GI provides results closest to the ideal case of having average confidence values equal to average accuracies. In order to provide further insight into the operation of GI and how imputations can potentially influence the outcomes of predictions, we conduct experiments on a synthesized dataset. One possible explanation could be the fact that imputing missing data with a uniform structure can be done by processing local regions and does not require attending to different distant regions across the image. From Table 6 it can be inferred that as the rate of missingness increases, the benefits of the suggested predictor algorithm increase significantly.",ner1_review2_citation4_metadata2_1k_15_epoch_v1,Generative Imputation and Stochastic Prediction,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values.","This paper proposes a method to impute missing features using a generative model and train a predictive model on top of imputed dataset to improve classification results. They first train a GAN model where the generator outputs an imputed representation of the input and discriminator is trained to predict if an individual features (such as a pixel) is imputed or not. Given the generator and incomplete sample, they train a predictor using the output of the generator, imputed sample, as input. Their main contribution is using a MC averaging to compute the prediction by repetitively sampling from the noise variable, z, and generating different imputations from generator. They show that the proposed model improves upon the previous SOTA on final classification performance.

Overall the paper is clearly written. But I do feel it is a bit incremental over the GAIN approach. The overall GAN architecture is very similar to GAIN's and although stochastic prediction shows clear improvements it is a bit straightforward. However, I think the uncertainty of the imputations and its effect on the final prediction is interesting. I suggest the authors to extend this part with more detailed analysis.

There are several parts that are confusing/missing in the paper:

- In GAIN, they use a hint vector as an input to the discriminator. They show that without the hint vector, there is no unique solution (this is shown without the MSE loss). The authors do not use this vector in their approach (as in Figure 1) and it is not clear to me if it causes any instabilities or if multiple experiments yield similar results or if the stochastic prediction benefits from this.
- On what type of examples GI is more accurate than other models? Since stochastic prediction is the main difference from GAIN, is this related to the multi-modality of the noisy examples?
- Can you explain the difference between the results in Figure-7 and Table-2? Results between the two mismatch.
- I think the statement in the first paragraph in Section 4.4 that ""MSE loss term would act as a denoising loss smoothing noisy missing pixels"" could be misleading. MSE is used with mask in GAIN, hence it only applies to the observed features during training. Its effect on smoothing noisy missing pixels is not clear.


I think the paper would benefit if the authors could explain/show:
- Increasing the missing rate would also increase the possibility that the ground truth be a more multi-modal distribution. Especially in rectangular generation part where it can remove a complete object. Does stochastic averaging benefit more in this case?"," This paper proposes a simple and effective method for imputing missing data imputations and estimating the distribution of target assignments given incomplete data. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets under different missingness rates and structures.The paper is well-written and the experimental results show the effectiveness of the proposed method in generating imputations that generate imputations. The paper is written well and the paper is easy to follow. However, there are a few issues that need to be addressed. First of all, it is not clear why the paper does not address the problem of missing data in a more general way. Second, it does not seem to be clear how the method is applied in the real world. For example, the authors do not discuss the impact of the missing data on the accuracy of their method in terms of accuracy. Third, they don't discuss the effect of the method on the quality of the data. Finally, there is no comparison between the method and the method used in the paper. It is unclear why the authors didn't compare the method to the method proposed in this paper. For instance, what is the difference between missing data and missing data? How does the method compare with the method that is used for missing data, and how does it compare to the one proposed in the literature? The paper doesn't seem to address the issue of missing values, and it is hard to understand why the results are not better than the results of the previous paper. The authors do nn't address this issue.",275
8,18864,hybrid,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting. Recent works have used this framework to achieve impressive feats such as learning to classify using one labeled image per class (Snell et al., 2017; Finn et al., 2017), learning unsupervised update rules that generalize to different domains (Metz et al., 2018), and accelerating training procedures that are millions of steps long (Flennerhag et al., 2018). DIMCO generalizes well to novel datasets because its learning objective encourages the model to compactly use all of its degrees of freedom, thus enabling it to effectively compare datapoints while only requiring a small number of bits per datapoint. Derive a generalization bound for meta-learning that makes the tradeoff between taskspecific and task-wise overfitting concrete. Notably, we suggest that certain previous meta-learning methods may have benefitted from implicit regularization. We present experiments in Section 5 and conclude the paper with a discussion about limitations and future directions of our approach in Section 6. This can be problematic since in many works, the task-specific learning algorithm is usually just a byproduct of whatever clever meta-learning loss was proposed. We propose a factorized discrete representation scheme which enables us to represent discrete distributions with exponentially fewer parameters compared to listing the probability of each event. We similarly perform image retrival by mapping each support image to its most likely code (10) and for each query image retrieving the support image that has highest (11). Regularizing Meta-Learners The ability to generalize to novel datasets is critical in meta-learning benchmarks, and even more so in benchmarks such as Meta-Dataset (Triantafillou et al., 2019), where a model is tested on datasets from an unseen domain. The following works have reported benefits from reducing the number of such task-specific parameters: Lee & Choi (2018) learns a subset of the full network to alter during task-specific learning, Rusu et al.(2018) explicitly represents each task with a low-dimensional latent space, and Zintgraf et al.(2018) alters only a pre-specified subset of the full network during task-specific learning. We showed through Theorem 1 that restricting inner-loop expressivity reduces the generalization gap; this provides theoretical understanding to this consensus that meta-learning models with simple task-specific learners generalize to new tasks more easily. Also related is the deterministic information bottleneck (Strouse & Schwab, 2017) which extends the information bottleneck by minimizing H(X̃) rather than I(X̃;X). These three approaches to generalization are related via the chain of inequalities I(X̃;X) ≤ H(X̃) ≤ log |X̃|, which is tight when X̃ is an efficient code. Recent deep learning methods directly learn discrete representations, by learning variational autoencoders with discrete latent variables (Rolfe, 2016; van den Oord et al., 2017; Razavi et al., 2019) or maximizing the mutual information between representation and 1 In practice, we add log probabilities instead of multiplying probabilities for numerical stability. Additionally, their method solves a minimum cost flow problem within each batch to find the locally optimal code, whereas DIMCO is able to directly compute its loss function. We randomly initialize weights for the 4-layer convnet and use pretrained weights for the Inception network This experiment empirically verifies whether mutual information I(X̃;Y ) is a reasonable metric for quality of representation. We observed similar trends when training with with previously proposed loss functions: we visualize these results in Figure 5 of the appendix due to space constraints. Results in Figure 3 show that the compact code of DIMCO takes roughly an order of magnitude less memory for similar performance to N-pair loss, and requires less query time as well. We additionally note that DIMCO is able to train using large backbones without significantly overfitting, whereas experiments reported in Mishra et al.(2017) indicate that MAML (Finn et al., 2017) overfits tremendously when using a deeper backbone. This challenging experimental setup measures how much generalizable information the model can extract from a limited set of datasets; it can be seen as the meta-learning analogue of measuring the performance of a classifier trained with a small dataset. We report the average and standard deviation of the top 5 results of a random hyperparameter search (see appendix for details). Towards Explicit Meta-Regularization In Section 4, we have suggested with analogy to Theorem 1 that the benefits of some previous meta-learning methods can be attributed to implicitly being regularized by reducing the expressivity of their task-specific learners. In future work, we would like to explore explicit meta-regularization schemes that can be applied to other problems (regression, reinforcement learning etc.) Along with showing that the traditional support/query split is not strictly necessary, we demonstrated in Section 5.4 that removing it has the benefit of enabling meta-learning in datasets having one image per class. (12) Assuming that the approximate distribution q(·) is sufficiently close to p(y|x̃), minimizing (12) can be seen as arg min xent(Y, X̃) ≈ arg minEy∼Y,x̃∼X̃ [ − log p(y|x̃) ] (13) = arg minH(Y |X̃) = arg max I(X̃;Y ), (14) where the last equality uses the fact that H(Y ) is independent of model parameters. (24) We similarly bound the error caused by estimating L with a finite number of tasks sampled from τ . Combining equations (26, 24), we have with high probability∣∣∣∣L(τ, θ)− − 1 n n∑ i=1 Î(X̃(XT i , θ);YT i) ∣∣∣∣ (27) ≤ ∣∣∣L(τ, θ)− L̂(τ, θ)∣∣∣+O( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (28) ≤O (√ dΘ n log n dΘ ) +O ( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (29) Hardware Every experiment was conducted on a single Nvidia V100 GPU with CUDA 9.2. Each experiment was performed with different fixed initial seeds; we manually fix seeds with manual_seed() for python, pytorch, and numpy. We additionally show in Figure 5 the correlation between 1-shot accuracies, Recall@1, and NMI using three previously proposed losses (triplet, npair, protonet).",ner1_review2_citation4_metadata2_1k_15_epoch_v1,Discrete InfoMax Codes for Meta-Learning,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting.","This paper presents DIMCO, a meta-learner that is trained by maximizing mutual information between a discrete data representation and class labels across tasks. DIMCO is inspired by an information theoretic lower bound on the generalisation gap for meta-learning, which the authors argue identifies overfitting in the task learner as the bottleneck. 

This work proposes to constrain a learner to output discrete codes that are learned to capture the mutual information with class labels. While the idea of using discrete codes is interesting, its presentation in the manuscript is not well motivated and at times hard to follow. This makes it challenging to evaluate the novelty, validity, and generality of the proposed approach. Meanwhile, the empirical evaluation is somewhat lacking. Thus, I do not believe this work is ready for publication in its current form.

Detailed comments

My main concern is with respect to the primary contribution of this paper, a generalisation bound on meta-learning. The bound appears to be on a multi-task loss without task adaptation, and thus the claims made with respect to the theorem seem somewhat over-reaching. I also believe the VC-dimensionality of the encoder is missing in Eq. 4? If so, this changes the interpretation since the length of the code and the expressivity of the encoder are interrelated. Further, I would welcome a deeper analysis of the theorem and its implications. The current interpretation states that the number of tasks is independent of the size of each task, hence given many tasks, using minimal representations is an effective approach to meta-generalisation. Yet minimal representations is a well-known idea and has features in several works that use mutual information as a regularizer, most notably works on the Information Bottleneck. 

Another reservation I have is the use of mutual information between encoder representations and class labels as a loss function (Eq. 1). It lacks context and a proper motivation, especially since the analysis of [1] shows that the loss function in Eq. 1 is the cross-entropy objective. The authors make a similar analysis in Appendix A and argue that Eq. 1 differs in that cross-entropy is an approximation because it adds a parametrized linear layer on top of \tilde{X}. Thus, in the absence of that layer they collapse to the same objective. As DIMCO itself directly extract class label predictions from \tilde{X}, I fail to see a difference between the loss in Eq. 1 and a cross-entropy objective. 

The main motivation behind their loss objective is that it does not require a support / query set. This does not seem to be a feature of the mutual information objective itself, but rather a choice made by the authors. I would have liked a deeper discussion of this seeing as the authors make it a central tenet of DIMCO. Prior works use a support set as a principled means of doing meta-learning: meta-training explicitly takes into account that at test time, the learner will be given a small support set from which to learn how to query points. As far as I understand, DIMCO does not take this into account during meta-training.  At meta-test time however, DIMCO does use a support set to map query points (Eqs. 10 and 11). Why should we break protocols between meta training and testing? Are there any downsides to doing so?  

Empirically, I find the CUB experiment compelling but would welcome some ablations. What are the trade-offs between p and d? Can DIMCO outperform N-pair when number of bits are unconstrained?

miniImagenet is a standard benchmark in few-shot learning, but I am unable to find a results table - could the authors please provide results on the standard setup so that the method can be compared against known baselines? Further, would the results currently presented hold in a N-way-5-shot setup?

As for the constrained version of miniImagenet that the authors propose, I am not convinced this is an interesting protocol. In general, the miniImagenet task distribution is created by N-way permutations of the classes in the meta-set (e.g. meta-training tasks are combinations of the 64 classes in the meta-training set). By keeping the number of classes constant but reducing the number of images per class, this protocol is not reducing samples per task: a task is always defined as 5/20-way-1-shot (Fig. 4). Instead, the effect should be that tasks are in (greater) violation of the task i.i.d. assumption. Thus, I question whether this setup demonstrates the trade-offs the authors present in Theorem 1 and whether the results can be interpreted in light of it. 

Finally, that both experiments are image-based raises questions as to the generality of the method. The paper could be considerably strengthened by evaluating DIMCO on a non-image task, or if not discuss the method’s limitations.  

The idea of discrete codes for few-shot classification is interesting and sufficiently novel, I am likely to increase my score if my concerns are addressed and the experimental section is strengthened.

Further questions and comments:

- I am unable to parse Eq. 11 - what does the notation \prod_i p_{\tilde{x}_i, i} mean? 
- It is unnecessarily hard to follow the proof of theorem 1. It would help the reader if you restated relevant definitions, such as Eq. 1, since the difference with Eq. 23 is very subtle. It would also be helpful to explain how the summand in Eq. 25 differs from either, and Eq. 26 could be expanded or briefly explained after the derivation. 
-  Because DIMCO is trained with backpropagation, the interpretation of Eq. 5 as d independent events seems invalid. How does it affect the method if they are not independent?
- Overloading X and Y as both random variables and mini-batch samples creates unnecessary confusion. I believe the objective in Eq. 1 is approximated, not calculated exactly? For instance, the mutual information in Eq. 8 is with respect to a mini-batch, so should it not be \hat{I}? 
- p^j_{ik} in Eq. 9 is undefined.
- Eq. 9 is interpreted as an exact entropy, however it appears to be a mini-batch approximation to the true entropy? 

References 
[1] Achille and Soatto. Emergence of Invariance and Disentanglement in Deep Representations. JMLR. 2018."," This paper proposes an information-theoretic generalization bound for meta-learning. The paper is well written and well-written. The main contribution of the paper is that it presents a novel approach to generalize to new datasets. The experiments show that DIMCO generalizes well to novel datasets, especially in a challenging small-data setting.The paper also proposes a novel generalization limit for Meta-Theoretic Generalization bound. This bound is based on the idea that the generalization of a task-specific learner is the key factor that makes generalization to a new dataset difficult. The authors show that the proposed bound is effective in generalizing to the new dataset. They also show that their method generalizes particularly well in a small data setting. They show that this is due to the fact that the model generalizes very well to a small set of datasets. However, the paper does not provide a clear explanation of how this generalizes to larger datasets. It is not clear to me that the authors have a clear understanding of how they achieve this generalization. For example, they do not explain how they do it in the paper, but I am not sure if they have a good idea of what is going to happen to the data. Also, the authors do not provide any explanation of why they do this. I think that the paper should be updated to include a more thorough discussion of the main contributions of this paper. I would also like to see some of the other work on generalization in this area. For instance, I would like to know more about the contributions of the proposed method.",271
9,13063,hybrid,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all. But in each case, the network was designed and tuned specifically for the problem at hand. Natural language processing models have been shown to benefit from a multi-task approach a long time ago (Collobert & Weston, 2008), and recently multi-task machine translation models (MinhThang Luong, 2015) have even been shown to exhibit zero-shot learning when trained on multiple languages (Melvin Johnson, 2016). Speech recognition has also been shown to benefit from multi-task training (Seltzer & Droppo, 2013), as have some vision problems, such as facial landmark detection (Zhang Z., 2014). But no competitive multi-task multi-modal model has been proposed, so the above question remains unanswered. (3) COCO image captioning dataset (Lin et al., 2014), used for image captioning. (4) WSJ parsing dataset (Marcus et al., 1999), used for constituency parsing. While the MultiModel is only a first step and will be improved in the future, two key insights are crucial to making it work at all and are our main contributions. We design modality nets to be computationally minimal, promoting heavy feature extraction and ensuring that the majority of computation is performed within the domain-agnostic body of the model. Two design decisions were important: • The unified representation is variable-size. While a fixed-size representation is tempting and easier to implement, it creates a bottleneck and limits the performance of the model. For example, separable convolutions were introduced in the Xception architecture (Chollet, 2016) and were not applied to text or speech processing before. We find that each of these mechanisms is indeed crucial for the domain it was introduced, e.g., attention is far more important for languagerelated tasks than for image-related ones. As already said above, the encoder and decoder are constructed using 3 key computational blocks to get good performance across different problems: (1) Convolutions allow the model to detect local patterns and generalize across space. We refer the reader to (Chollet, 2016) for a complete definition; here we will denote a depthwise separable convolution with weights Wh×w corresponding to f kernels of size h × w applied to an input tensor x with stride s and dilated by a factor d (see (Yu & Koltun, 2015)) as SepConvd,s,f (W,x). The source tensor is finally passed through two different pointwise convolutions to generate the memory keys K and values V and the query keys, memory keys and memory values are used to apply the attention mechanism between the self-attended target and the source (see Figure 3). On the output side, the language modality takes the decoded output of the body and performs a learned linear mapping, followed by a Softmax, resulting in a probability distribution over the token vocabulary. The spectral modality does not perform any striding along the frequency bin dimension, preserving full resolution in the spectral domain. Convolutional architectures yielded good results on word-level neural machine translation starting from (Kalchbrenner & Blunsom, 2013) and later in (Meng et al., 2015). These early models used a standard RNN on top of the convolution to generate the output and had a bottleneck there that hurt performance, especially on longer sentences, similarly to the limitations of RNN sequence-to-sequence models without attention (Sutskever et al., 2014; Cho et al., 2014). This idea, introduced in WaveNet (van den Oord et al., 2016) and also used in MultiModel (see above) significantly improves efficiency. Depthwise separable convolutions were first studied by Sifre (Sifre & Mallat, 2013) and later they were used to get good results on large-scale image classification with Xception (Chollet, 2016). We focused our experiments so as to answer the following questions: (1) How far is the MultiModel trained on 8 tasks simultaneously from state-of-the-art results? Especially the 4 translation problems behave very similarly, so we decided to not include them all in each comparison but we focused on the more varied problems instead. The results we achieve are similar to the ones task-specific models get without heavy tuning, e.g., on English-French translation we improve on the recent Extended Neural GPU results (Kaiser & Bengio, 2016). Since we are comparing different instantiations of the same model, we report two internal metrics: the negative log-perplexity and per-token accuracy (measured on the development set). As can be seen from the results in Table 2, the joint 8-problem model performs similarly to single-model on large tasks, and better, sometimes significantly, on tasks where less data is available, such as parsing. The difference in performance is significant, and since we use both dropout and early stopping, we conjecture that it is not related to over-fitting. In fact, one could expect that removing these blocks will improve performance on ImageNet alone if they were truly useless for this task. In contrast, we see in Table 4 that these blocks either don’t affect or slightly improve performance. This leads us to conclude that mixing different computation blocks is in fact a good way to improve performance on many various tasks. We believe that this treads a path towards interesting future work on more general deep learning architectures, especially since our model shows transfer learning from tasks with a large amount of available data to ones where the data is limited.",ner1_review2_citation4_metadata2_1k_15_epoch_v1,Large Scale Multi-Domain Multi-Task Learning with MultiModel,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.","The paper presents a multi-task, multi-domain model based on deep neural networks. The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition. The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic. The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task.

The paper is well written and easy to follow.

The contributions of the paper are novel and significant. The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring. The experiments clearly show the viability of the approach and give interesting insights. This is surely an important step towards more general deep learning models. 

Comments:

* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database. Moreover, some databases could be used for different tasks, such as WSJ or ImageNet.

* The training procedure of the model is not explained in the paper. What is the cost function and what is the strategy to train on multiple tasks ? The paper should at least outline the strategy.

* The experiments are sufficient to demonstrate the viability of the approach, but the experimental setup is not clear. Specifically, there is an issue about the speech recognition part of the experiment. It is not clear what the task exactly is: continuous speech recognition, isolated word recognition ? The metrics used in Table 1 are also not clear, they should be explained in the text. Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used. Information about the detailed setup is also lacking, specifically which test and development sets are used (the WSJ corpus has several sets).

* Using raw waveforms as audio modality is very interesting, but this approach is not standard for speech recognition, some references should be provided, such as:
P. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26–30.
D. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE.
T. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals. Learning the Speech Front-end With Raw Waveform CLDNNs. Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015.

Revised Review:
The main idea of the paper is very interesting and the work presented is impressive. However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks. The experiments are interesting, except for the WSJ speech task, which is almost meaningless. Indeed, it is not clear what the network has learned given the metrics presented, as the WER on WSJ should be around 5% for speech recognition.
I thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition. A simpler speech task such as Keyword Spotting could also be investigated.
"," This paper presents a model architecture for multi-task multi-modal learning that works well on a number of tasks, including speech recognition, image captioning, speech recognition corpus, and an English parsing task. The model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. It is trained on ImageNet, multiple translation tasks, and a speech-recognition corpus. The paper is well-written, and the paper is easy to follow. The main contribution of this paper is to show that the model works on a variety of tasks. The proposed model architecture is well written and easy to understand.The paper is very well written, but the main contribution is not a novel one. I would have liked to see more discussion on the topic. I am not sure what the main contributions of the paper are, and I think it would be nice to know more about them. For example, what is the contribution of the model architecture to the performance of the proposed model on the proposed tasks? How does the model compare to other models? What is the performance on large tasks? I would like to see the results on large task. I think the paper would have been better if the authors could provide a more thorough explanation of the results. For instance, the authors should also include a discussion of why the model is not quite right for the large task, and why it is not good for the small task. Also, the paper should include more discussion of how the model performs on small task tasks. It would be interesting to see how well on these tasks. In addition of the attention mechanism is crucial for the performance, but I think this is a bit of a novel idea. I also think that the paper could have been improved by adding the attention function. However, I don't think that this would have improved performance on small tasks.",326
10,6299,hybrid,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines. The problem can also be understood as constructing a function f(G, nS , q) to predict nT , where the functional form of f(·) is generally unknown and has to be learned from a training dataset consisting of samples like (nS , q, nT ). Since nT is unknown, the problem cannot be solved by conventional search algorithms such as A⇤-search [11], which seeks to find paths between the given source and target nodes. For the KBC example in Figure 1(a), having access to the current node nt = Hawaii alone is not sufficient to know that the best action is moving to nt+1 = USA. Second, to address the challenge of sparse rewards, M-Walk exploits the fact that the MDP transition model is known and deterministic.2 Specifically, it combines Monte Carlo Tree Search (MCTS) with the RNN to generate trajectories that obtain significantly more positive rewards than using the RNN policy alone. The rest of the paper is organized as follows: Section 3 develops the M-Walk agent, including the model architecture, the training and testing algorithms.3 Experimental results are presented in Section 4. To see this, we observe from Figure 1(b) that once an action at (i.e., an edge in Ent or “STOP”) is selected, the next node nt+1 and its associated Ent+1 and Nnt+1 are known. The objective is to learn a policy that maximizes the terminal rewards, i.e., correctly identifies the target node with high probability. Recall from Section 2 (e.g., (1)) that one challenge in applying RL to the graph-walking problem is that the state st nominally includes the entire history of observations. We will explain in Section 3.2 how such parameter sharing enables indirect updates to the policy ⇡✓ via Q-learning from off-policy data. To address this issue, we apply the same FCN f✓(·) to 5There could be alternative ways to compute the score, such as Score(n) = maxsT!n Q✓(sT , STOP). Inspired by this recursion, we propose using the GRU-RNN [4] to encode qt into a vector representation6: qt+1 = f✓q (qt, [hA,t, hat,t, nt+1]) with initialization q0 = f✓q (q, [0, 0, nS ]), where ✓q is the model parameter, and hat,t denotes the vector hn0,t at n0 = at. Three Glass Puzzle The Three Glass Puzzle [20] is a problem studied in math puzzles and graph theory. WN18RR [6] is created from the original WN18 [2] by removing various sources of test leakage, making the dataset more challenging. We use HITS@1,3 and mean reciprocal rank (MRR) as the evaluation metrics for WN18RR, and use mean average precision (MAP) for NELL995,7 where HITS@K computes the percentage of the desired entities being ranked among the top-K list, and MRR computes an average of the reciprocal rank of the desired entities. We compare against RL-based methods [38, 5], embedding-based models (including DistMult [39], ComplEx [32] and ConvE [6]) and recent work in logical rules (NeuralLP) [40]. For all the baseline methods, we used the implementation released by the corresponding authors with their best-reported hyperparameter settings.8 The details of the hyperparameters for M-Walk are described in Appendix B.2.2 of the supplementary material. We ran the experiments three times and report the means and standard deviations (except for PRA, TransE, and TransR on NELL995, whose results are directly quoted from [38]). We observed that the novel neural architecture of M-Walk contributes an overall 1% gain relative to MINERVA on NELL995, and it is still 1% worse than M-Walk, which uses MCTS for training and testing. Third, we analyze the performance of M-Walk under different numbers of MCTS rollout simulations and different search horizons on WN18RR dataset, with results shown in Figure 5(a). We observe that M-Walk outperforms the strong baseline ConvE by 4.6–10.9% in samples that require 2 or 3 steps, while it is nearly on par for paths of length one. Therefore, M-Walk does better at reasoning over longer paths than ConvE. To examine this effect, we show in Figure 5(c)-top the HITS@K accuracies when the ground truth is in the candidate set.9 It shows that M-Walk has very high accuracy in this case, which is significantly higher than ConvE (80% vs 39.6% in HITS@1). These observations point to an important direction for improving M-Walk in future work: increasing the chance of covering the target by the candidate set. Finally, in Table 4, we show examples of reasoning paths found by M-Walk.10 Reinforcement Learning Recently, deep reinforcement learning has achieved great success in many artificial intelligence problems [17, 24, 25]. Note that the former is constructed from the visit counts of all the edges connected to the MCTS root node; it only uses information near the root node to improve the policy. Recent approaches have demonstrated limitations of these prior approaches: they suffer from cascading errors when dealing with compositional (multi-step) relationships [10]. Hence, recent works [8, 18, 10, 15, 30] have proposed approaches for injecting multi-step paths such as random walks through sequences of triples during training, further improving performance on KBC tasks. IRN [23] and Neural LP [40] explore multi-step relations by using an RNN controller with attention over an external memory. Empirically, our proposed algorithm outperforms both DeepPath and MINERVA in the KBC benchmarks.11 We developed an RL-agent (M-Walk) that learns to walk over a graph towards a desired target node for given input query and source nodes. Furthermore, we also performed extensive experimental analysis to understand M-Walk. 11A preliminary version of M-Walk with limited experiments was reported in the workshop paper [22].",ner1_review1_1k_3_epoch_v1,M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines.","Summary of the paper:  - This paper proposes a walk-based method for knowledge graph completion (KGC).  - For triples (s, q, o), an agent is trained so that given s and q, it traverses a path from the initial node s to the   terminal node t in a knowledge graph.  - This problem is formulated as that of Q-learning using RNN to model policy and Q functions, but the issue is the   sparseness of rewards.  - Monte Carlo tree search is used to overcome this issue.  Pros:  - The paper is clearly written.  - The approach is natural and sound.  - Very good performance compared with existing KGC methods.  - Ablation study with PG-Walk and Q-Walk is nice.  Cons:  - The work is mostly empirical, without much theoretical justification (admittedly, as in many deep learning papers).  - Not much novelty or surprise, given the success of AlphaGo.  This is a type of paper in which a successful technique   in other fields is applied to a different field.  I could not see exactly what is the novelty/creativity in terms of   how MCTS is applied for this specific problem of KGC.  - No analysis on running time is provided.   General Comments:  This paper is well written (but I would like the content of Section 4.2 to be more organized --- Page 7 consists of a single paragraph now!).  The novelty of the proposal is somewhat questionable, because reinforcement learning has already been introduced for KGC, and the application of MCTS naturally comes to mind.  Although this work is mostly empirical and little theoretical contribution is provided, it still achieves the state-of-the-art performance, and that could be a merit for its publication.  Questions:  * Related work  MCTS has been introduced in walk-based KGC by Sheng et al. in ICLR 2018 Workshop (ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search), and this work must be cited.  Please elaborate on how M-Walk differs from ReinforceWalk as much as you can tell from the extended abstract.  * Experiments:  - Dataset:  Is there any reason the comparison is done with NELL995 and WN18RR?  FB15k is also the standard dataset.  - Baselines  Why do the set of compared methods differ between datasets?  In particular, we know that TransE and TransR are somewhat weaker compared with more recent bilinear models, such as ComplEx, ANALOGY, and ConvE.  - Evaluation metrics  Also, why is the MAP score used on NELL995 whereas HITS@/MRR are used on WN18RR?  I have nothing against MAP scores, but typically HITS@/MRR are reported in literature on KGC even on the NELL-995 dataset (e.g., the MINERVA paper); using different metrics makes it difficult to validate the experimental results.  - Figure 6(a)  Is there any explanation as to why accuracy drops as the number of roll outs are increased to 256?  * Application to classification task  The method is only concerned with prediction of knowledge triple in which only one entity is missing. Can the proposed method be applied to the triple classification task where the truth of a given triple must be predicted? This is more or less straightforward in score-function-based models (provided that a suitable threshold can be determined), but I cannot see how this can be addressed with walk-based methods in general.  * Running time  Training and inference both involves roll-out, which raises concern about the running time of the algorithms.  How does M-walk fare against scoring-based methods (e.g., ComplEx) or MINERVA in terms of running time?  Please provide the time required for training and inference.  I am curious about inference time in particular. "," This paper proposes a graph-walking agent for the knowledge base completion ( KBC ) problem. To overcome the challenge of sparse rewards, the authors use a deep recurrent neural network ( RNN ) and Monte Carlo Tree Search ( MCTS ) to generate trajectories that yield positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Experimental results show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. The paper is well-written and easy to follow. However, I have a few concerns about the paper :1. The authors do not provide a clear explanation of how the MDP transition model is known and deterministic, which makes it difficult to understand the motivation behind the proposed method.2. The proposed method is based on a deep neural network and a Monte Carlo tree search algorithm, which is not well-known in the literature.3. The experimental results on several graph- walking benchmarks are presented in Section 4.4. It is not clear to me why the authors did not compare their method to other methods that are not based on RL. For example, in the case of WN18RR, it seems that the authors don't compare the method to the original WN, which was created from the source and source-based WN. This makes it hard to judge whether the method is better than the original method.5. The results are not very convincing. It would be interesting to see how the method compares to other RL methods in terms of performance.6. In particular, it would be nice to see the performance on the three-glass-puzzle benchmark.",282
11,837,hybrid,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art. Since the introduction of the “AlexNet” architecture [14] in the 2012 ImageNet competition, ConvNets have dominated state-of-the-art performance across a variety of computer vision tasks, including object-detection, image segmentation, image classification, face recognition, human pose estimation and tracking. In conjunction with these advances as well as the evolution of network architectures, several design best practices have emerged [8, 21, 23, 24]. This functionality can be achieved by stacking many small filters or using large filters in the network; notably, the first choice can be implemented with fewer operations (faster, fewer parameters) and also allows inclusion of more nonlinearities. Third, dimensionality reduction (1×1 convolutions) before spatially aggregating filters (e.g.3×3) is supported by the fact that outputs of neighbouring filters are highly correlated and therefore these activations can be reduced before aggregation [23]. Fourth, spatial factorization into asymmetric filters can even further reduce computational cost and 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. Fifth, it is important to normalize the responses of each feature channel within a batch to reduce internal covariate shift [11]. To demonstrate the benefits of our proposed spatiotemporal ResNet architecture, it has been evaluated on two standard action recognition benchmarks where it greatly boosts the state-of-the-art. Yet another study compared several approaches to extending ConvNets into the temporal domain, but with rather disappointing results [13]: The architectures were not particularly sensitive to temporal modelling, with a slow fusion model performing slightly better than early and late fusion alternatives; moreover, similar levels of performance were achieved by a purely spatial network. Another research branch has investigated combining image information in network architectures across longer time periods. That approach takes advantage of the local spatial similarity in images; however, it only yields a minor increase over their baseline, which is a two-stream VGG-16 ConvNet [21] used as the input to their convolutional RNN. Batch normalization [11] and ReLU [14] are applied after each convolution; the network does not use hidden fc, dropout, or max-pooling (except immediately after the first layer). In preliminary experiments we found that direct connections between identical layers of the two streams led to an increase in validation error. We conjecture that these results are due to the large change that the signal of one network stream undergoes after injecting a fusion signal from the other stream. Therefore, we developed a more subtle alternative solution based on additive interactions, as follows. To enable learning of spatiotemporal features at all possible scales, this modification is applied before the second residual unit at each spatial resolution of the network (indicated by “skip-stream” in Table 1), as exemplified by the connection at the conv5_x layers in Fig. Based on the above observations, we developed a novel approach to temporal residual connections that builds on the ConvNet design guidelines of chaining small [21] asymmetric [10, 23] filters, noted in Sec. This allows the straightforward use of standard two-stream ConvNets that have been pre-trained on large-scale datasets e.g.to leverage the massive amounts of training data from the ImageNet challenge. From left to right, top to bottom, the first row shows the convolutional and pooling building blocks, with the filter and pooling size shown as (W ×H × T,C), denoting width, height, temporal extent and number of feature channels, resp. In the last two rows we show the output size of these metalayers as well as the receptive field on which they operate. We also apply random cropping and scale augmentations: We randomly jitter the width and height of the 224× 224 input frame by ±25% and also randomly crop it from a maximum of 25% distance from the image borders. We found that this strategy significantly reduces the training duration with the drawback that each loss does not capture all available information. For our final model, we equip the spatiotemporal ResNet with a temporal max-pooling layer after pool5 (see Table 1, temporal average pooling led to inferior results) and continue training as above with the learning rate starting from 10−4 for 2K iterations followed by 10−5. For fair comparison, we follow the evaluation procedure of the original two-stream work [20] by sampling 25 frames (and their horizontal flips). However, rather than using 10 spatial 224× 224 crops from each of the frames, we apply fully convolutional testing both spatially (smallest side rescaled to 256) and temporally (the 25 frame-chunks) by classifying the video in a single forward pass, which takes ≈250ms on a Titan X GPU. Second, we consider HMDB51 [15], which has 6766 videos that show 51 different actions and generally is considered more challenging than UCF0101 due to the even wider variations in which actions occur. For both datasets, we use the provided evaluation protocol and report mean average accuracy over three splits into training and test sets. Interestingly, research in neuroscience also suggests that the human visual cortex is equipped with connections between the dorsal and the ventral stream to distribute motion information to separate visual areas [3, 27]. We achieve this by simply averaging the L2-normalized SVM scores of the FV-encoded IDT descriptors (i.e.HOG, HOF, MBH) [29] with the L2-normalized video predictions of our ST-ResNet*, again without softmax normalization. These relatively larger performance decrements again underline that our approach is better able to capture the available dynamic information, as there is less to be gained by augmenting it with IDT. Still, there is a benefit from the hand-crafted IDT features even with our approach, which could be attributed to its explicit compensation of camera motion.",ner1_review1_1k_3_epoch_v1,Spatiotemporal Residual Networks for Video Action Recognition,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art.","The paper presents a novel architecture that 1) combines residual networks with two-stream convolutional networks, and 2) injects connections from the motion stream to the appearance stream, to be able to capture spatio-temporal features. The paper shows experiments in both of the main action recognition datasets, achieving state-of-the-art accuracy in both. Overall I think the paper is great: good idea, careful experimentation, great results and clearly written.   Although the basic components of the architecture are pre-existing, I think that the high performance and careful experimentation and description make it a very useful contribution.   I only miss some experiments to visualize what kinds of spatio-temporal features are being learned.   Since large temporal windows are important (278-280) I would add a relevant reference: Long-term Temporal Convolutions for Action Recognition, Gul Varol, Ivan Laptev, Cordelia Schmid  Small typos:  (L 187)  ""resp.""  (L 255) ""UCF0101""  "," This paper proposes a new Residual ResNet architecture for two-stream Convolutional Networks ( ResNets ). The idea is to combine the two existing ResNet architectures, which are based on the two-layer convolutional networks ( ResNet and ResNet 2 ), by adding temporal residual connections between the appearance and motion pathways of the two streams to allow spatiotemporal interaction between the two networks. The proposed ResNet is evaluated on two standard action recognition benchmarks where it greatly boosts the state-of-the-art.The paper is well written and easy to follow. It is well-written and well-motivated. The paper is clearly written and well organized. The results are interesting. The novelty of the paper is that it is a combination of the ResNet approach and the ResNet architecture. However, there are a few things that I do not like about the paper :1. The authors do not provide a clear explanation of how the proposed Resnet is different from the existing Resnet architecture.2. They do not explain how the Resnet network is trained end-to-end to allow hierarchical learning of complex features. 3. They also do not discuss how the model is trained to learn spatially aggregating filters. 4. They did not explain why they do not use hidden fc, dropout, or max-pooling. 5. I don't understand why they didn't compare to the previous ResNet model. I would like to see the performance of the new ResNet to the state of the art in terms of accuracy and accuracy.",244
12,22112,hybrid,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values. While a large body of the machine learning literature is built upon the assumption of having access to complete datasets, in many real-world problems only incomplete datasets are available. Single imputation methods try to replace each missing value with a plausible value that is the best fit given the value of other correlated features and knowledge extracted from the dataset (Hastie et al., 1999; Anderson, 1957). While MI offers interesting statistical insights about the reliability of analysis on incomplete data, the insight is imprecise as it is mainly concerned about the population of data samples rather than individual instances. While these methods are easy to implement and analyze, they often fail to capture the complex feature dependencies as well as structures present in many problems. For instance, Mattei & Frellsen (2018) suggested a method based on deep latent variable models and importance sampling that offers a tighter likelihood bound compared to the standard VAE bound. Also, these methods are often complicated to be applied in practical setups by practitioners. A few exceptions exist such as Bayesian models and decision trees that permit direct analysis on incomplete data (Nielsen & Jensen, 2009; Zhang et al., 2005). However, given an incomplete training dataset and without any known causal structure as a priori, learning such models is a very challenging problem with the complexity of at least NP-complete to learn the network architecture in addition to an iterative EM optimization to learn model parameters (Darwiche, 2009; Neapolitan et al., 2004). Similarly, for the second part, we are interested in obtaining a distribution over the possible target assignments and the confidence of each class rather than maximum likelihood class assignments. Instead, we use recent advances in GAN stabilization and training to improve the training process (see Section 3.4). Additionally, in our experiments, we provide supporting evidence that this simple loss function enables us to sample from the conditional distribution and prevents biased inclinations toward distribution modes. For instance, for a specific test sample at hand, if a certain missing feature is a strong indicator of the target class, we would like to observe the impact of different imputations for that feature on the final hypothesis. Input: G (trained imputer), D (dataset) Output: Fθ (trained predictor) foreach Training Epoch do foreach (xi, ki, yi) in D do z ∼ N(0, I) x̂i ← ki xi+(1−ki) G(xi,ki, z) ypredi ← Fθ(x̂i) loss← L(yi, ypredi ) Backpropagate loss Update Fθ Algorithm 2: Estimating target distributions. The first assumption is consistent with the theoretical analysis of generative adversarial networks that they can converge to the true underlying distribution (Arora et al., 2018; Liu et al., 2017). It should be noted that, while Zhang et al.(2018) suggests using a single self-attention layer in the middle of the network, we observed consistent improvements by inserting multiple self-attention layers before each residual block within the network. We explored best TTUR learning-rate settings from the set of {0.001, 0.0005, 0.0001, 0.00005}. To evaluate the proposed method we use CIFAR-10 (Krizhevsky & Hinton, 2009) as an image classification dataset as well as three non-image datasets: UCI Landsat (Dua & Graff, 2017)2, MIT-BIH arrhythmia (Moody & Mark, 2001), and Diabetes classification (Kachuee et al., 2019) 3. Also, while different encoding and representation methods are suggested in the literature to handle categorical features (Jang et al., 2016; Nazabal et al., 2018), in this paper, we take the simple approach of encoding categorical variables using one-hot representation and smoothing them by adding Gaussian noise with zero mean and variance equal to 5% of feature variances. We would like to note that while the suggested solution in this paper is readily compatible with MAR structures, in our experiments, to simplify the presentation of results and to have a fair comparison with other work that does not support the MAR assumption, we limited the scope of our experiments to MCAR. Furthermore, to simulate incomplete datasets and to make sure the same features are missing without explicitly storing masks, we use hashed feature vectors to seed random number generators used to sample missing features. More detail is provided in Appendix C. Fréchet inception distance (FID) (Heusel et al., 2017) score is used to measure the quality of missing data imputation in experiments with images5. We also considered using root means squared error (RMSE); however, we decided not to use this measure as we observed an inconsistent behavior using RMSE in our comparisons as RMSE favors methods that show less variance rather than realistic and sharp samples from the distribution. We compare our results with MisGAN (Li et al., 2019) and GAIN (Yoon et al., 2018) as the state of the art imputation algorithms based on GANs as well as basic denoising autoencoder (DAE) (Vincent et al., 2008) and multiple imputation by chained equations (MICE) (Buuren & Groothuis-Oudshoorn, 2010) as baselines. Due to scalability issues, we were only able to use MICE for the smaller non-image datasets. One possible explanation for this behavior might be the fact that GAIN has an MSE loss term acting similar to an autoencoder loss smoothing noisy missing pixels. Regarding the MIT-BIH experiemts, GI outperforms other work for missing rates more than 30% while achieving similar accuracies to GAIN for lower missing rates. As it can be seen from the plots, GI provides results closest to the ideal case of having average confidence values equal to average accuracies. In order to provide further insight into the operation of GI and how imputations can potentially influence the outcomes of predictions, we conduct experiments on a synthesized dataset. One possible explanation could be the fact that imputing missing data with a uniform structure can be done by processing local regions and does not require attending to different distant regions across the image. From Table 6 it can be inferred that as the rate of missingness increases, the benefits of the suggested predictor algorithm increase significantly.",ner1_review1_1k_3_epoch_v1,Generative Imputation and Stochastic Prediction,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values.","This paper proposes a method to impute missing features using a generative model and train a predictive model on top of imputed dataset to improve classification results. They first train a GAN model where the generator outputs an imputed representation of the input and discriminator is trained to predict if an individual features (such as a pixel) is imputed or not. Given the generator and incomplete sample, they train a predictor using the output of the generator, imputed sample, as input. Their main contribution is using a MC averaging to compute the prediction by repetitively sampling from the noise variable, z, and generating different imputations from generator. They show that the proposed model improves upon the previous SOTA on final classification performance.

Overall the paper is clearly written. But I do feel it is a bit incremental over the GAIN approach. The overall GAN architecture is very similar to GAIN's and although stochastic prediction shows clear improvements it is a bit straightforward. However, I think the uncertainty of the imputations and its effect on the final prediction is interesting. I suggest the authors to extend this part with more detailed analysis.

There are several parts that are confusing/missing in the paper:

- In GAIN, they use a hint vector as an input to the discriminator. They show that without the hint vector, there is no unique solution (this is shown without the MSE loss). The authors do not use this vector in their approach (as in Figure 1) and it is not clear to me if it causes any instabilities or if multiple experiments yield similar results or if the stochastic prediction benefits from this.
- On what type of examples GI is more accurate than other models? Since stochastic prediction is the main difference from GAIN, is this related to the multi-modality of the noisy examples?
- Can you explain the difference between the results in Figure-7 and Table-2? Results between the two mismatch.
- I think the statement in the first paragraph in Section 4.4 that ""MSE loss term would act as a denoising loss smoothing noisy missing pixels"" could be misleading. MSE is used with mask in GAIN, hence it only applies to the observed features during training. Its effect on smoothing noisy missing pixels is not clear.


I think the paper would benefit if the authors could explain/show:
- Increasing the missing rate would also increase the possibility that the ground truth be a more multi-modal distribution. Especially in rectangular generation part where it can remove a complete object. Does stochastic averaging benefit more in this case?"," This paper proposes a method for imputing missing features and estimating the distribution of target assignments given incomplete data. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures.The paper is well written and easy to follow. The method is well-written and well-motivated. The experimental results show the effectiveness of the proposed method in generating imputations and providing estimates for the class uncertainties in a classification task when faced with missing values. The paper is clearly written and well written. The experiments are well-constructed. The results are interesting and the paper is easy to read. However, there are a few things that I would like to see more of. For example, I would have liked to see the impact of different imputations for different features on the final hypothesis. For instance, for a specific test sample at hand, if a certain missing feature is a strong indicator of the target class, it would be interesting to see how the imputations are affected by the missing features. I would also like the authors to provide more details on the training process of the generator and predictor networks. I am not sure how to interpret the results of the experiments.",212
13,18864,hybrid,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting. Recent works have used this framework to achieve impressive feats such as learning to classify using one labeled image per class (Snell et al., 2017; Finn et al., 2017), learning unsupervised update rules that generalize to different domains (Metz et al., 2018), and accelerating training procedures that are millions of steps long (Flennerhag et al., 2018). DIMCO generalizes well to novel datasets because its learning objective encourages the model to compactly use all of its degrees of freedom, thus enabling it to effectively compare datapoints while only requiring a small number of bits per datapoint. Derive a generalization bound for meta-learning that makes the tradeoff between taskspecific and task-wise overfitting concrete. Notably, we suggest that certain previous meta-learning methods may have benefitted from implicit regularization. We present experiments in Section 5 and conclude the paper with a discussion about limitations and future directions of our approach in Section 6. This can be problematic since in many works, the task-specific learning algorithm is usually just a byproduct of whatever clever meta-learning loss was proposed. We propose a factorized discrete representation scheme which enables us to represent discrete distributions with exponentially fewer parameters compared to listing the probability of each event. We similarly perform image retrival by mapping each support image to its most likely code (10) and for each query image retrieving the support image that has highest (11). Regularizing Meta-Learners The ability to generalize to novel datasets is critical in meta-learning benchmarks, and even more so in benchmarks such as Meta-Dataset (Triantafillou et al., 2019), where a model is tested on datasets from an unseen domain. The following works have reported benefits from reducing the number of such task-specific parameters: Lee & Choi (2018) learns a subset of the full network to alter during task-specific learning, Rusu et al.(2018) explicitly represents each task with a low-dimensional latent space, and Zintgraf et al.(2018) alters only a pre-specified subset of the full network during task-specific learning. We showed through Theorem 1 that restricting inner-loop expressivity reduces the generalization gap; this provides theoretical understanding to this consensus that meta-learning models with simple task-specific learners generalize to new tasks more easily. Also related is the deterministic information bottleneck (Strouse & Schwab, 2017) which extends the information bottleneck by minimizing H(X̃) rather than I(X̃;X). These three approaches to generalization are related via the chain of inequalities I(X̃;X) ≤ H(X̃) ≤ log |X̃|, which is tight when X̃ is an efficient code. Recent deep learning methods directly learn discrete representations, by learning variational autoencoders with discrete latent variables (Rolfe, 2016; van den Oord et al., 2017; Razavi et al., 2019) or maximizing the mutual information between representation and 1 In practice, we add log probabilities instead of multiplying probabilities for numerical stability. Additionally, their method solves a minimum cost flow problem within each batch to find the locally optimal code, whereas DIMCO is able to directly compute its loss function. We randomly initialize weights for the 4-layer convnet and use pretrained weights for the Inception network This experiment empirically verifies whether mutual information I(X̃;Y ) is a reasonable metric for quality of representation. We observed similar trends when training with with previously proposed loss functions: we visualize these results in Figure 5 of the appendix due to space constraints. Results in Figure 3 show that the compact code of DIMCO takes roughly an order of magnitude less memory for similar performance to N-pair loss, and requires less query time as well. We additionally note that DIMCO is able to train using large backbones without significantly overfitting, whereas experiments reported in Mishra et al.(2017) indicate that MAML (Finn et al., 2017) overfits tremendously when using a deeper backbone. This challenging experimental setup measures how much generalizable information the model can extract from a limited set of datasets; it can be seen as the meta-learning analogue of measuring the performance of a classifier trained with a small dataset. We report the average and standard deviation of the top 5 results of a random hyperparameter search (see appendix for details). Towards Explicit Meta-Regularization In Section 4, we have suggested with analogy to Theorem 1 that the benefits of some previous meta-learning methods can be attributed to implicitly being regularized by reducing the expressivity of their task-specific learners. In future work, we would like to explore explicit meta-regularization schemes that can be applied to other problems (regression, reinforcement learning etc.) Along with showing that the traditional support/query split is not strictly necessary, we demonstrated in Section 5.4 that removing it has the benefit of enabling meta-learning in datasets having one image per class. (12) Assuming that the approximate distribution q(·) is sufficiently close to p(y|x̃), minimizing (12) can be seen as arg min xent(Y, X̃) ≈ arg minEy∼Y,x̃∼X̃ [ − log p(y|x̃) ] (13) = arg minH(Y |X̃) = arg max I(X̃;Y ), (14) where the last equality uses the fact that H(Y ) is independent of model parameters. (24) We similarly bound the error caused by estimating L with a finite number of tasks sampled from τ . Combining equations (26, 24), we have with high probability∣∣∣∣L(τ, θ)− − 1 n n∑ i=1 Î(X̃(XT i , θ);YT i) ∣∣∣∣ (27) ≤ ∣∣∣L(τ, θ)− L̂(τ, θ)∣∣∣+O( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (28) ≤O (√ dΘ n log n dΘ ) +O ( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (29) Hardware Every experiment was conducted on a single Nvidia V100 GPU with CUDA 9.2. Each experiment was performed with different fixed initial seeds; we manually fix seeds with manual_seed() for python, pytorch, and numpy. We additionally show in Figure 5 the correlation between 1-shot accuracies, Recall@1, and NMI using three previously proposed losses (triplet, npair, protonet).",ner1_review1_1k_3_epoch_v1,Discrete InfoMax Codes for Meta-Learning,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting.","This paper presents DIMCO, a meta-learner that is trained by maximizing mutual information between a discrete data representation and class labels across tasks. DIMCO is inspired by an information theoretic lower bound on the generalisation gap for meta-learning, which the authors argue identifies overfitting in the task learner as the bottleneck. 

This work proposes to constrain a learner to output discrete codes that are learned to capture the mutual information with class labels. While the idea of using discrete codes is interesting, its presentation in the manuscript is not well motivated and at times hard to follow. This makes it challenging to evaluate the novelty, validity, and generality of the proposed approach. Meanwhile, the empirical evaluation is somewhat lacking. Thus, I do not believe this work is ready for publication in its current form.

Detailed comments

My main concern is with respect to the primary contribution of this paper, a generalisation bound on meta-learning. The bound appears to be on a multi-task loss without task adaptation, and thus the claims made with respect to the theorem seem somewhat over-reaching. I also believe the VC-dimensionality of the encoder is missing in Eq. 4? If so, this changes the interpretation since the length of the code and the expressivity of the encoder are interrelated. Further, I would welcome a deeper analysis of the theorem and its implications. The current interpretation states that the number of tasks is independent of the size of each task, hence given many tasks, using minimal representations is an effective approach to meta-generalisation. Yet minimal representations is a well-known idea and has features in several works that use mutual information as a regularizer, most notably works on the Information Bottleneck. 

Another reservation I have is the use of mutual information between encoder representations and class labels as a loss function (Eq. 1). It lacks context and a proper motivation, especially since the analysis of [1] shows that the loss function in Eq. 1 is the cross-entropy objective. The authors make a similar analysis in Appendix A and argue that Eq. 1 differs in that cross-entropy is an approximation because it adds a parametrized linear layer on top of \tilde{X}. Thus, in the absence of that layer they collapse to the same objective. As DIMCO itself directly extract class label predictions from \tilde{X}, I fail to see a difference between the loss in Eq. 1 and a cross-entropy objective. 

The main motivation behind their loss objective is that it does not require a support / query set. This does not seem to be a feature of the mutual information objective itself, but rather a choice made by the authors. I would have liked a deeper discussion of this seeing as the authors make it a central tenet of DIMCO. Prior works use a support set as a principled means of doing meta-learning: meta-training explicitly takes into account that at test time, the learner will be given a small support set from which to learn how to query points. As far as I understand, DIMCO does not take this into account during meta-training.  At meta-test time however, DIMCO does use a support set to map query points (Eqs. 10 and 11). Why should we break protocols between meta training and testing? Are there any downsides to doing so?  

Empirically, I find the CUB experiment compelling but would welcome some ablations. What are the trade-offs between p and d? Can DIMCO outperform N-pair when number of bits are unconstrained?

miniImagenet is a standard benchmark in few-shot learning, but I am unable to find a results table - could the authors please provide results on the standard setup so that the method can be compared against known baselines? Further, would the results currently presented hold in a N-way-5-shot setup?

As for the constrained version of miniImagenet that the authors propose, I am not convinced this is an interesting protocol. In general, the miniImagenet task distribution is created by N-way permutations of the classes in the meta-set (e.g. meta-training tasks are combinations of the 64 classes in the meta-training set). By keeping the number of classes constant but reducing the number of images per class, this protocol is not reducing samples per task: a task is always defined as 5/20-way-1-shot (Fig. 4). Instead, the effect should be that tasks are in (greater) violation of the task i.i.d. assumption. Thus, I question whether this setup demonstrates the trade-offs the authors present in Theorem 1 and whether the results can be interpreted in light of it. 

Finally, that both experiments are image-based raises questions as to the generality of the method. The paper could be considerably strengthened by evaluating DIMCO on a non-image task, or if not discuss the method’s limitations.  

The idea of discrete codes for few-shot classification is interesting and sufficiently novel, I am likely to increase my score if my concerns are addressed and the experimental section is strengthened.

Further questions and comments:

- I am unable to parse Eq. 11 - what does the notation \prod_i p_{\tilde{x}_i, i} mean? 
- It is unnecessarily hard to follow the proof of theorem 1. It would help the reader if you restated relevant definitions, such as Eq. 1, since the difference with Eq. 23 is very subtle. It would also be helpful to explain how the summand in Eq. 25 differs from either, and Eq. 26 could be expanded or briefly explained after the derivation. 
-  Because DIMCO is trained with backpropagation, the interpretation of Eq. 5 as d independent events seems invalid. How does it affect the method if they are not independent?
- Overloading X and Y as both random variables and mini-batch samples creates unnecessary confusion. I believe the objective in Eq. 1 is approximated, not calculated exactly? For instance, the mutual information in Eq. 8 is with respect to a mini-batch, so should it not be \hat{I}? 
- p^j_{ik} in Eq. 9 is undefined.
- Eq. 9 is interpreted as an exact entropy, however it appears to be a mini-batch approximation to the true entropy? 

References 
[1] Achille and Soatto. Emergence of Invariance and Disentanglement in Deep Representations. JMLR. 2018."," Theorem 1 shows that restricting inner-loop expressivity reduces the generalization gap in meta-learning. The paper proposes a new method called Discrete InfoMax Codes ( DIMCO ) that uses a stochastic encoder to output discrete codes.The paper is well-written and easy to follow. The main contribution is an information-theoretic generalization bound for meta- learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult.Experiments show that DIMco requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting. I think the paper has a lot of potential to be improved. However, I have a few concerns with the paper :1 ) The paper is written in a way that makes it difficult to understand how it works, and it is not clear to me that the results are representative of the state-of-the-art.2 ) It is unclear to me how the experiments are comparable to the results reported in Mishra et al. ( 2017 ). 3 ) The experiments are not very convincing, and I am not sure how well the results compare to the performance of MAMFinn et al. ( 2018 ). 4 ) It would be interesting to see the results of the experiments on a Meta-Dataset, which is a very challenging dataset. 5 ) The results on Meta-Data are not particularly impressive. It would have been nice to see a comparison of the performance on a classifier trained with and without the same classifier. 6 ). The experiments on the other side of the paper would be nice to compare with the results on MAMLinn's method. 7 ) The experimental results are not that impressive either. I would have liked to see some comparison on a different set of datasets. 8 ) I would also like to see more discussion on the impact of the experimental results on generalization.",325
14,13063,hybrid,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all. But in each case, the network was designed and tuned specifically for the problem at hand. Natural language processing models have been shown to benefit from a multi-task approach a long time ago (Collobert & Weston, 2008), and recently multi-task machine translation models (MinhThang Luong, 2015) have even been shown to exhibit zero-shot learning when trained on multiple languages (Melvin Johnson, 2016). Speech recognition has also been shown to benefit from multi-task training (Seltzer & Droppo, 2013), as have some vision problems, such as facial landmark detection (Zhang Z., 2014). But no competitive multi-task multi-modal model has been proposed, so the above question remains unanswered. (3) COCO image captioning dataset (Lin et al., 2014), used for image captioning. (4) WSJ parsing dataset (Marcus et al., 1999), used for constituency parsing. While the MultiModel is only a first step and will be improved in the future, two key insights are crucial to making it work at all and are our main contributions. We design modality nets to be computationally minimal, promoting heavy feature extraction and ensuring that the majority of computation is performed within the domain-agnostic body of the model. Two design decisions were important: • The unified representation is variable-size. While a fixed-size representation is tempting and easier to implement, it creates a bottleneck and limits the performance of the model. For example, separable convolutions were introduced in the Xception architecture (Chollet, 2016) and were not applied to text or speech processing before. We find that each of these mechanisms is indeed crucial for the domain it was introduced, e.g., attention is far more important for languagerelated tasks than for image-related ones. As already said above, the encoder and decoder are constructed using 3 key computational blocks to get good performance across different problems: (1) Convolutions allow the model to detect local patterns and generalize across space. We refer the reader to (Chollet, 2016) for a complete definition; here we will denote a depthwise separable convolution with weights Wh×w corresponding to f kernels of size h × w applied to an input tensor x with stride s and dilated by a factor d (see (Yu & Koltun, 2015)) as SepConvd,s,f (W,x). The source tensor is finally passed through two different pointwise convolutions to generate the memory keys K and values V and the query keys, memory keys and memory values are used to apply the attention mechanism between the self-attended target and the source (see Figure 3). On the output side, the language modality takes the decoded output of the body and performs a learned linear mapping, followed by a Softmax, resulting in a probability distribution over the token vocabulary. The spectral modality does not perform any striding along the frequency bin dimension, preserving full resolution in the spectral domain. Convolutional architectures yielded good results on word-level neural machine translation starting from (Kalchbrenner & Blunsom, 2013) and later in (Meng et al., 2015). These early models used a standard RNN on top of the convolution to generate the output and had a bottleneck there that hurt performance, especially on longer sentences, similarly to the limitations of RNN sequence-to-sequence models without attention (Sutskever et al., 2014; Cho et al., 2014). This idea, introduced in WaveNet (van den Oord et al., 2016) and also used in MultiModel (see above) significantly improves efficiency. Depthwise separable convolutions were first studied by Sifre (Sifre & Mallat, 2013) and later they were used to get good results on large-scale image classification with Xception (Chollet, 2016). We focused our experiments so as to answer the following questions: (1) How far is the MultiModel trained on 8 tasks simultaneously from state-of-the-art results? Especially the 4 translation problems behave very similarly, so we decided to not include them all in each comparison but we focused on the more varied problems instead. The results we achieve are similar to the ones task-specific models get without heavy tuning, e.g., on English-French translation we improve on the recent Extended Neural GPU results (Kaiser & Bengio, 2016). Since we are comparing different instantiations of the same model, we report two internal metrics: the negative log-perplexity and per-token accuracy (measured on the development set). As can be seen from the results in Table 2, the joint 8-problem model performs similarly to single-model on large tasks, and better, sometimes significantly, on tasks where less data is available, such as parsing. The difference in performance is significant, and since we use both dropout and early stopping, we conjecture that it is not related to over-fitting. In fact, one could expect that removing these blocks will improve performance on ImageNet alone if they were truly useless for this task. In contrast, we see in Table 4 that these blocks either don’t affect or slightly improve performance. This leads us to conclude that mixing different computation blocks is in fact a good way to improve performance on many various tasks. We believe that this treads a path towards interesting future work on more general deep learning architectures, especially since our model shows transfer learning from tasks with a large amount of available data to ones where the data is limited.",ner1_review1_1k_3_epoch_v1,Large Scale Multi-Domain Multi-Task Learning with MultiModel,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.","The paper presents a multi-task, multi-domain model based on deep neural networks. The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition. The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic. The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task.

The paper is well written and easy to follow.

The contributions of the paper are novel and significant. The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring. The experiments clearly show the viability of the approach and give interesting insights. This is surely an important step towards more general deep learning models. 

Comments:

* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database. Moreover, some databases could be used for different tasks, such as WSJ or ImageNet.

* The training procedure of the model is not explained in the paper. What is the cost function and what is the strategy to train on multiple tasks ? The paper should at least outline the strategy.

* The experiments are sufficient to demonstrate the viability of the approach, but the experimental setup is not clear. Specifically, there is an issue about the speech recognition part of the experiment. It is not clear what the task exactly is: continuous speech recognition, isolated word recognition ? The metrics used in Table 1 are also not clear, they should be explained in the text. Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used. Information about the detailed setup is also lacking, specifically which test and development sets are used (the WSJ corpus has several sets).

* Using raw waveforms as audio modality is very interesting, but this approach is not standard for speech recognition, some references should be provided, such as:
P. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26–30.
D. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE.
T. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals. Learning the Speech Front-end With Raw Waveform CLDNNs. Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015.

Revised Review:
The main idea of the paper is very interesting and the work presented is impressive. However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks. The experiments are interesting, except for the WSJ speech task, which is almost meaningless. Indeed, it is not clear what the network has learned given the metrics presented, as the WER on WSJ should be around 5% for speech recognition.
I thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition. A simpler speech task such as Keyword Spotting could also be investigated.
"," This paper presents a model architecture for multi-task multi-modal language processing. The model is trained on a number of tasks, including translation, image captioning, speech recognition, and parsing tasks. The architecture is based on a single model, which is trained concurrently on ImageNet, multiple translation tasks, and a speech recognition task. The proposed model architecture is a combination of several different building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on.The paper is well-written and easy to follow. The paper is easy to read. The writing is clear and easy-to-follow. However, I have a few questions about the paper :1. The main contribution of the paper is the introduction of the multi-model architecture. It is not clear to me what the main contribution is.2. How is the model designed? How is it trained? What is the architecture of the model? 3. What are the contributions of the proposed model? 4. How are the proposed architecture compared to the previous work? 5. How does the model compare to the existing multi-tasking models? 6. How do you compare the performance of the new model with the existing models? 7. What do you think is the difference between your model and the other models? 8. Is there a reason why your model is better than the previous models?",237
15,6299,hybrid,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines. The problem can also be understood as constructing a function f(G, nS , q) to predict nT , where the functional form of f(·) is generally unknown and has to be learned from a training dataset consisting of samples like (nS , q, nT ). Since nT is unknown, the problem cannot be solved by conventional search algorithms such as A⇤-search [11], which seeks to find paths between the given source and target nodes. For the KBC example in Figure 1(a), having access to the current node nt = Hawaii alone is not sufficient to know that the best action is moving to nt+1 = USA. Second, to address the challenge of sparse rewards, M-Walk exploits the fact that the MDP transition model is known and deterministic.2 Specifically, it combines Monte Carlo Tree Search (MCTS) with the RNN to generate trajectories that obtain significantly more positive rewards than using the RNN policy alone. The rest of the paper is organized as follows: Section 3 develops the M-Walk agent, including the model architecture, the training and testing algorithms.3 Experimental results are presented in Section 4. To see this, we observe from Figure 1(b) that once an action at (i.e., an edge in Ent or “STOP”) is selected, the next node nt+1 and its associated Ent+1 and Nnt+1 are known. The objective is to learn a policy that maximizes the terminal rewards, i.e., correctly identifies the target node with high probability. Recall from Section 2 (e.g., (1)) that one challenge in applying RL to the graph-walking problem is that the state st nominally includes the entire history of observations. We will explain in Section 3.2 how such parameter sharing enables indirect updates to the policy ⇡✓ via Q-learning from off-policy data. To address this issue, we apply the same FCN f✓(·) to 5There could be alternative ways to compute the score, such as Score(n) = maxsT!n Q✓(sT , STOP). Inspired by this recursion, we propose using the GRU-RNN [4] to encode qt into a vector representation6: qt+1 = f✓q (qt, [hA,t, hat,t, nt+1]) with initialization q0 = f✓q (q, [0, 0, nS ]), where ✓q is the model parameter, and hat,t denotes the vector hn0,t at n0 = at. Three Glass Puzzle The Three Glass Puzzle [20] is a problem studied in math puzzles and graph theory. WN18RR [6] is created from the original WN18 [2] by removing various sources of test leakage, making the dataset more challenging. We use HITS@1,3 and mean reciprocal rank (MRR) as the evaluation metrics for WN18RR, and use mean average precision (MAP) for NELL995,7 where HITS@K computes the percentage of the desired entities being ranked among the top-K list, and MRR computes an average of the reciprocal rank of the desired entities. We compare against RL-based methods [38, 5], embedding-based models (including DistMult [39], ComplEx [32] and ConvE [6]) and recent work in logical rules (NeuralLP) [40]. For all the baseline methods, we used the implementation released by the corresponding authors with their best-reported hyperparameter settings.8 The details of the hyperparameters for M-Walk are described in Appendix B.2.2 of the supplementary material. We ran the experiments three times and report the means and standard deviations (except for PRA, TransE, and TransR on NELL995, whose results are directly quoted from [38]). We observed that the novel neural architecture of M-Walk contributes an overall 1% gain relative to MINERVA on NELL995, and it is still 1% worse than M-Walk, which uses MCTS for training and testing. Third, we analyze the performance of M-Walk under different numbers of MCTS rollout simulations and different search horizons on WN18RR dataset, with results shown in Figure 5(a). We observe that M-Walk outperforms the strong baseline ConvE by 4.6–10.9% in samples that require 2 or 3 steps, while it is nearly on par for paths of length one. Therefore, M-Walk does better at reasoning over longer paths than ConvE. To examine this effect, we show in Figure 5(c)-top the HITS@K accuracies when the ground truth is in the candidate set.9 It shows that M-Walk has very high accuracy in this case, which is significantly higher than ConvE (80% vs 39.6% in HITS@1). These observations point to an important direction for improving M-Walk in future work: increasing the chance of covering the target by the candidate set. Finally, in Table 4, we show examples of reasoning paths found by M-Walk.10 Reinforcement Learning Recently, deep reinforcement learning has achieved great success in many artificial intelligence problems [17, 24, 25]. Note that the former is constructed from the visit counts of all the edges connected to the MCTS root node; it only uses information near the root node to improve the policy. Recent approaches have demonstrated limitations of these prior approaches: they suffer from cascading errors when dealing with compositional (multi-step) relationships [10]. Hence, recent works [8, 18, 10, 15, 30] have proposed approaches for injecting multi-step paths such as random walks through sequences of triples during training, further improving performance on KBC tasks. IRN [23] and Neural LP [40] explore multi-step relations by using an RNN controller with attention over an external memory. Empirically, our proposed algorithm outperforms both DeepPath and MINERVA in the KBC benchmarks.11 We developed an RL-agent (M-Walk) that learns to walk over a graph towards a desired target node for given input query and source nodes. Furthermore, we also performed extensive experimental analysis to understand M-Walk. 11A preliminary version of M-Walk with limited experiments was reported in the workshop paper [22].",soundness_1000_section_model,M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines.","Summary of the paper:  - This paper proposes a walk-based method for knowledge graph completion (KGC).  - For triples (s, q, o), an agent is trained so that given s and q, it traverses a path from the initial node s to the   terminal node t in a knowledge graph.  - This problem is formulated as that of Q-learning using RNN to model policy and Q functions, but the issue is the   sparseness of rewards.  - Monte Carlo tree search is used to overcome this issue.  Pros:  - The paper is clearly written.  - The approach is natural and sound.  - Very good performance compared with existing KGC methods.  - Ablation study with PG-Walk and Q-Walk is nice.  Cons:  - The work is mostly empirical, without much theoretical justification (admittedly, as in many deep learning papers).  - Not much novelty or surprise, given the success of AlphaGo.  This is a type of paper in which a successful technique   in other fields is applied to a different field.  I could not see exactly what is the novelty/creativity in terms of   how MCTS is applied for this specific problem of KGC.  - No analysis on running time is provided.   General Comments:  This paper is well written (but I would like the content of Section 4.2 to be more organized --- Page 7 consists of a single paragraph now!).  The novelty of the proposal is somewhat questionable, because reinforcement learning has already been introduced for KGC, and the application of MCTS naturally comes to mind.  Although this work is mostly empirical and little theoretical contribution is provided, it still achieves the state-of-the-art performance, and that could be a merit for its publication.  Questions:  * Related work  MCTS has been introduced in walk-based KGC by Sheng et al. in ICLR 2018 Workshop (ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search), and this work must be cited.  Please elaborate on how M-Walk differs from ReinforceWalk as much as you can tell from the extended abstract.  * Experiments:  - Dataset:  Is there any reason the comparison is done with NELL995 and WN18RR?  FB15k is also the standard dataset.  - Baselines  Why do the set of compared methods differ between datasets?  In particular, we know that TransE and TransR are somewhat weaker compared with more recent bilinear models, such as ComplEx, ANALOGY, and ConvE.  - Evaluation metrics  Also, why is the MAP score used on NELL995 whereas HITS@/MRR are used on WN18RR?  I have nothing against MAP scores, but typically HITS@/MRR are reported in literature on KGC even on the NELL-995 dataset (e.g., the MINERVA paper); using different metrics makes it difficult to validate the experimental results.  - Figure 6(a)  Is there any explanation as to why accuracy drops as the number of roll outs are increased to 256?  * Application to classification task  The method is only concerned with prediction of knowledge triple in which only one entity is missing. Can the proposed method be applied to the triple classification task where the truth of a given triple must be predicted? This is more or less straightforward in score-function-based models (provided that a suitable threshold can be determined), but I cannot see how this can be addressed with walk-based methods in general.  * Running time  Training and inference both involves roll-out, which raises concern about the running time of the algorithms.  How does M-walk fare against scoring-based methods (e.g., ComplEx) or MINERVA in terms of running time?  Please provide the time required for training and inference.  I am curious about inference time in particular. "," The paper is well written and contains a thorough experimental design. The experiments are convincing and show that M-Walk is able to learn faster and better policies than other graph-walking methods.The paper is clearly written and the results are impressive. However, the paper does not address the problem of learning to walk on a graph with sparse rewards, and does not provide any theoretical results to support this claim. It would be better if the authors could provide some insights on why this is a problem that needs to be addressed.",92
16,837,hybrid,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art. Since the introduction of the “AlexNet” architecture [14] in the 2012 ImageNet competition, ConvNets have dominated state-of-the-art performance across a variety of computer vision tasks, including object-detection, image segmentation, image classification, face recognition, human pose estimation and tracking. In conjunction with these advances as well as the evolution of network architectures, several design best practices have emerged [8, 21, 23, 24]. This functionality can be achieved by stacking many small filters or using large filters in the network; notably, the first choice can be implemented with fewer operations (faster, fewer parameters) and also allows inclusion of more nonlinearities. Third, dimensionality reduction (1×1 convolutions) before spatially aggregating filters (e.g.3×3) is supported by the fact that outputs of neighbouring filters are highly correlated and therefore these activations can be reduced before aggregation [23]. Fourth, spatial factorization into asymmetric filters can even further reduce computational cost and 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. Fifth, it is important to normalize the responses of each feature channel within a batch to reduce internal covariate shift [11]. To demonstrate the benefits of our proposed spatiotemporal ResNet architecture, it has been evaluated on two standard action recognition benchmarks where it greatly boosts the state-of-the-art. Yet another study compared several approaches to extending ConvNets into the temporal domain, but with rather disappointing results [13]: The architectures were not particularly sensitive to temporal modelling, with a slow fusion model performing slightly better than early and late fusion alternatives; moreover, similar levels of performance were achieved by a purely spatial network. Another research branch has investigated combining image information in network architectures across longer time periods. That approach takes advantage of the local spatial similarity in images; however, it only yields a minor increase over their baseline, which is a two-stream VGG-16 ConvNet [21] used as the input to their convolutional RNN. Batch normalization [11] and ReLU [14] are applied after each convolution; the network does not use hidden fc, dropout, or max-pooling (except immediately after the first layer). In preliminary experiments we found that direct connections between identical layers of the two streams led to an increase in validation error. We conjecture that these results are due to the large change that the signal of one network stream undergoes after injecting a fusion signal from the other stream. Therefore, we developed a more subtle alternative solution based on additive interactions, as follows. To enable learning of spatiotemporal features at all possible scales, this modification is applied before the second residual unit at each spatial resolution of the network (indicated by “skip-stream” in Table 1), as exemplified by the connection at the conv5_x layers in Fig. Based on the above observations, we developed a novel approach to temporal residual connections that builds on the ConvNet design guidelines of chaining small [21] asymmetric [10, 23] filters, noted in Sec. This allows the straightforward use of standard two-stream ConvNets that have been pre-trained on large-scale datasets e.g.to leverage the massive amounts of training data from the ImageNet challenge. From left to right, top to bottom, the first row shows the convolutional and pooling building blocks, with the filter and pooling size shown as (W ×H × T,C), denoting width, height, temporal extent and number of feature channels, resp. In the last two rows we show the output size of these metalayers as well as the receptive field on which they operate. We also apply random cropping and scale augmentations: We randomly jitter the width and height of the 224× 224 input frame by ±25% and also randomly crop it from a maximum of 25% distance from the image borders. We found that this strategy significantly reduces the training duration with the drawback that each loss does not capture all available information. For our final model, we equip the spatiotemporal ResNet with a temporal max-pooling layer after pool5 (see Table 1, temporal average pooling led to inferior results) and continue training as above with the learning rate starting from 10−4 for 2K iterations followed by 10−5. For fair comparison, we follow the evaluation procedure of the original two-stream work [20] by sampling 25 frames (and their horizontal flips). However, rather than using 10 spatial 224× 224 crops from each of the frames, we apply fully convolutional testing both spatially (smallest side rescaled to 256) and temporally (the 25 frame-chunks) by classifying the video in a single forward pass, which takes ≈250ms on a Titan X GPU. Second, we consider HMDB51 [15], which has 6766 videos that show 51 different actions and generally is considered more challenging than UCF0101 due to the even wider variations in which actions occur. For both datasets, we use the provided evaluation protocol and report mean average accuracy over three splits into training and test sets. Interestingly, research in neuroscience also suggests that the human visual cortex is equipped with connections between the dorsal and the ventral stream to distribute motion information to separate visual areas [3, 27]. We achieve this by simply averaging the L2-normalized SVM scores of the FV-encoded IDT descriptors (i.e.HOG, HOF, MBH) [29] with the L2-normalized video predictions of our ST-ResNet*, again without softmax normalization. These relatively larger performance decrements again underline that our approach is better able to capture the available dynamic information, as there is less to be gained by augmenting it with IDT. Still, there is a benefit from the hand-crafted IDT features even with our approach, which could be attributed to its explicit compensation of camera motion.",soundness_1000_section_model,Spatiotemporal Residual Networks for Video Action Recognition,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art.","The paper presents a novel architecture that 1) combines residual networks with two-stream convolutional networks, and 2) injects connections from the motion stream to the appearance stream, to be able to capture spatio-temporal features. The paper shows experiments in both of the main action recognition datasets, achieving state-of-the-art accuracy in both. Overall I think the paper is great: good idea, careful experimentation, great results and clearly written.   Although the basic components of the architecture are pre-existing, I think that the high performance and careful experimentation and description make it a very useful contribution.   I only miss some experiments to visualize what kinds of spatio-temporal features are being learned.   Since large temporal windows are important (278-280) I would add a relevant reference: Long-term Temporal Convolutions for Action Recognition, Gul Varol, Ivan Laptev, Cordelia Schmid  Small typos:  (L 187)  ""resp.""  (L 255) ""UCF0101""  "," The paper is well written and the results are interesting and mathematically sound. However, the main weakness of the paper is that it spends a lot of time on the big picture, rather than on showing experimental results in relation to the spatiotemporal domain, which is the main contribution of this paper. It is therefore difficult to reproduce the results solely based on the description. It would be better if the authors could provide some intuition on why this is a good idea and how we might want to approach this problem in the future.",96
17,22112,hybrid,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values. While a large body of the machine learning literature is built upon the assumption of having access to complete datasets, in many real-world problems only incomplete datasets are available. Single imputation methods try to replace each missing value with a plausible value that is the best fit given the value of other correlated features and knowledge extracted from the dataset (Hastie et al., 1999; Anderson, 1957). While MI offers interesting statistical insights about the reliability of analysis on incomplete data, the insight is imprecise as it is mainly concerned about the population of data samples rather than individual instances. While these methods are easy to implement and analyze, they often fail to capture the complex feature dependencies as well as structures present in many problems. For instance, Mattei & Frellsen (2018) suggested a method based on deep latent variable models and importance sampling that offers a tighter likelihood bound compared to the standard VAE bound. Also, these methods are often complicated to be applied in practical setups by practitioners. A few exceptions exist such as Bayesian models and decision trees that permit direct analysis on incomplete data (Nielsen & Jensen, 2009; Zhang et al., 2005). However, given an incomplete training dataset and without any known causal structure as a priori, learning such models is a very challenging problem with the complexity of at least NP-complete to learn the network architecture in addition to an iterative EM optimization to learn model parameters (Darwiche, 2009; Neapolitan et al., 2004). Similarly, for the second part, we are interested in obtaining a distribution over the possible target assignments and the confidence of each class rather than maximum likelihood class assignments. Instead, we use recent advances in GAN stabilization and training to improve the training process (see Section 3.4). Additionally, in our experiments, we provide supporting evidence that this simple loss function enables us to sample from the conditional distribution and prevents biased inclinations toward distribution modes. For instance, for a specific test sample at hand, if a certain missing feature is a strong indicator of the target class, we would like to observe the impact of different imputations for that feature on the final hypothesis. Input: G (trained imputer), D (dataset) Output: Fθ (trained predictor) foreach Training Epoch do foreach (xi, ki, yi) in D do z ∼ N(0, I) x̂i ← ki xi+(1−ki) G(xi,ki, z) ypredi ← Fθ(x̂i) loss← L(yi, ypredi ) Backpropagate loss Update Fθ Algorithm 2: Estimating target distributions. The first assumption is consistent with the theoretical analysis of generative adversarial networks that they can converge to the true underlying distribution (Arora et al., 2018; Liu et al., 2017). It should be noted that, while Zhang et al.(2018) suggests using a single self-attention layer in the middle of the network, we observed consistent improvements by inserting multiple self-attention layers before each residual block within the network. We explored best TTUR learning-rate settings from the set of {0.001, 0.0005, 0.0001, 0.00005}. To evaluate the proposed method we use CIFAR-10 (Krizhevsky & Hinton, 2009) as an image classification dataset as well as three non-image datasets: UCI Landsat (Dua & Graff, 2017)2, MIT-BIH arrhythmia (Moody & Mark, 2001), and Diabetes classification (Kachuee et al., 2019) 3. Also, while different encoding and representation methods are suggested in the literature to handle categorical features (Jang et al., 2016; Nazabal et al., 2018), in this paper, we take the simple approach of encoding categorical variables using one-hot representation and smoothing them by adding Gaussian noise with zero mean and variance equal to 5% of feature variances. We would like to note that while the suggested solution in this paper is readily compatible with MAR structures, in our experiments, to simplify the presentation of results and to have a fair comparison with other work that does not support the MAR assumption, we limited the scope of our experiments to MCAR. Furthermore, to simulate incomplete datasets and to make sure the same features are missing without explicitly storing masks, we use hashed feature vectors to seed random number generators used to sample missing features. More detail is provided in Appendix C. Fréchet inception distance (FID) (Heusel et al., 2017) score is used to measure the quality of missing data imputation in experiments with images5. We also considered using root means squared error (RMSE); however, we decided not to use this measure as we observed an inconsistent behavior using RMSE in our comparisons as RMSE favors methods that show less variance rather than realistic and sharp samples from the distribution. We compare our results with MisGAN (Li et al., 2019) and GAIN (Yoon et al., 2018) as the state of the art imputation algorithms based on GANs as well as basic denoising autoencoder (DAE) (Vincent et al., 2008) and multiple imputation by chained equations (MICE) (Buuren & Groothuis-Oudshoorn, 2010) as baselines. Due to scalability issues, we were only able to use MICE for the smaller non-image datasets. One possible explanation for this behavior might be the fact that GAIN has an MSE loss term acting similar to an autoencoder loss smoothing noisy missing pixels. Regarding the MIT-BIH experiemts, GI outperforms other work for missing rates more than 30% while achieving similar accuracies to GAIN for lower missing rates. As it can be seen from the plots, GI provides results closest to the ideal case of having average confidence values equal to average accuracies. In order to provide further insight into the operation of GI and how imputations can potentially influence the outcomes of predictions, we conduct experiments on a synthesized dataset. One possible explanation could be the fact that imputing missing data with a uniform structure can be done by processing local regions and does not require attending to different distant regions across the image. From Table 6 it can be inferred that as the rate of missingness increases, the benefits of the suggested predictor algorithm increase significantly.",soundness_1000_section_model,Generative Imputation and Stochastic Prediction,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values.","This paper proposes a method to impute missing features using a generative model and train a predictive model on top of imputed dataset to improve classification results. They first train a GAN model where the generator outputs an imputed representation of the input and discriminator is trained to predict if an individual features (such as a pixel) is imputed or not. Given the generator and incomplete sample, they train a predictor using the output of the generator, imputed sample, as input. Their main contribution is using a MC averaging to compute the prediction by repetitively sampling from the noise variable, z, and generating different imputations from generator. They show that the proposed model improves upon the previous SOTA on final classification performance.

Overall the paper is clearly written. But I do feel it is a bit incremental over the GAIN approach. The overall GAN architecture is very similar to GAIN's and although stochastic prediction shows clear improvements it is a bit straightforward. However, I think the uncertainty of the imputations and its effect on the final prediction is interesting. I suggest the authors to extend this part with more detailed analysis.

There are several parts that are confusing/missing in the paper:

- In GAIN, they use a hint vector as an input to the discriminator. They show that without the hint vector, there is no unique solution (this is shown without the MSE loss). The authors do not use this vector in their approach (as in Figure 1) and it is not clear to me if it causes any instabilities or if multiple experiments yield similar results or if the stochastic prediction benefits from this.
- On what type of examples GI is more accurate than other models? Since stochastic prediction is the main difference from GAIN, is this related to the multi-modality of the noisy examples?
- Can you explain the difference between the results in Figure-7 and Table-2? Results between the two mismatch.
- I think the statement in the first paragraph in Section 4.4 that ""MSE loss term would act as a denoising loss smoothing noisy missing pixels"" could be misleading. MSE is used with mask in GAIN, hence it only applies to the observed features during training. Its effect on smoothing noisy missing pixels is not clear.


I think the paper would benefit if the authors could explain/show:
- Increasing the missing rate would also increase the possibility that the ground truth be a more multi-modal distribution. Especially in rectangular generation part where it can remove a complete object. Does stochastic averaging benefit more in this case?"," The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures.The paper is well written and the results are interesting and mathematically sound. However, I am not convinced by the empirical results as they seem to be quite strong in the sense that they do not match the theoretical results and do not seem to match the experimental results in the paper. The paper is clearly written, and the experiments seem to show the effectiveness of the proposed method.",92
18,18864,hybrid,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting. Recent works have used this framework to achieve impressive feats such as learning to classify using one labeled image per class (Snell et al., 2017; Finn et al., 2017), learning unsupervised update rules that generalize to different domains (Metz et al., 2018), and accelerating training procedures that are millions of steps long (Flennerhag et al., 2018). DIMCO generalizes well to novel datasets because its learning objective encourages the model to compactly use all of its degrees of freedom, thus enabling it to effectively compare datapoints while only requiring a small number of bits per datapoint. Derive a generalization bound for meta-learning that makes the tradeoff between taskspecific and task-wise overfitting concrete. Notably, we suggest that certain previous meta-learning methods may have benefitted from implicit regularization. We present experiments in Section 5 and conclude the paper with a discussion about limitations and future directions of our approach in Section 6. This can be problematic since in many works, the task-specific learning algorithm is usually just a byproduct of whatever clever meta-learning loss was proposed. We propose a factorized discrete representation scheme which enables us to represent discrete distributions with exponentially fewer parameters compared to listing the probability of each event. We similarly perform image retrival by mapping each support image to its most likely code (10) and for each query image retrieving the support image that has highest (11). Regularizing Meta-Learners The ability to generalize to novel datasets is critical in meta-learning benchmarks, and even more so in benchmarks such as Meta-Dataset (Triantafillou et al., 2019), where a model is tested on datasets from an unseen domain. The following works have reported benefits from reducing the number of such task-specific parameters: Lee & Choi (2018) learns a subset of the full network to alter during task-specific learning, Rusu et al.(2018) explicitly represents each task with a low-dimensional latent space, and Zintgraf et al.(2018) alters only a pre-specified subset of the full network during task-specific learning. We showed through Theorem 1 that restricting inner-loop expressivity reduces the generalization gap; this provides theoretical understanding to this consensus that meta-learning models with simple task-specific learners generalize to new tasks more easily. Also related is the deterministic information bottleneck (Strouse & Schwab, 2017) which extends the information bottleneck by minimizing H(X̃) rather than I(X̃;X). These three approaches to generalization are related via the chain of inequalities I(X̃;X) ≤ H(X̃) ≤ log |X̃|, which is tight when X̃ is an efficient code. Recent deep learning methods directly learn discrete representations, by learning variational autoencoders with discrete latent variables (Rolfe, 2016; van den Oord et al., 2017; Razavi et al., 2019) or maximizing the mutual information between representation and 1 In practice, we add log probabilities instead of multiplying probabilities for numerical stability. Additionally, their method solves a minimum cost flow problem within each batch to find the locally optimal code, whereas DIMCO is able to directly compute its loss function. We randomly initialize weights for the 4-layer convnet and use pretrained weights for the Inception network This experiment empirically verifies whether mutual information I(X̃;Y ) is a reasonable metric for quality of representation. We observed similar trends when training with with previously proposed loss functions: we visualize these results in Figure 5 of the appendix due to space constraints. Results in Figure 3 show that the compact code of DIMCO takes roughly an order of magnitude less memory for similar performance to N-pair loss, and requires less query time as well. We additionally note that DIMCO is able to train using large backbones without significantly overfitting, whereas experiments reported in Mishra et al.(2017) indicate that MAML (Finn et al., 2017) overfits tremendously when using a deeper backbone. This challenging experimental setup measures how much generalizable information the model can extract from a limited set of datasets; it can be seen as the meta-learning analogue of measuring the performance of a classifier trained with a small dataset. We report the average and standard deviation of the top 5 results of a random hyperparameter search (see appendix for details). Towards Explicit Meta-Regularization In Section 4, we have suggested with analogy to Theorem 1 that the benefits of some previous meta-learning methods can be attributed to implicitly being regularized by reducing the expressivity of their task-specific learners. In future work, we would like to explore explicit meta-regularization schemes that can be applied to other problems (regression, reinforcement learning etc.) Along with showing that the traditional support/query split is not strictly necessary, we demonstrated in Section 5.4 that removing it has the benefit of enabling meta-learning in datasets having one image per class. (12) Assuming that the approximate distribution q(·) is sufficiently close to p(y|x̃), minimizing (12) can be seen as arg min xent(Y, X̃) ≈ arg minEy∼Y,x̃∼X̃ [ − log p(y|x̃) ] (13) = arg minH(Y |X̃) = arg max I(X̃;Y ), (14) where the last equality uses the fact that H(Y ) is independent of model parameters. (24) We similarly bound the error caused by estimating L with a finite number of tasks sampled from τ . Combining equations (26, 24), we have with high probability∣∣∣∣L(τ, θ)− − 1 n n∑ i=1 Î(X̃(XT i , θ);YT i) ∣∣∣∣ (27) ≤ ∣∣∣L(τ, θ)− L̂(τ, θ)∣∣∣+O( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (28) ≤O (√ dΘ n log n dΘ ) +O ( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (29) Hardware Every experiment was conducted on a single Nvidia V100 GPU with CUDA 9.2. Each experiment was performed with different fixed initial seeds; we manually fix seeds with manual_seed() for python, pytorch, and numpy. We additionally show in Figure 5 the correlation between 1-shot accuracies, Recall@1, and NMI using three previously proposed losses (triplet, npair, protonet).",soundness_1000_section_model,Discrete InfoMax Codes for Meta-Learning,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting.","This paper presents DIMCO, a meta-learner that is trained by maximizing mutual information between a discrete data representation and class labels across tasks. DIMCO is inspired by an information theoretic lower bound on the generalisation gap for meta-learning, which the authors argue identifies overfitting in the task learner as the bottleneck. 

This work proposes to constrain a learner to output discrete codes that are learned to capture the mutual information with class labels. While the idea of using discrete codes is interesting, its presentation in the manuscript is not well motivated and at times hard to follow. This makes it challenging to evaluate the novelty, validity, and generality of the proposed approach. Meanwhile, the empirical evaluation is somewhat lacking. Thus, I do not believe this work is ready for publication in its current form.

Detailed comments

My main concern is with respect to the primary contribution of this paper, a generalisation bound on meta-learning. The bound appears to be on a multi-task loss without task adaptation, and thus the claims made with respect to the theorem seem somewhat over-reaching. I also believe the VC-dimensionality of the encoder is missing in Eq. 4? If so, this changes the interpretation since the length of the code and the expressivity of the encoder are interrelated. Further, I would welcome a deeper analysis of the theorem and its implications. The current interpretation states that the number of tasks is independent of the size of each task, hence given many tasks, using minimal representations is an effective approach to meta-generalisation. Yet minimal representations is a well-known idea and has features in several works that use mutual information as a regularizer, most notably works on the Information Bottleneck. 

Another reservation I have is the use of mutual information between encoder representations and class labels as a loss function (Eq. 1). It lacks context and a proper motivation, especially since the analysis of [1] shows that the loss function in Eq. 1 is the cross-entropy objective. The authors make a similar analysis in Appendix A and argue that Eq. 1 differs in that cross-entropy is an approximation because it adds a parametrized linear layer on top of \tilde{X}. Thus, in the absence of that layer they collapse to the same objective. As DIMCO itself directly extract class label predictions from \tilde{X}, I fail to see a difference between the loss in Eq. 1 and a cross-entropy objective. 

The main motivation behind their loss objective is that it does not require a support / query set. This does not seem to be a feature of the mutual information objective itself, but rather a choice made by the authors. I would have liked a deeper discussion of this seeing as the authors make it a central tenet of DIMCO. Prior works use a support set as a principled means of doing meta-learning: meta-training explicitly takes into account that at test time, the learner will be given a small support set from which to learn how to query points. As far as I understand, DIMCO does not take this into account during meta-training.  At meta-test time however, DIMCO does use a support set to map query points (Eqs. 10 and 11). Why should we break protocols between meta training and testing? Are there any downsides to doing so?  

Empirically, I find the CUB experiment compelling but would welcome some ablations. What are the trade-offs between p and d? Can DIMCO outperform N-pair when number of bits are unconstrained?

miniImagenet is a standard benchmark in few-shot learning, but I am unable to find a results table - could the authors please provide results on the standard setup so that the method can be compared against known baselines? Further, would the results currently presented hold in a N-way-5-shot setup?

As for the constrained version of miniImagenet that the authors propose, I am not convinced this is an interesting protocol. In general, the miniImagenet task distribution is created by N-way permutations of the classes in the meta-set (e.g. meta-training tasks are combinations of the 64 classes in the meta-training set). By keeping the number of classes constant but reducing the number of images per class, this protocol is not reducing samples per task: a task is always defined as 5/20-way-1-shot (Fig. 4). Instead, the effect should be that tasks are in (greater) violation of the task i.i.d. assumption. Thus, I question whether this setup demonstrates the trade-offs the authors present in Theorem 1 and whether the results can be interpreted in light of it. 

Finally, that both experiments are image-based raises questions as to the generality of the method. The paper could be considerably strengthened by evaluating DIMCO on a non-image task, or if not discuss the method’s limitations.  

The idea of discrete codes for few-shot classification is interesting and sufficiently novel, I am likely to increase my score if my concerns are addressed and the experimental section is strengthened.

Further questions and comments:

- I am unable to parse Eq. 11 - what does the notation \prod_i p_{\tilde{x}_i, i} mean? 
- It is unnecessarily hard to follow the proof of theorem 1. It would help the reader if you restated relevant definitions, such as Eq. 1, since the difference with Eq. 23 is very subtle. It would also be helpful to explain how the summand in Eq. 25 differs from either, and Eq. 26 could be expanded or briefly explained after the derivation. 
-  Because DIMCO is trained with backpropagation, the interpretation of Eq. 5 as d independent events seems invalid. How does it affect the method if they are not independent?
- Overloading X and Y as both random variables and mini-batch samples creates unnecessary confusion. I believe the objective in Eq. 1 is approximated, not calculated exactly? For instance, the mutual information in Eq. 8 is with respect to a mini-batch, so should it not be \hat{I}? 
- p^j_{ik} in Eq. 9 is undefined.
- Eq. 9 is interpreted as an exact entropy, however it appears to be a mini-batch approximation to the true entropy? 

References 
[1] Achille and Soatto. Emergence of Invariance and Disentanglement in Deep Representations. JMLR. 2018."," The paper is clearly written and the results are interesting and mathematically sound. However, I am not convinced by the theoretical results because the experiments are not very thorough and the overall results are quite significant. The paper should also include some discussion on why the proposed method is better than simply using the discriminator to learn discrete codes and why it is better to use a stochastic encoder to generalize the model to larger datasets.Overall, the paper is well-written and the experiments seem to be well-performed.",88
19,13063,hybrid,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all. But in each case, the network was designed and tuned specifically for the problem at hand. Natural language processing models have been shown to benefit from a multi-task approach a long time ago (Collobert & Weston, 2008), and recently multi-task machine translation models (MinhThang Luong, 2015) have even been shown to exhibit zero-shot learning when trained on multiple languages (Melvin Johnson, 2016). Speech recognition has also been shown to benefit from multi-task training (Seltzer & Droppo, 2013), as have some vision problems, such as facial landmark detection (Zhang Z., 2014). But no competitive multi-task multi-modal model has been proposed, so the above question remains unanswered. (3) COCO image captioning dataset (Lin et al., 2014), used for image captioning. (4) WSJ parsing dataset (Marcus et al., 1999), used for constituency parsing. While the MultiModel is only a first step and will be improved in the future, two key insights are crucial to making it work at all and are our main contributions. We design modality nets to be computationally minimal, promoting heavy feature extraction and ensuring that the majority of computation is performed within the domain-agnostic body of the model. Two design decisions were important: • The unified representation is variable-size. While a fixed-size representation is tempting and easier to implement, it creates a bottleneck and limits the performance of the model. For example, separable convolutions were introduced in the Xception architecture (Chollet, 2016) and were not applied to text or speech processing before. We find that each of these mechanisms is indeed crucial for the domain it was introduced, e.g., attention is far more important for languagerelated tasks than for image-related ones. As already said above, the encoder and decoder are constructed using 3 key computational blocks to get good performance across different problems: (1) Convolutions allow the model to detect local patterns and generalize across space. We refer the reader to (Chollet, 2016) for a complete definition; here we will denote a depthwise separable convolution with weights Wh×w corresponding to f kernels of size h × w applied to an input tensor x with stride s and dilated by a factor d (see (Yu & Koltun, 2015)) as SepConvd,s,f (W,x). The source tensor is finally passed through two different pointwise convolutions to generate the memory keys K and values V and the query keys, memory keys and memory values are used to apply the attention mechanism between the self-attended target and the source (see Figure 3). On the output side, the language modality takes the decoded output of the body and performs a learned linear mapping, followed by a Softmax, resulting in a probability distribution over the token vocabulary. The spectral modality does not perform any striding along the frequency bin dimension, preserving full resolution in the spectral domain. Convolutional architectures yielded good results on word-level neural machine translation starting from (Kalchbrenner & Blunsom, 2013) and later in (Meng et al., 2015). These early models used a standard RNN on top of the convolution to generate the output and had a bottleneck there that hurt performance, especially on longer sentences, similarly to the limitations of RNN sequence-to-sequence models without attention (Sutskever et al., 2014; Cho et al., 2014). This idea, introduced in WaveNet (van den Oord et al., 2016) and also used in MultiModel (see above) significantly improves efficiency. Depthwise separable convolutions were first studied by Sifre (Sifre & Mallat, 2013) and later they were used to get good results on large-scale image classification with Xception (Chollet, 2016). We focused our experiments so as to answer the following questions: (1) How far is the MultiModel trained on 8 tasks simultaneously from state-of-the-art results? Especially the 4 translation problems behave very similarly, so we decided to not include them all in each comparison but we focused on the more varied problems instead. The results we achieve are similar to the ones task-specific models get without heavy tuning, e.g., on English-French translation we improve on the recent Extended Neural GPU results (Kaiser & Bengio, 2016). Since we are comparing different instantiations of the same model, we report two internal metrics: the negative log-perplexity and per-token accuracy (measured on the development set). As can be seen from the results in Table 2, the joint 8-problem model performs similarly to single-model on large tasks, and better, sometimes significantly, on tasks where less data is available, such as parsing. The difference in performance is significant, and since we use both dropout and early stopping, we conjecture that it is not related to over-fitting. In fact, one could expect that removing these blocks will improve performance on ImageNet alone if they were truly useless for this task. In contrast, we see in Table 4 that these blocks either don’t affect or slightly improve performance. This leads us to conclude that mixing different computation blocks is in fact a good way to improve performance on many various tasks. We believe that this treads a path towards interesting future work on more general deep learning architectures, especially since our model shows transfer learning from tasks with a large amount of available data to ones where the data is limited.",soundness_1000_section_model,Large Scale Multi-Domain Multi-Task Learning with MultiModel,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.","The paper presents a multi-task, multi-domain model based on deep neural networks. The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition. The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic. The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task.

The paper is well written and easy to follow.

The contributions of the paper are novel and significant. The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring. The experiments clearly show the viability of the approach and give interesting insights. This is surely an important step towards more general deep learning models. 

Comments:

* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database. Moreover, some databases could be used for different tasks, such as WSJ or ImageNet.

* The training procedure of the model is not explained in the paper. What is the cost function and what is the strategy to train on multiple tasks ? The paper should at least outline the strategy.

* The experiments are sufficient to demonstrate the viability of the approach, but the experimental setup is not clear. Specifically, there is an issue about the speech recognition part of the experiment. It is not clear what the task exactly is: continuous speech recognition, isolated word recognition ? The metrics used in Table 1 are also not clear, they should be explained in the text. Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used. Information about the detailed setup is also lacking, specifically which test and development sets are used (the WSJ corpus has several sets).

* Using raw waveforms as audio modality is very interesting, but this approach is not standard for speech recognition, some references should be provided, such as:
P. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26–30.
D. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE.
T. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals. Learning the Speech Front-end With Raw Waveform CLDNNs. Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015.

Revised Review:
The main idea of the paper is very interesting and the work presented is impressive. However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks. The experiments are interesting, except for the WSJ speech task, which is almost meaningless. Indeed, it is not clear what the network has learned given the metrics presented, as the WER on WSJ should be around 5% for speech recognition.
I thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition. A simpler speech task such as Keyword Spotting could also be investigated.
"," Overall, the paper is well written and contains a thorough experimental design, but lacks of high-level discussion on why it works so well on some problems and how it could do better on others. The paper is clearly written, and the experiments seem to be well-performed. However, there is no discussion of why the proposed model is better than the ones already proposed in the paper, or how it relates to competing models in terms of memory usage and memory complexity.The paper is generally well-written and contains thorough experiments, but lacking of discussion about why it is better to combine multiple models into a single model.",107
20,6299,hybrid,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines. The problem can also be understood as constructing a function f(G, nS , q) to predict nT , where the functional form of f(·) is generally unknown and has to be learned from a training dataset consisting of samples like (nS , q, nT ). Since nT is unknown, the problem cannot be solved by conventional search algorithms such as A⇤-search [11], which seeks to find paths between the given source and target nodes. For the KBC example in Figure 1(a), having access to the current node nt = Hawaii alone is not sufficient to know that the best action is moving to nt+1 = USA. Second, to address the challenge of sparse rewards, M-Walk exploits the fact that the MDP transition model is known and deterministic.2 Specifically, it combines Monte Carlo Tree Search (MCTS) with the RNN to generate trajectories that obtain significantly more positive rewards than using the RNN policy alone. The rest of the paper is organized as follows: Section 3 develops the M-Walk agent, including the model architecture, the training and testing algorithms.3 Experimental results are presented in Section 4. To see this, we observe from Figure 1(b) that once an action at (i.e., an edge in Ent or “STOP”) is selected, the next node nt+1 and its associated Ent+1 and Nnt+1 are known. The objective is to learn a policy that maximizes the terminal rewards, i.e., correctly identifies the target node with high probability. Recall from Section 2 (e.g., (1)) that one challenge in applying RL to the graph-walking problem is that the state st nominally includes the entire history of observations. We will explain in Section 3.2 how such parameter sharing enables indirect updates to the policy ⇡✓ via Q-learning from off-policy data. To address this issue, we apply the same FCN f✓(·) to 5There could be alternative ways to compute the score, such as Score(n) = maxsT!n Q✓(sT , STOP). Inspired by this recursion, we propose using the GRU-RNN [4] to encode qt into a vector representation6: qt+1 = f✓q (qt, [hA,t, hat,t, nt+1]) with initialization q0 = f✓q (q, [0, 0, nS ]), where ✓q is the model parameter, and hat,t denotes the vector hn0,t at n0 = at. Three Glass Puzzle The Three Glass Puzzle [20] is a problem studied in math puzzles and graph theory. WN18RR [6] is created from the original WN18 [2] by removing various sources of test leakage, making the dataset more challenging. We use HITS@1,3 and mean reciprocal rank (MRR) as the evaluation metrics for WN18RR, and use mean average precision (MAP) for NELL995,7 where HITS@K computes the percentage of the desired entities being ranked among the top-K list, and MRR computes an average of the reciprocal rank of the desired entities. We compare against RL-based methods [38, 5], embedding-based models (including DistMult [39], ComplEx [32] and ConvE [6]) and recent work in logical rules (NeuralLP) [40]. For all the baseline methods, we used the implementation released by the corresponding authors with their best-reported hyperparameter settings.8 The details of the hyperparameters for M-Walk are described in Appendix B.2.2 of the supplementary material. We ran the experiments three times and report the means and standard deviations (except for PRA, TransE, and TransR on NELL995, whose results are directly quoted from [38]). We observed that the novel neural architecture of M-Walk contributes an overall 1% gain relative to MINERVA on NELL995, and it is still 1% worse than M-Walk, which uses MCTS for training and testing. Third, we analyze the performance of M-Walk under different numbers of MCTS rollout simulations and different search horizons on WN18RR dataset, with results shown in Figure 5(a). We observe that M-Walk outperforms the strong baseline ConvE by 4.6–10.9% in samples that require 2 or 3 steps, while it is nearly on par for paths of length one. Therefore, M-Walk does better at reasoning over longer paths than ConvE. To examine this effect, we show in Figure 5(c)-top the HITS@K accuracies when the ground truth is in the candidate set.9 It shows that M-Walk has very high accuracy in this case, which is significantly higher than ConvE (80% vs 39.6% in HITS@1). These observations point to an important direction for improving M-Walk in future work: increasing the chance of covering the target by the candidate set. Finally, in Table 4, we show examples of reasoning paths found by M-Walk.10 Reinforcement Learning Recently, deep reinforcement learning has achieved great success in many artificial intelligence problems [17, 24, 25]. Note that the former is constructed from the visit counts of all the edges connected to the MCTS root node; it only uses information near the root node to improve the policy. Recent approaches have demonstrated limitations of these prior approaches: they suffer from cascading errors when dealing with compositional (multi-step) relationships [10]. Hence, recent works [8, 18, 10, 15, 30] have proposed approaches for injecting multi-step paths such as random walks through sequences of triples during training, further improving performance on KBC tasks. IRN [23] and Neural LP [40] explore multi-step relations by using an RNN controller with attention over an external memory. Empirically, our proposed algorithm outperforms both DeepPath and MINERVA in the KBC benchmarks.11 We developed an RL-agent (M-Walk) that learns to walk over a graph towards a desired target node for given input query and source nodes. Furthermore, we also performed extensive experimental analysis to understand M-Walk. 11A preliminary version of M-Walk with limited experiments was reported in the workshop paper [22].",substance_1000_section_model,M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines.","Summary of the paper:  - This paper proposes a walk-based method for knowledge graph completion (KGC).  - For triples (s, q, o), an agent is trained so that given s and q, it traverses a path from the initial node s to the   terminal node t in a knowledge graph.  - This problem is formulated as that of Q-learning using RNN to model policy and Q functions, but the issue is the   sparseness of rewards.  - Monte Carlo tree search is used to overcome this issue.  Pros:  - The paper is clearly written.  - The approach is natural and sound.  - Very good performance compared with existing KGC methods.  - Ablation study with PG-Walk and Q-Walk is nice.  Cons:  - The work is mostly empirical, without much theoretical justification (admittedly, as in many deep learning papers).  - Not much novelty or surprise, given the success of AlphaGo.  This is a type of paper in which a successful technique   in other fields is applied to a different field.  I could not see exactly what is the novelty/creativity in terms of   how MCTS is applied for this specific problem of KGC.  - No analysis on running time is provided.   General Comments:  This paper is well written (but I would like the content of Section 4.2 to be more organized --- Page 7 consists of a single paragraph now!).  The novelty of the proposal is somewhat questionable, because reinforcement learning has already been introduced for KGC, and the application of MCTS naturally comes to mind.  Although this work is mostly empirical and little theoretical contribution is provided, it still achieves the state-of-the-art performance, and that could be a merit for its publication.  Questions:  * Related work  MCTS has been introduced in walk-based KGC by Sheng et al. in ICLR 2018 Workshop (ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search), and this work must be cited.  Please elaborate on how M-Walk differs from ReinforceWalk as much as you can tell from the extended abstract.  * Experiments:  - Dataset:  Is there any reason the comparison is done with NELL995 and WN18RR?  FB15k is also the standard dataset.  - Baselines  Why do the set of compared methods differ between datasets?  In particular, we know that TransE and TransR are somewhat weaker compared with more recent bilinear models, such as ComplEx, ANALOGY, and ConvE.  - Evaluation metrics  Also, why is the MAP score used on NELL995 whereas HITS@/MRR are used on WN18RR?  I have nothing against MAP scores, but typically HITS@/MRR are reported in literature on KGC even on the NELL-995 dataset (e.g., the MINERVA paper); using different metrics makes it difficult to validate the experimental results.  - Figure 6(a)  Is there any explanation as to why accuracy drops as the number of roll outs are increased to 256?  * Application to classification task  The method is only concerned with prediction of knowledge triple in which only one entity is missing. Can the proposed method be applied to the triple classification task where the truth of a given triple must be predicted? This is more or less straightforward in score-function-based models (provided that a suitable threshold can be determined), but I cannot see how this can be addressed with walk-based methods in general.  * Running time  Training and inference both involves roll-out, which raises concern about the running time of the algorithms.  How does M-walk fare against scoring-based methods (e.g., ComplEx) or MINERVA in terms of running time?  Please provide the time required for training and inference.  I am curious about inference time in particular. "," The experimental results on graph-walking benchmarks show that M-Walk outperforms the state-of-the-art in terms of performance and robustness to perturbation.The paper is well written and easy to follow. It would be better if the authors could provide more details and results on more complex problems such as hyper-parameters and hyperparameter sharing to better understand what is really important for the performance of the method. The paper is also well written, and the experiments are well-designed, but could be better.",80
21,837,hybrid,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art. Since the introduction of the “AlexNet” architecture [14] in the 2012 ImageNet competition, ConvNets have dominated state-of-the-art performance across a variety of computer vision tasks, including object-detection, image segmentation, image classification, face recognition, human pose estimation and tracking. In conjunction with these advances as well as the evolution of network architectures, several design best practices have emerged [8, 21, 23, 24]. This functionality can be achieved by stacking many small filters or using large filters in the network; notably, the first choice can be implemented with fewer operations (faster, fewer parameters) and also allows inclusion of more nonlinearities. Third, dimensionality reduction (1×1 convolutions) before spatially aggregating filters (e.g.3×3) is supported by the fact that outputs of neighbouring filters are highly correlated and therefore these activations can be reduced before aggregation [23]. Fourth, spatial factorization into asymmetric filters can even further reduce computational cost and 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. Fifth, it is important to normalize the responses of each feature channel within a batch to reduce internal covariate shift [11]. To demonstrate the benefits of our proposed spatiotemporal ResNet architecture, it has been evaluated on two standard action recognition benchmarks where it greatly boosts the state-of-the-art. Yet another study compared several approaches to extending ConvNets into the temporal domain, but with rather disappointing results [13]: The architectures were not particularly sensitive to temporal modelling, with a slow fusion model performing slightly better than early and late fusion alternatives; moreover, similar levels of performance were achieved by a purely spatial network. Another research branch has investigated combining image information in network architectures across longer time periods. That approach takes advantage of the local spatial similarity in images; however, it only yields a minor increase over their baseline, which is a two-stream VGG-16 ConvNet [21] used as the input to their convolutional RNN. Batch normalization [11] and ReLU [14] are applied after each convolution; the network does not use hidden fc, dropout, or max-pooling (except immediately after the first layer). In preliminary experiments we found that direct connections between identical layers of the two streams led to an increase in validation error. We conjecture that these results are due to the large change that the signal of one network stream undergoes after injecting a fusion signal from the other stream. Therefore, we developed a more subtle alternative solution based on additive interactions, as follows. To enable learning of spatiotemporal features at all possible scales, this modification is applied before the second residual unit at each spatial resolution of the network (indicated by “skip-stream” in Table 1), as exemplified by the connection at the conv5_x layers in Fig. Based on the above observations, we developed a novel approach to temporal residual connections that builds on the ConvNet design guidelines of chaining small [21] asymmetric [10, 23] filters, noted in Sec. This allows the straightforward use of standard two-stream ConvNets that have been pre-trained on large-scale datasets e.g.to leverage the massive amounts of training data from the ImageNet challenge. From left to right, top to bottom, the first row shows the convolutional and pooling building blocks, with the filter and pooling size shown as (W ×H × T,C), denoting width, height, temporal extent and number of feature channels, resp. In the last two rows we show the output size of these metalayers as well as the receptive field on which they operate. We also apply random cropping and scale augmentations: We randomly jitter the width and height of the 224× 224 input frame by ±25% and also randomly crop it from a maximum of 25% distance from the image borders. We found that this strategy significantly reduces the training duration with the drawback that each loss does not capture all available information. For our final model, we equip the spatiotemporal ResNet with a temporal max-pooling layer after pool5 (see Table 1, temporal average pooling led to inferior results) and continue training as above with the learning rate starting from 10−4 for 2K iterations followed by 10−5. For fair comparison, we follow the evaluation procedure of the original two-stream work [20] by sampling 25 frames (and their horizontal flips). However, rather than using 10 spatial 224× 224 crops from each of the frames, we apply fully convolutional testing both spatially (smallest side rescaled to 256) and temporally (the 25 frame-chunks) by classifying the video in a single forward pass, which takes ≈250ms on a Titan X GPU. Second, we consider HMDB51 [15], which has 6766 videos that show 51 different actions and generally is considered more challenging than UCF0101 due to the even wider variations in which actions occur. For both datasets, we use the provided evaluation protocol and report mean average accuracy over three splits into training and test sets. Interestingly, research in neuroscience also suggests that the human visual cortex is equipped with connections between the dorsal and the ventral stream to distribute motion information to separate visual areas [3, 27]. We achieve this by simply averaging the L2-normalized SVM scores of the FV-encoded IDT descriptors (i.e.HOG, HOF, MBH) [29] with the L2-normalized video predictions of our ST-ResNet*, again without softmax normalization. These relatively larger performance decrements again underline that our approach is better able to capture the available dynamic information, as there is less to be gained by augmenting it with IDT. Still, there is a benefit from the hand-crafted IDT features even with our approach, which could be attributed to its explicit compensation of camera motion.",substance_1000_section_model,Spatiotemporal Residual Networks for Video Action Recognition,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art.","The paper presents a novel architecture that 1) combines residual networks with two-stream convolutional networks, and 2) injects connections from the motion stream to the appearance stream, to be able to capture spatio-temporal features. The paper shows experiments in both of the main action recognition datasets, achieving state-of-the-art accuracy in both. Overall I think the paper is great: good idea, careful experimentation, great results and clearly written.   Although the basic components of the architecture are pre-existing, I think that the high performance and careful experimentation and description make it a very useful contribution.   I only miss some experiments to visualize what kinds of spatio-temporal features are being learned.   Since large temporal windows are important (278-280) I would add a relevant reference: Long-term Temporal Convolutions for Action Recognition, Gul Varol, Ivan Laptev, Cordelia Schmid  Small typos:  (L 187)  ""resp.""  (L 255) ""UCF0101""  "," The experiments are conducted on two standard action recognition benchmarks and the results seem to align with our intuition. However, the authors do not provide any analysis of the sensitivity of their approach to temporal residual connections, which could potentially cause confusion and extra experimentation for practitioners. The authors should provide more analysis of their method to better understand what is really important for the performance of the method. The paper is well written and easy to follow, and the experiments are well-designed, but the results are not convincing enough to support the claims.",95
22,22112,hybrid,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values. While a large body of the machine learning literature is built upon the assumption of having access to complete datasets, in many real-world problems only incomplete datasets are available. Single imputation methods try to replace each missing value with a plausible value that is the best fit given the value of other correlated features and knowledge extracted from the dataset (Hastie et al., 1999; Anderson, 1957). While MI offers interesting statistical insights about the reliability of analysis on incomplete data, the insight is imprecise as it is mainly concerned about the population of data samples rather than individual instances. While these methods are easy to implement and analyze, they often fail to capture the complex feature dependencies as well as structures present in many problems. For instance, Mattei & Frellsen (2018) suggested a method based on deep latent variable models and importance sampling that offers a tighter likelihood bound compared to the standard VAE bound. Also, these methods are often complicated to be applied in practical setups by practitioners. A few exceptions exist such as Bayesian models and decision trees that permit direct analysis on incomplete data (Nielsen & Jensen, 2009; Zhang et al., 2005). However, given an incomplete training dataset and without any known causal structure as a priori, learning such models is a very challenging problem with the complexity of at least NP-complete to learn the network architecture in addition to an iterative EM optimization to learn model parameters (Darwiche, 2009; Neapolitan et al., 2004). Similarly, for the second part, we are interested in obtaining a distribution over the possible target assignments and the confidence of each class rather than maximum likelihood class assignments. Instead, we use recent advances in GAN stabilization and training to improve the training process (see Section 3.4). Additionally, in our experiments, we provide supporting evidence that this simple loss function enables us to sample from the conditional distribution and prevents biased inclinations toward distribution modes. For instance, for a specific test sample at hand, if a certain missing feature is a strong indicator of the target class, we would like to observe the impact of different imputations for that feature on the final hypothesis. Input: G (trained imputer), D (dataset) Output: Fθ (trained predictor) foreach Training Epoch do foreach (xi, ki, yi) in D do z ∼ N(0, I) x̂i ← ki xi+(1−ki) G(xi,ki, z) ypredi ← Fθ(x̂i) loss← L(yi, ypredi ) Backpropagate loss Update Fθ Algorithm 2: Estimating target distributions. The first assumption is consistent with the theoretical analysis of generative adversarial networks that they can converge to the true underlying distribution (Arora et al., 2018; Liu et al., 2017). It should be noted that, while Zhang et al.(2018) suggests using a single self-attention layer in the middle of the network, we observed consistent improvements by inserting multiple self-attention layers before each residual block within the network. We explored best TTUR learning-rate settings from the set of {0.001, 0.0005, 0.0001, 0.00005}. To evaluate the proposed method we use CIFAR-10 (Krizhevsky & Hinton, 2009) as an image classification dataset as well as three non-image datasets: UCI Landsat (Dua & Graff, 2017)2, MIT-BIH arrhythmia (Moody & Mark, 2001), and Diabetes classification (Kachuee et al., 2019) 3. Also, while different encoding and representation methods are suggested in the literature to handle categorical features (Jang et al., 2016; Nazabal et al., 2018), in this paper, we take the simple approach of encoding categorical variables using one-hot representation and smoothing them by adding Gaussian noise with zero mean and variance equal to 5% of feature variances. We would like to note that while the suggested solution in this paper is readily compatible with MAR structures, in our experiments, to simplify the presentation of results and to have a fair comparison with other work that does not support the MAR assumption, we limited the scope of our experiments to MCAR. Furthermore, to simulate incomplete datasets and to make sure the same features are missing without explicitly storing masks, we use hashed feature vectors to seed random number generators used to sample missing features. More detail is provided in Appendix C. Fréchet inception distance (FID) (Heusel et al., 2017) score is used to measure the quality of missing data imputation in experiments with images5. We also considered using root means squared error (RMSE); however, we decided not to use this measure as we observed an inconsistent behavior using RMSE in our comparisons as RMSE favors methods that show less variance rather than realistic and sharp samples from the distribution. We compare our results with MisGAN (Li et al., 2019) and GAIN (Yoon et al., 2018) as the state of the art imputation algorithms based on GANs as well as basic denoising autoencoder (DAE) (Vincent et al., 2008) and multiple imputation by chained equations (MICE) (Buuren & Groothuis-Oudshoorn, 2010) as baselines. Due to scalability issues, we were only able to use MICE for the smaller non-image datasets. One possible explanation for this behavior might be the fact that GAIN has an MSE loss term acting similar to an autoencoder loss smoothing noisy missing pixels. Regarding the MIT-BIH experiemts, GI outperforms other work for missing rates more than 30% while achieving similar accuracies to GAIN for lower missing rates. As it can be seen from the plots, GI provides results closest to the ideal case of having average confidence values equal to average accuracies. In order to provide further insight into the operation of GI and how imputations can potentially influence the outcomes of predictions, we conduct experiments on a synthesized dataset. One possible explanation could be the fact that imputing missing data with a uniform structure can be done by processing local regions and does not require attending to different distant regions across the image. From Table 6 it can be inferred that as the rate of missingness increases, the benefits of the suggested predictor algorithm increase significantly.",substance_1000_section_model,Generative Imputation and Stochastic Prediction,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values.","This paper proposes a method to impute missing features using a generative model and train a predictive model on top of imputed dataset to improve classification results. They first train a GAN model where the generator outputs an imputed representation of the input and discriminator is trained to predict if an individual features (such as a pixel) is imputed or not. Given the generator and incomplete sample, they train a predictor using the output of the generator, imputed sample, as input. Their main contribution is using a MC averaging to compute the prediction by repetitively sampling from the noise variable, z, and generating different imputations from generator. They show that the proposed model improves upon the previous SOTA on final classification performance.

Overall the paper is clearly written. But I do feel it is a bit incremental over the GAIN approach. The overall GAN architecture is very similar to GAIN's and although stochastic prediction shows clear improvements it is a bit straightforward. However, I think the uncertainty of the imputations and its effect on the final prediction is interesting. I suggest the authors to extend this part with more detailed analysis.

There are several parts that are confusing/missing in the paper:

- In GAIN, they use a hint vector as an input to the discriminator. They show that without the hint vector, there is no unique solution (this is shown without the MSE loss). The authors do not use this vector in their approach (as in Figure 1) and it is not clear to me if it causes any instabilities or if multiple experiments yield similar results or if the stochastic prediction benefits from this.
- On what type of examples GI is more accurate than other models? Since stochastic prediction is the main difference from GAIN, is this related to the multi-modality of the noisy examples?
- Can you explain the difference between the results in Figure-7 and Table-2? Results between the two mismatch.
- I think the statement in the first paragraph in Section 4.4 that ""MSE loss term would act as a denoising loss smoothing noisy missing pixels"" could be misleading. MSE is used with mask in GAIN, hence it only applies to the observed features during training. Its effect on smoothing noisy missing pixels is not clear.


I think the paper would benefit if the authors could explain/show:
- Increasing the missing rate would also increase the possibility that the ground truth be a more multi-modal distribution. Especially in rectangular generation part where it can remove a complete object. Does stochastic averaging benefit more in this case?"," The experiments are well-designed and the results seem to align with our intuition. However, this is not tested in the paper and the authors do not provide any analysis about the distribution of missing values or how this distribution is affected by the imputation.The paper is well-written and well-argued, and the experiments show that the proposed method works well on CIFAR-10 and UCI-10 as well as the other datasets. The paper is also well-evaluated, and it would be great if the authors could provide some additional analysis on the distributions that these distributions may be based on.",98
23,18864,hybrid,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting. Recent works have used this framework to achieve impressive feats such as learning to classify using one labeled image per class (Snell et al., 2017; Finn et al., 2017), learning unsupervised update rules that generalize to different domains (Metz et al., 2018), and accelerating training procedures that are millions of steps long (Flennerhag et al., 2018). DIMCO generalizes well to novel datasets because its learning objective encourages the model to compactly use all of its degrees of freedom, thus enabling it to effectively compare datapoints while only requiring a small number of bits per datapoint. Derive a generalization bound for meta-learning that makes the tradeoff between taskspecific and task-wise overfitting concrete. Notably, we suggest that certain previous meta-learning methods may have benefitted from implicit regularization. We present experiments in Section 5 and conclude the paper with a discussion about limitations and future directions of our approach in Section 6. This can be problematic since in many works, the task-specific learning algorithm is usually just a byproduct of whatever clever meta-learning loss was proposed. We propose a factorized discrete representation scheme which enables us to represent discrete distributions with exponentially fewer parameters compared to listing the probability of each event. We similarly perform image retrival by mapping each support image to its most likely code (10) and for each query image retrieving the support image that has highest (11). Regularizing Meta-Learners The ability to generalize to novel datasets is critical in meta-learning benchmarks, and even more so in benchmarks such as Meta-Dataset (Triantafillou et al., 2019), where a model is tested on datasets from an unseen domain. The following works have reported benefits from reducing the number of such task-specific parameters: Lee & Choi (2018) learns a subset of the full network to alter during task-specific learning, Rusu et al.(2018) explicitly represents each task with a low-dimensional latent space, and Zintgraf et al.(2018) alters only a pre-specified subset of the full network during task-specific learning. We showed through Theorem 1 that restricting inner-loop expressivity reduces the generalization gap; this provides theoretical understanding to this consensus that meta-learning models with simple task-specific learners generalize to new tasks more easily. Also related is the deterministic information bottleneck (Strouse & Schwab, 2017) which extends the information bottleneck by minimizing H(X̃) rather than I(X̃;X). These three approaches to generalization are related via the chain of inequalities I(X̃;X) ≤ H(X̃) ≤ log |X̃|, which is tight when X̃ is an efficient code. Recent deep learning methods directly learn discrete representations, by learning variational autoencoders with discrete latent variables (Rolfe, 2016; van den Oord et al., 2017; Razavi et al., 2019) or maximizing the mutual information between representation and 1 In practice, we add log probabilities instead of multiplying probabilities for numerical stability. Additionally, their method solves a minimum cost flow problem within each batch to find the locally optimal code, whereas DIMCO is able to directly compute its loss function. We randomly initialize weights for the 4-layer convnet and use pretrained weights for the Inception network This experiment empirically verifies whether mutual information I(X̃;Y ) is a reasonable metric for quality of representation. We observed similar trends when training with with previously proposed loss functions: we visualize these results in Figure 5 of the appendix due to space constraints. Results in Figure 3 show that the compact code of DIMCO takes roughly an order of magnitude less memory for similar performance to N-pair loss, and requires less query time as well. We additionally note that DIMCO is able to train using large backbones without significantly overfitting, whereas experiments reported in Mishra et al.(2017) indicate that MAML (Finn et al., 2017) overfits tremendously when using a deeper backbone. This challenging experimental setup measures how much generalizable information the model can extract from a limited set of datasets; it can be seen as the meta-learning analogue of measuring the performance of a classifier trained with a small dataset. We report the average and standard deviation of the top 5 results of a random hyperparameter search (see appendix for details). Towards Explicit Meta-Regularization In Section 4, we have suggested with analogy to Theorem 1 that the benefits of some previous meta-learning methods can be attributed to implicitly being regularized by reducing the expressivity of their task-specific learners. In future work, we would like to explore explicit meta-regularization schemes that can be applied to other problems (regression, reinforcement learning etc.) Along with showing that the traditional support/query split is not strictly necessary, we demonstrated in Section 5.4 that removing it has the benefit of enabling meta-learning in datasets having one image per class. (12) Assuming that the approximate distribution q(·) is sufficiently close to p(y|x̃), minimizing (12) can be seen as arg min xent(Y, X̃) ≈ arg minEy∼Y,x̃∼X̃ [ − log p(y|x̃) ] (13) = arg minH(Y |X̃) = arg max I(X̃;Y ), (14) where the last equality uses the fact that H(Y ) is independent of model parameters. (24) We similarly bound the error caused by estimating L with a finite number of tasks sampled from τ . Combining equations (26, 24), we have with high probability∣∣∣∣L(τ, θ)− − 1 n n∑ i=1 Î(X̃(XT i , θ);YT i) ∣∣∣∣ (27) ≤ ∣∣∣L(τ, θ)− L̂(τ, θ)∣∣∣+O( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (28) ≤O (√ dΘ n log n dΘ ) +O ( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (29) Hardware Every experiment was conducted on a single Nvidia V100 GPU with CUDA 9.2. Each experiment was performed with different fixed initial seeds; we manually fix seeds with manual_seed() for python, pytorch, and numpy. We additionally show in Figure 5 the correlation between 1-shot accuracies, Recall@1, and NMI using three previously proposed losses (triplet, npair, protonet).",substance_1000_section_model,Discrete InfoMax Codes for Meta-Learning,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting.","This paper presents DIMCO, a meta-learner that is trained by maximizing mutual information between a discrete data representation and class labels across tasks. DIMCO is inspired by an information theoretic lower bound on the generalisation gap for meta-learning, which the authors argue identifies overfitting in the task learner as the bottleneck. 

This work proposes to constrain a learner to output discrete codes that are learned to capture the mutual information with class labels. While the idea of using discrete codes is interesting, its presentation in the manuscript is not well motivated and at times hard to follow. This makes it challenging to evaluate the novelty, validity, and generality of the proposed approach. Meanwhile, the empirical evaluation is somewhat lacking. Thus, I do not believe this work is ready for publication in its current form.

Detailed comments

My main concern is with respect to the primary contribution of this paper, a generalisation bound on meta-learning. The bound appears to be on a multi-task loss without task adaptation, and thus the claims made with respect to the theorem seem somewhat over-reaching. I also believe the VC-dimensionality of the encoder is missing in Eq. 4? If so, this changes the interpretation since the length of the code and the expressivity of the encoder are interrelated. Further, I would welcome a deeper analysis of the theorem and its implications. The current interpretation states that the number of tasks is independent of the size of each task, hence given many tasks, using minimal representations is an effective approach to meta-generalisation. Yet minimal representations is a well-known idea and has features in several works that use mutual information as a regularizer, most notably works on the Information Bottleneck. 

Another reservation I have is the use of mutual information between encoder representations and class labels as a loss function (Eq. 1). It lacks context and a proper motivation, especially since the analysis of [1] shows that the loss function in Eq. 1 is the cross-entropy objective. The authors make a similar analysis in Appendix A and argue that Eq. 1 differs in that cross-entropy is an approximation because it adds a parametrized linear layer on top of \tilde{X}. Thus, in the absence of that layer they collapse to the same objective. As DIMCO itself directly extract class label predictions from \tilde{X}, I fail to see a difference between the loss in Eq. 1 and a cross-entropy objective. 

The main motivation behind their loss objective is that it does not require a support / query set. This does not seem to be a feature of the mutual information objective itself, but rather a choice made by the authors. I would have liked a deeper discussion of this seeing as the authors make it a central tenet of DIMCO. Prior works use a support set as a principled means of doing meta-learning: meta-training explicitly takes into account that at test time, the learner will be given a small support set from which to learn how to query points. As far as I understand, DIMCO does not take this into account during meta-training.  At meta-test time however, DIMCO does use a support set to map query points (Eqs. 10 and 11). Why should we break protocols between meta training and testing? Are there any downsides to doing so?  

Empirically, I find the CUB experiment compelling but would welcome some ablations. What are the trade-offs between p and d? Can DIMCO outperform N-pair when number of bits are unconstrained?

miniImagenet is a standard benchmark in few-shot learning, but I am unable to find a results table - could the authors please provide results on the standard setup so that the method can be compared against known baselines? Further, would the results currently presented hold in a N-way-5-shot setup?

As for the constrained version of miniImagenet that the authors propose, I am not convinced this is an interesting protocol. In general, the miniImagenet task distribution is created by N-way permutations of the classes in the meta-set (e.g. meta-training tasks are combinations of the 64 classes in the meta-training set). By keeping the number of classes constant but reducing the number of images per class, this protocol is not reducing samples per task: a task is always defined as 5/20-way-1-shot (Fig. 4). Instead, the effect should be that tasks are in (greater) violation of the task i.i.d. assumption. Thus, I question whether this setup demonstrates the trade-offs the authors present in Theorem 1 and whether the results can be interpreted in light of it. 

Finally, that both experiments are image-based raises questions as to the generality of the method. The paper could be considerably strengthened by evaluating DIMCO on a non-image task, or if not discuss the method’s limitations.  

The idea of discrete codes for few-shot classification is interesting and sufficiently novel, I am likely to increase my score if my concerns are addressed and the experimental section is strengthened.

Further questions and comments:

- I am unable to parse Eq. 11 - what does the notation \prod_i p_{\tilde{x}_i, i} mean? 
- It is unnecessarily hard to follow the proof of theorem 1. It would help the reader if you restated relevant definitions, such as Eq. 1, since the difference with Eq. 23 is very subtle. It would also be helpful to explain how the summand in Eq. 25 differs from either, and Eq. 26 could be expanded or briefly explained after the derivation. 
-  Because DIMCO is trained with backpropagation, the interpretation of Eq. 5 as d independent events seems invalid. How does it affect the method if they are not independent?
- Overloading X and Y as both random variables and mini-batch samples creates unnecessary confusion. I believe the objective in Eq. 1 is approximated, not calculated exactly? For instance, the mutual information in Eq. 8 is with respect to a mini-batch, so should it not be \hat{I}? 
- p^j_{ik} in Eq. 9 is undefined.
- Eq. 9 is interpreted as an exact entropy, however it appears to be a mini-batch approximation to the true entropy? 

References 
[1] Achille and Soatto. Emergence of Invariance and Disentanglement in Deep Representations. JMLR. 2018."," The experiments are well-designed, and the results seem to align with our intuition. However, I would have liked to see a more in-depth analysis of the performance of DIMCO on larger datasets to be fully persuaded. It would be great if the authors could provide such results on more challenging datasets, e.g., ImageNet or ImageNet 2, which are very similar to the ones used in the experiments. I would like to see more analysis on this topic, as it is very important to me.",85
24,13063,hybrid,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all. But in each case, the network was designed and tuned specifically for the problem at hand. Natural language processing models have been shown to benefit from a multi-task approach a long time ago (Collobert & Weston, 2008), and recently multi-task machine translation models (MinhThang Luong, 2015) have even been shown to exhibit zero-shot learning when trained on multiple languages (Melvin Johnson, 2016). Speech recognition has also been shown to benefit from multi-task training (Seltzer & Droppo, 2013), as have some vision problems, such as facial landmark detection (Zhang Z., 2014). But no competitive multi-task multi-modal model has been proposed, so the above question remains unanswered. (3) COCO image captioning dataset (Lin et al., 2014), used for image captioning. (4) WSJ parsing dataset (Marcus et al., 1999), used for constituency parsing. While the MultiModel is only a first step and will be improved in the future, two key insights are crucial to making it work at all and are our main contributions. We design modality nets to be computationally minimal, promoting heavy feature extraction and ensuring that the majority of computation is performed within the domain-agnostic body of the model. Two design decisions were important: • The unified representation is variable-size. While a fixed-size representation is tempting and easier to implement, it creates a bottleneck and limits the performance of the model. For example, separable convolutions were introduced in the Xception architecture (Chollet, 2016) and were not applied to text or speech processing before. We find that each of these mechanisms is indeed crucial for the domain it was introduced, e.g., attention is far more important for languagerelated tasks than for image-related ones. As already said above, the encoder and decoder are constructed using 3 key computational blocks to get good performance across different problems: (1) Convolutions allow the model to detect local patterns and generalize across space. We refer the reader to (Chollet, 2016) for a complete definition; here we will denote a depthwise separable convolution with weights Wh×w corresponding to f kernels of size h × w applied to an input tensor x with stride s and dilated by a factor d (see (Yu & Koltun, 2015)) as SepConvd,s,f (W,x). The source tensor is finally passed through two different pointwise convolutions to generate the memory keys K and values V and the query keys, memory keys and memory values are used to apply the attention mechanism between the self-attended target and the source (see Figure 3). On the output side, the language modality takes the decoded output of the body and performs a learned linear mapping, followed by a Softmax, resulting in a probability distribution over the token vocabulary. The spectral modality does not perform any striding along the frequency bin dimension, preserving full resolution in the spectral domain. Convolutional architectures yielded good results on word-level neural machine translation starting from (Kalchbrenner & Blunsom, 2013) and later in (Meng et al., 2015). These early models used a standard RNN on top of the convolution to generate the output and had a bottleneck there that hurt performance, especially on longer sentences, similarly to the limitations of RNN sequence-to-sequence models without attention (Sutskever et al., 2014; Cho et al., 2014). This idea, introduced in WaveNet (van den Oord et al., 2016) and also used in MultiModel (see above) significantly improves efficiency. Depthwise separable convolutions were first studied by Sifre (Sifre & Mallat, 2013) and later they were used to get good results on large-scale image classification with Xception (Chollet, 2016). We focused our experiments so as to answer the following questions: (1) How far is the MultiModel trained on 8 tasks simultaneously from state-of-the-art results? Especially the 4 translation problems behave very similarly, so we decided to not include them all in each comparison but we focused on the more varied problems instead. The results we achieve are similar to the ones task-specific models get without heavy tuning, e.g., on English-French translation we improve on the recent Extended Neural GPU results (Kaiser & Bengio, 2016). Since we are comparing different instantiations of the same model, we report two internal metrics: the negative log-perplexity and per-token accuracy (measured on the development set). As can be seen from the results in Table 2, the joint 8-problem model performs similarly to single-model on large tasks, and better, sometimes significantly, on tasks where less data is available, such as parsing. The difference in performance is significant, and since we use both dropout and early stopping, we conjecture that it is not related to over-fitting. In fact, one could expect that removing these blocks will improve performance on ImageNet alone if they were truly useless for this task. In contrast, we see in Table 4 that these blocks either don’t affect or slightly improve performance. This leads us to conclude that mixing different computation blocks is in fact a good way to improve performance on many various tasks. We believe that this treads a path towards interesting future work on more general deep learning architectures, especially since our model shows transfer learning from tasks with a large amount of available data to ones where the data is limited.",substance_1000_section_model,Large Scale Multi-Domain Multi-Task Learning with MultiModel,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.","The paper presents a multi-task, multi-domain model based on deep neural networks. The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition. The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic. The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task.

The paper is well written and easy to follow.

The contributions of the paper are novel and significant. The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring. The experiments clearly show the viability of the approach and give interesting insights. This is surely an important step towards more general deep learning models. 

Comments:

* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database. Moreover, some databases could be used for different tasks, such as WSJ or ImageNet.

* The training procedure of the model is not explained in the paper. What is the cost function and what is the strategy to train on multiple tasks ? The paper should at least outline the strategy.

* The experiments are sufficient to demonstrate the viability of the approach, but the experimental setup is not clear. Specifically, there is an issue about the speech recognition part of the experiment. It is not clear what the task exactly is: continuous speech recognition, isolated word recognition ? The metrics used in Table 1 are also not clear, they should be explained in the text. Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used. Information about the detailed setup is also lacking, specifically which test and development sets are used (the WSJ corpus has several sets).

* Using raw waveforms as audio modality is very interesting, but this approach is not standard for speech recognition, some references should be provided, such as:
P. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26–30.
D. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE.
T. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals. Learning the Speech Front-end With Raw Waveform CLDNNs. Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015.

Revised Review:
The main idea of the paper is very interesting and the work presented is impressive. However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks. The experiments are interesting, except for the WSJ speech task, which is almost meaningless. Indeed, it is not clear what the network has learned given the metrics presented, as the WER on WSJ should be around 5% for speech recognition.
I thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition. A simpler speech task such as Keyword Spotting could also be investigated.
"," The experiments are well-designed, and the model is well-evaluated. It would be good to see a more in-depth analysis of the performance of the model on more challenging tasks, e.g.word-level translation, image captioning, and speech-related tasks, as well as a more detailed analysis of its performance on these tasks. However, I would have liked to see an analysis of how performance is influenced by the number of layers, and how attentional efficiency is influenced.",75
25,6299,hybrid,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines. The problem can also be understood as constructing a function f(G, nS , q) to predict nT , where the functional form of f(·) is generally unknown and has to be learned from a training dataset consisting of samples like (nS , q, nT ). Since nT is unknown, the problem cannot be solved by conventional search algorithms such as A⇤-search [11], which seeks to find paths between the given source and target nodes. For the KBC example in Figure 1(a), having access to the current node nt = Hawaii alone is not sufficient to know that the best action is moving to nt+1 = USA. Second, to address the challenge of sparse rewards, M-Walk exploits the fact that the MDP transition model is known and deterministic.2 Specifically, it combines Monte Carlo Tree Search (MCTS) with the RNN to generate trajectories that obtain significantly more positive rewards than using the RNN policy alone. The rest of the paper is organized as follows: Section 3 develops the M-Walk agent, including the model architecture, the training and testing algorithms.3 Experimental results are presented in Section 4. To see this, we observe from Figure 1(b) that once an action at (i.e., an edge in Ent or “STOP”) is selected, the next node nt+1 and its associated Ent+1 and Nnt+1 are known. The objective is to learn a policy that maximizes the terminal rewards, i.e., correctly identifies the target node with high probability. Recall from Section 2 (e.g., (1)) that one challenge in applying RL to the graph-walking problem is that the state st nominally includes the entire history of observations. We will explain in Section 3.2 how such parameter sharing enables indirect updates to the policy ⇡✓ via Q-learning from off-policy data. To address this issue, we apply the same FCN f✓(·) to 5There could be alternative ways to compute the score, such as Score(n) = maxsT!n Q✓(sT , STOP). Inspired by this recursion, we propose using the GRU-RNN [4] to encode qt into a vector representation6: qt+1 = f✓q (qt, [hA,t, hat,t, nt+1]) with initialization q0 = f✓q (q, [0, 0, nS ]), where ✓q is the model parameter, and hat,t denotes the vector hn0,t at n0 = at. Three Glass Puzzle The Three Glass Puzzle [20] is a problem studied in math puzzles and graph theory. WN18RR [6] is created from the original WN18 [2] by removing various sources of test leakage, making the dataset more challenging. We use HITS@1,3 and mean reciprocal rank (MRR) as the evaluation metrics for WN18RR, and use mean average precision (MAP) for NELL995,7 where HITS@K computes the percentage of the desired entities being ranked among the top-K list, and MRR computes an average of the reciprocal rank of the desired entities. We compare against RL-based methods [38, 5], embedding-based models (including DistMult [39], ComplEx [32] and ConvE [6]) and recent work in logical rules (NeuralLP) [40]. For all the baseline methods, we used the implementation released by the corresponding authors with their best-reported hyperparameter settings.8 The details of the hyperparameters for M-Walk are described in Appendix B.2.2 of the supplementary material. We ran the experiments three times and report the means and standard deviations (except for PRA, TransE, and TransR on NELL995, whose results are directly quoted from [38]). We observed that the novel neural architecture of M-Walk contributes an overall 1% gain relative to MINERVA on NELL995, and it is still 1% worse than M-Walk, which uses MCTS for training and testing. Third, we analyze the performance of M-Walk under different numbers of MCTS rollout simulations and different search horizons on WN18RR dataset, with results shown in Figure 5(a). We observe that M-Walk outperforms the strong baseline ConvE by 4.6–10.9% in samples that require 2 or 3 steps, while it is nearly on par for paths of length one. Therefore, M-Walk does better at reasoning over longer paths than ConvE. To examine this effect, we show in Figure 5(c)-top the HITS@K accuracies when the ground truth is in the candidate set.9 It shows that M-Walk has very high accuracy in this case, which is significantly higher than ConvE (80% vs 39.6% in HITS@1). These observations point to an important direction for improving M-Walk in future work: increasing the chance of covering the target by the candidate set. Finally, in Table 4, we show examples of reasoning paths found by M-Walk.10 Reinforcement Learning Recently, deep reinforcement learning has achieved great success in many artificial intelligence problems [17, 24, 25]. Note that the former is constructed from the visit counts of all the edges connected to the MCTS root node; it only uses information near the root node to improve the policy. Recent approaches have demonstrated limitations of these prior approaches: they suffer from cascading errors when dealing with compositional (multi-step) relationships [10]. Hence, recent works [8, 18, 10, 15, 30] have proposed approaches for injecting multi-step paths such as random walks through sequences of triples during training, further improving performance on KBC tasks. IRN [23] and Neural LP [40] explore multi-step relations by using an RNN controller with attention over an external memory. Empirically, our proposed algorithm outperforms both DeepPath and MINERVA in the KBC benchmarks.11 We developed an RL-agent (M-Walk) that learns to walk over a graph towards a desired target node for given input query and source nodes. Furthermore, we also performed extensive experimental analysis to understand M-Walk. 11A preliminary version of M-Walk with limited experiments was reported in the workshop paper [22].",summary_1000_section_model,M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines.","Summary of the paper:  - This paper proposes a walk-based method for knowledge graph completion (KGC).  - For triples (s, q, o), an agent is trained so that given s and q, it traverses a path from the initial node s to the   terminal node t in a knowledge graph.  - This problem is formulated as that of Q-learning using RNN to model policy and Q functions, but the issue is the   sparseness of rewards.  - Monte Carlo tree search is used to overcome this issue.  Pros:  - The paper is clearly written.  - The approach is natural and sound.  - Very good performance compared with existing KGC methods.  - Ablation study with PG-Walk and Q-Walk is nice.  Cons:  - The work is mostly empirical, without much theoretical justification (admittedly, as in many deep learning papers).  - Not much novelty or surprise, given the success of AlphaGo.  This is a type of paper in which a successful technique   in other fields is applied to a different field.  I could not see exactly what is the novelty/creativity in terms of   how MCTS is applied for this specific problem of KGC.  - No analysis on running time is provided.   General Comments:  This paper is well written (but I would like the content of Section 4.2 to be more organized --- Page 7 consists of a single paragraph now!).  The novelty of the proposal is somewhat questionable, because reinforcement learning has already been introduced for KGC, and the application of MCTS naturally comes to mind.  Although this work is mostly empirical and little theoretical contribution is provided, it still achieves the state-of-the-art performance, and that could be a merit for its publication.  Questions:  * Related work  MCTS has been introduced in walk-based KGC by Sheng et al. in ICLR 2018 Workshop (ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search), and this work must be cited.  Please elaborate on how M-Walk differs from ReinforceWalk as much as you can tell from the extended abstract.  * Experiments:  - Dataset:  Is there any reason the comparison is done with NELL995 and WN18RR?  FB15k is also the standard dataset.  - Baselines  Why do the set of compared methods differ between datasets?  In particular, we know that TransE and TransR are somewhat weaker compared with more recent bilinear models, such as ComplEx, ANALOGY, and ConvE.  - Evaluation metrics  Also, why is the MAP score used on NELL995 whereas HITS@/MRR are used on WN18RR?  I have nothing against MAP scores, but typically HITS@/MRR are reported in literature on KGC even on the NELL-995 dataset (e.g., the MINERVA paper); using different metrics makes it difficult to validate the experimental results.  - Figure 6(a)  Is there any explanation as to why accuracy drops as the number of roll outs are increased to 256?  * Application to classification task  The method is only concerned with prediction of knowledge triple in which only one entity is missing. Can the proposed method be applied to the triple classification task where the truth of a given triple must be predicted? This is more or less straightforward in score-function-based models (provided that a suitable threshold can be determined), but I cannot see how this can be addressed with walk-based methods in general.  * Running time  Training and inference both involves roll-out, which raises concern about the running time of the algorithms.  How does M-walk fare against scoring-based methods (e.g., ComplEx) or MINERVA in terms of running time?  Please provide the time required for training and inference.  I am curious about inference time in particular. "," This paper proposes a novel approach to the problem of knowledge base completion ( KBC ) by developing a graph-walking agent called M-Walk that consists of a deep recurrent neural network ( RNN ) and Monte Carlo Tree Search ( MCTS ). The RNN encodes the history of the walked path and maps it separately to a policy and Q-value, which is used to generate trajectories that are used to learn a policy for the target node. In order to overcome the challenge of sparse rewards, the authors propose to use a Q-learning approach to improve the policy of the RNN.",102
26,837,hybrid,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art. Since the introduction of the “AlexNet” architecture [14] in the 2012 ImageNet competition, ConvNets have dominated state-of-the-art performance across a variety of computer vision tasks, including object-detection, image segmentation, image classification, face recognition, human pose estimation and tracking. In conjunction with these advances as well as the evolution of network architectures, several design best practices have emerged [8, 21, 23, 24]. This functionality can be achieved by stacking many small filters or using large filters in the network; notably, the first choice can be implemented with fewer operations (faster, fewer parameters) and also allows inclusion of more nonlinearities. Third, dimensionality reduction (1×1 convolutions) before spatially aggregating filters (e.g.3×3) is supported by the fact that outputs of neighbouring filters are highly correlated and therefore these activations can be reduced before aggregation [23]. Fourth, spatial factorization into asymmetric filters can even further reduce computational cost and 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. Fifth, it is important to normalize the responses of each feature channel within a batch to reduce internal covariate shift [11]. To demonstrate the benefits of our proposed spatiotemporal ResNet architecture, it has been evaluated on two standard action recognition benchmarks where it greatly boosts the state-of-the-art. Yet another study compared several approaches to extending ConvNets into the temporal domain, but with rather disappointing results [13]: The architectures were not particularly sensitive to temporal modelling, with a slow fusion model performing slightly better than early and late fusion alternatives; moreover, similar levels of performance were achieved by a purely spatial network. Another research branch has investigated combining image information in network architectures across longer time periods. That approach takes advantage of the local spatial similarity in images; however, it only yields a minor increase over their baseline, which is a two-stream VGG-16 ConvNet [21] used as the input to their convolutional RNN. Batch normalization [11] and ReLU [14] are applied after each convolution; the network does not use hidden fc, dropout, or max-pooling (except immediately after the first layer). In preliminary experiments we found that direct connections between identical layers of the two streams led to an increase in validation error. We conjecture that these results are due to the large change that the signal of one network stream undergoes after injecting a fusion signal from the other stream. Therefore, we developed a more subtle alternative solution based on additive interactions, as follows. To enable learning of spatiotemporal features at all possible scales, this modification is applied before the second residual unit at each spatial resolution of the network (indicated by “skip-stream” in Table 1), as exemplified by the connection at the conv5_x layers in Fig. Based on the above observations, we developed a novel approach to temporal residual connections that builds on the ConvNet design guidelines of chaining small [21] asymmetric [10, 23] filters, noted in Sec. This allows the straightforward use of standard two-stream ConvNets that have been pre-trained on large-scale datasets e.g.to leverage the massive amounts of training data from the ImageNet challenge. From left to right, top to bottom, the first row shows the convolutional and pooling building blocks, with the filter and pooling size shown as (W ×H × T,C), denoting width, height, temporal extent and number of feature channels, resp. In the last two rows we show the output size of these metalayers as well as the receptive field on which they operate. We also apply random cropping and scale augmentations: We randomly jitter the width and height of the 224× 224 input frame by ±25% and also randomly crop it from a maximum of 25% distance from the image borders. We found that this strategy significantly reduces the training duration with the drawback that each loss does not capture all available information. For our final model, we equip the spatiotemporal ResNet with a temporal max-pooling layer after pool5 (see Table 1, temporal average pooling led to inferior results) and continue training as above with the learning rate starting from 10−4 for 2K iterations followed by 10−5. For fair comparison, we follow the evaluation procedure of the original two-stream work [20] by sampling 25 frames (and their horizontal flips). However, rather than using 10 spatial 224× 224 crops from each of the frames, we apply fully convolutional testing both spatially (smallest side rescaled to 256) and temporally (the 25 frame-chunks) by classifying the video in a single forward pass, which takes ≈250ms on a Titan X GPU. Second, we consider HMDB51 [15], which has 6766 videos that show 51 different actions and generally is considered more challenging than UCF0101 due to the even wider variations in which actions occur. For both datasets, we use the provided evaluation protocol and report mean average accuracy over three splits into training and test sets. Interestingly, research in neuroscience also suggests that the human visual cortex is equipped with connections between the dorsal and the ventral stream to distribute motion information to separate visual areas [3, 27]. We achieve this by simply averaging the L2-normalized SVM scores of the FV-encoded IDT descriptors (i.e.HOG, HOF, MBH) [29] with the L2-normalized video predictions of our ST-ResNet*, again without softmax normalization. These relatively larger performance decrements again underline that our approach is better able to capture the available dynamic information, as there is less to be gained by augmenting it with IDT. Still, there is a benefit from the hand-crafted IDT features even with our approach, which could be attributed to its explicit compensation of camera motion.",summary_1000_section_model,Spatiotemporal Residual Networks for Video Action Recognition,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art.","The paper presents a novel architecture that 1) combines residual networks with two-stream convolutional networks, and 2) injects connections from the motion stream to the appearance stream, to be able to capture spatio-temporal features. The paper shows experiments in both of the main action recognition datasets, achieving state-of-the-art accuracy in both. Overall I think the paper is great: good idea, careful experimentation, great results and clearly written.   Although the basic components of the architecture are pre-existing, I think that the high performance and careful experimentation and description make it a very useful contribution.   I only miss some experiments to visualize what kinds of spatio-temporal features are being learned.   Since large temporal windows are important (278-280) I would add a relevant reference: Long-term Temporal Convolutions for Action Recognition, Gul Varol, Ivan Laptev, Cordelia Schmid  Small typos:  (L 187)  ""resp.""  (L 255) ""UCF0101""  "," This paper presents a method for training a spatiotemporal ResNet based on two-stream convolutional networks. The authors propose to add temporal residual connections between the appearance and motion pathways of a two stream architecture to allow spatio-temporal interaction between the two streams. The proposed method is evaluated on two tasks : ( i ) action recognition and ( ii ) object-detection.The paper is well written and the experiments show promising results.The main contribution of this paper is to propose a method that combines two existing methods : ( a ) Residual networks ( ResNets ), and ( b ) temporal residual networks ( TNNs ). The ResNet is a combination of these two approaches.",115
27,22112,hybrid,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values. While a large body of the machine learning literature is built upon the assumption of having access to complete datasets, in many real-world problems only incomplete datasets are available. Single imputation methods try to replace each missing value with a plausible value that is the best fit given the value of other correlated features and knowledge extracted from the dataset (Hastie et al., 1999; Anderson, 1957). While MI offers interesting statistical insights about the reliability of analysis on incomplete data, the insight is imprecise as it is mainly concerned about the population of data samples rather than individual instances. While these methods are easy to implement and analyze, they often fail to capture the complex feature dependencies as well as structures present in many problems. For instance, Mattei & Frellsen (2018) suggested a method based on deep latent variable models and importance sampling that offers a tighter likelihood bound compared to the standard VAE bound. Also, these methods are often complicated to be applied in practical setups by practitioners. A few exceptions exist such as Bayesian models and decision trees that permit direct analysis on incomplete data (Nielsen & Jensen, 2009; Zhang et al., 2005). However, given an incomplete training dataset and without any known causal structure as a priori, learning such models is a very challenging problem with the complexity of at least NP-complete to learn the network architecture in addition to an iterative EM optimization to learn model parameters (Darwiche, 2009; Neapolitan et al., 2004). Similarly, for the second part, we are interested in obtaining a distribution over the possible target assignments and the confidence of each class rather than maximum likelihood class assignments. Instead, we use recent advances in GAN stabilization and training to improve the training process (see Section 3.4). Additionally, in our experiments, we provide supporting evidence that this simple loss function enables us to sample from the conditional distribution and prevents biased inclinations toward distribution modes. For instance, for a specific test sample at hand, if a certain missing feature is a strong indicator of the target class, we would like to observe the impact of different imputations for that feature on the final hypothesis. Input: G (trained imputer), D (dataset) Output: Fθ (trained predictor) foreach Training Epoch do foreach (xi, ki, yi) in D do z ∼ N(0, I) x̂i ← ki xi+(1−ki) G(xi,ki, z) ypredi ← Fθ(x̂i) loss← L(yi, ypredi ) Backpropagate loss Update Fθ Algorithm 2: Estimating target distributions. The first assumption is consistent with the theoretical analysis of generative adversarial networks that they can converge to the true underlying distribution (Arora et al., 2018; Liu et al., 2017). It should be noted that, while Zhang et al.(2018) suggests using a single self-attention layer in the middle of the network, we observed consistent improvements by inserting multiple self-attention layers before each residual block within the network. We explored best TTUR learning-rate settings from the set of {0.001, 0.0005, 0.0001, 0.00005}. To evaluate the proposed method we use CIFAR-10 (Krizhevsky & Hinton, 2009) as an image classification dataset as well as three non-image datasets: UCI Landsat (Dua & Graff, 2017)2, MIT-BIH arrhythmia (Moody & Mark, 2001), and Diabetes classification (Kachuee et al., 2019) 3. Also, while different encoding and representation methods are suggested in the literature to handle categorical features (Jang et al., 2016; Nazabal et al., 2018), in this paper, we take the simple approach of encoding categorical variables using one-hot representation and smoothing them by adding Gaussian noise with zero mean and variance equal to 5% of feature variances. We would like to note that while the suggested solution in this paper is readily compatible with MAR structures, in our experiments, to simplify the presentation of results and to have a fair comparison with other work that does not support the MAR assumption, we limited the scope of our experiments to MCAR. Furthermore, to simulate incomplete datasets and to make sure the same features are missing without explicitly storing masks, we use hashed feature vectors to seed random number generators used to sample missing features. More detail is provided in Appendix C. Fréchet inception distance (FID) (Heusel et al., 2017) score is used to measure the quality of missing data imputation in experiments with images5. We also considered using root means squared error (RMSE); however, we decided not to use this measure as we observed an inconsistent behavior using RMSE in our comparisons as RMSE favors methods that show less variance rather than realistic and sharp samples from the distribution. We compare our results with MisGAN (Li et al., 2019) and GAIN (Yoon et al., 2018) as the state of the art imputation algorithms based on GANs as well as basic denoising autoencoder (DAE) (Vincent et al., 2008) and multiple imputation by chained equations (MICE) (Buuren & Groothuis-Oudshoorn, 2010) as baselines. Due to scalability issues, we were only able to use MICE for the smaller non-image datasets. One possible explanation for this behavior might be the fact that GAIN has an MSE loss term acting similar to an autoencoder loss smoothing noisy missing pixels. Regarding the MIT-BIH experiemts, GI outperforms other work for missing rates more than 30% while achieving similar accuracies to GAIN for lower missing rates. As it can be seen from the plots, GI provides results closest to the ideal case of having average confidence values equal to average accuracies. In order to provide further insight into the operation of GI and how imputations can potentially influence the outcomes of predictions, we conduct experiments on a synthesized dataset. One possible explanation could be the fact that imputing missing data with a uniform structure can be done by processing local regions and does not require attending to different distant regions across the image. From Table 6 it can be inferred that as the rate of missingness increases, the benefits of the suggested predictor algorithm increase significantly.",summary_1000_section_model,Generative Imputation and Stochastic Prediction,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values.","This paper proposes a method to impute missing features using a generative model and train a predictive model on top of imputed dataset to improve classification results. They first train a GAN model where the generator outputs an imputed representation of the input and discriminator is trained to predict if an individual features (such as a pixel) is imputed or not. Given the generator and incomplete sample, they train a predictor using the output of the generator, imputed sample, as input. Their main contribution is using a MC averaging to compute the prediction by repetitively sampling from the noise variable, z, and generating different imputations from generator. They show that the proposed model improves upon the previous SOTA on final classification performance.

Overall the paper is clearly written. But I do feel it is a bit incremental over the GAIN approach. The overall GAN architecture is very similar to GAIN's and although stochastic prediction shows clear improvements it is a bit straightforward. However, I think the uncertainty of the imputations and its effect on the final prediction is interesting. I suggest the authors to extend this part with more detailed analysis.

There are several parts that are confusing/missing in the paper:

- In GAIN, they use a hint vector as an input to the discriminator. They show that without the hint vector, there is no unique solution (this is shown without the MSE loss). The authors do not use this vector in their approach (as in Figure 1) and it is not clear to me if it causes any instabilities or if multiple experiments yield similar results or if the stochastic prediction benefits from this.
- On what type of examples GI is more accurate than other models? Since stochastic prediction is the main difference from GAIN, is this related to the multi-modality of the noisy examples?
- Can you explain the difference between the results in Figure-7 and Table-2? Results between the two mismatch.
- I think the statement in the first paragraph in Section 4.4 that ""MSE loss term would act as a denoising loss smoothing noisy missing pixels"" could be misleading. MSE is used with mask in GAIN, hence it only applies to the observed features during training. Its effect on smoothing noisy missing pixels is not clear.


I think the paper would benefit if the authors could explain/show:
- Increasing the missing rate would also increase the possibility that the ground truth be a more multi-modal distribution. Especially in rectangular generation part where it can remove a complete object. Does stochastic averaging benefit more in this case?"," This paper proposes a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, a generator network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on the CIFAR-10 dataset and three real-world tabular classification datasets under different missingness rates and structures.The paper is well written and the experiments show that the proposed method can generate imputations and provide estimates for the class uncertainties in a classification task.",97
28,18864,hybrid,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting. Recent works have used this framework to achieve impressive feats such as learning to classify using one labeled image per class (Snell et al., 2017; Finn et al., 2017), learning unsupervised update rules that generalize to different domains (Metz et al., 2018), and accelerating training procedures that are millions of steps long (Flennerhag et al., 2018). DIMCO generalizes well to novel datasets because its learning objective encourages the model to compactly use all of its degrees of freedom, thus enabling it to effectively compare datapoints while only requiring a small number of bits per datapoint. Derive a generalization bound for meta-learning that makes the tradeoff between taskspecific and task-wise overfitting concrete. Notably, we suggest that certain previous meta-learning methods may have benefitted from implicit regularization. We present experiments in Section 5 and conclude the paper with a discussion about limitations and future directions of our approach in Section 6. This can be problematic since in many works, the task-specific learning algorithm is usually just a byproduct of whatever clever meta-learning loss was proposed. We propose a factorized discrete representation scheme which enables us to represent discrete distributions with exponentially fewer parameters compared to listing the probability of each event. We similarly perform image retrival by mapping each support image to its most likely code (10) and for each query image retrieving the support image that has highest (11). Regularizing Meta-Learners The ability to generalize to novel datasets is critical in meta-learning benchmarks, and even more so in benchmarks such as Meta-Dataset (Triantafillou et al., 2019), where a model is tested on datasets from an unseen domain. The following works have reported benefits from reducing the number of such task-specific parameters: Lee & Choi (2018) learns a subset of the full network to alter during task-specific learning, Rusu et al.(2018) explicitly represents each task with a low-dimensional latent space, and Zintgraf et al.(2018) alters only a pre-specified subset of the full network during task-specific learning. We showed through Theorem 1 that restricting inner-loop expressivity reduces the generalization gap; this provides theoretical understanding to this consensus that meta-learning models with simple task-specific learners generalize to new tasks more easily. Also related is the deterministic information bottleneck (Strouse & Schwab, 2017) which extends the information bottleneck by minimizing H(X̃) rather than I(X̃;X). These three approaches to generalization are related via the chain of inequalities I(X̃;X) ≤ H(X̃) ≤ log |X̃|, which is tight when X̃ is an efficient code. Recent deep learning methods directly learn discrete representations, by learning variational autoencoders with discrete latent variables (Rolfe, 2016; van den Oord et al., 2017; Razavi et al., 2019) or maximizing the mutual information between representation and 1 In practice, we add log probabilities instead of multiplying probabilities for numerical stability. Additionally, their method solves a minimum cost flow problem within each batch to find the locally optimal code, whereas DIMCO is able to directly compute its loss function. We randomly initialize weights for the 4-layer convnet and use pretrained weights for the Inception network This experiment empirically verifies whether mutual information I(X̃;Y ) is a reasonable metric for quality of representation. We observed similar trends when training with with previously proposed loss functions: we visualize these results in Figure 5 of the appendix due to space constraints. Results in Figure 3 show that the compact code of DIMCO takes roughly an order of magnitude less memory for similar performance to N-pair loss, and requires less query time as well. We additionally note that DIMCO is able to train using large backbones without significantly overfitting, whereas experiments reported in Mishra et al.(2017) indicate that MAML (Finn et al., 2017) overfits tremendously when using a deeper backbone. This challenging experimental setup measures how much generalizable information the model can extract from a limited set of datasets; it can be seen as the meta-learning analogue of measuring the performance of a classifier trained with a small dataset. We report the average and standard deviation of the top 5 results of a random hyperparameter search (see appendix for details). Towards Explicit Meta-Regularization In Section 4, we have suggested with analogy to Theorem 1 that the benefits of some previous meta-learning methods can be attributed to implicitly being regularized by reducing the expressivity of their task-specific learners. In future work, we would like to explore explicit meta-regularization schemes that can be applied to other problems (regression, reinforcement learning etc.) Along with showing that the traditional support/query split is not strictly necessary, we demonstrated in Section 5.4 that removing it has the benefit of enabling meta-learning in datasets having one image per class. (12) Assuming that the approximate distribution q(·) is sufficiently close to p(y|x̃), minimizing (12) can be seen as arg min xent(Y, X̃) ≈ arg minEy∼Y,x̃∼X̃ [ − log p(y|x̃) ] (13) = arg minH(Y |X̃) = arg max I(X̃;Y ), (14) where the last equality uses the fact that H(Y ) is independent of model parameters. (24) We similarly bound the error caused by estimating L with a finite number of tasks sampled from τ . Combining equations (26, 24), we have with high probability∣∣∣∣L(τ, θ)− − 1 n n∑ i=1 Î(X̃(XT i , θ);YT i) ∣∣∣∣ (27) ≤ ∣∣∣L(τ, θ)− L̂(τ, θ)∣∣∣+O( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (28) ≤O (√ dΘ n log n dΘ ) +O ( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (29) Hardware Every experiment was conducted on a single Nvidia V100 GPU with CUDA 9.2. Each experiment was performed with different fixed initial seeds; we manually fix seeds with manual_seed() for python, pytorch, and numpy. We additionally show in Figure 5 the correlation between 1-shot accuracies, Recall@1, and NMI using three previously proposed losses (triplet, npair, protonet).",summary_1000_section_model,Discrete InfoMax Codes for Meta-Learning,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting.","This paper presents DIMCO, a meta-learner that is trained by maximizing mutual information between a discrete data representation and class labels across tasks. DIMCO is inspired by an information theoretic lower bound on the generalisation gap for meta-learning, which the authors argue identifies overfitting in the task learner as the bottleneck. 

This work proposes to constrain a learner to output discrete codes that are learned to capture the mutual information with class labels. While the idea of using discrete codes is interesting, its presentation in the manuscript is not well motivated and at times hard to follow. This makes it challenging to evaluate the novelty, validity, and generality of the proposed approach. Meanwhile, the empirical evaluation is somewhat lacking. Thus, I do not believe this work is ready for publication in its current form.

Detailed comments

My main concern is with respect to the primary contribution of this paper, a generalisation bound on meta-learning. The bound appears to be on a multi-task loss without task adaptation, and thus the claims made with respect to the theorem seem somewhat over-reaching. I also believe the VC-dimensionality of the encoder is missing in Eq. 4? If so, this changes the interpretation since the length of the code and the expressivity of the encoder are interrelated. Further, I would welcome a deeper analysis of the theorem and its implications. The current interpretation states that the number of tasks is independent of the size of each task, hence given many tasks, using minimal representations is an effective approach to meta-generalisation. Yet minimal representations is a well-known idea and has features in several works that use mutual information as a regularizer, most notably works on the Information Bottleneck. 

Another reservation I have is the use of mutual information between encoder representations and class labels as a loss function (Eq. 1). It lacks context and a proper motivation, especially since the analysis of [1] shows that the loss function in Eq. 1 is the cross-entropy objective. The authors make a similar analysis in Appendix A and argue that Eq. 1 differs in that cross-entropy is an approximation because it adds a parametrized linear layer on top of \tilde{X}. Thus, in the absence of that layer they collapse to the same objective. As DIMCO itself directly extract class label predictions from \tilde{X}, I fail to see a difference between the loss in Eq. 1 and a cross-entropy objective. 

The main motivation behind their loss objective is that it does not require a support / query set. This does not seem to be a feature of the mutual information objective itself, but rather a choice made by the authors. I would have liked a deeper discussion of this seeing as the authors make it a central tenet of DIMCO. Prior works use a support set as a principled means of doing meta-learning: meta-training explicitly takes into account that at test time, the learner will be given a small support set from which to learn how to query points. As far as I understand, DIMCO does not take this into account during meta-training.  At meta-test time however, DIMCO does use a support set to map query points (Eqs. 10 and 11). Why should we break protocols between meta training and testing? Are there any downsides to doing so?  

Empirically, I find the CUB experiment compelling but would welcome some ablations. What are the trade-offs between p and d? Can DIMCO outperform N-pair when number of bits are unconstrained?

miniImagenet is a standard benchmark in few-shot learning, but I am unable to find a results table - could the authors please provide results on the standard setup so that the method can be compared against known baselines? Further, would the results currently presented hold in a N-way-5-shot setup?

As for the constrained version of miniImagenet that the authors propose, I am not convinced this is an interesting protocol. In general, the miniImagenet task distribution is created by N-way permutations of the classes in the meta-set (e.g. meta-training tasks are combinations of the 64 classes in the meta-training set). By keeping the number of classes constant but reducing the number of images per class, this protocol is not reducing samples per task: a task is always defined as 5/20-way-1-shot (Fig. 4). Instead, the effect should be that tasks are in (greater) violation of the task i.i.d. assumption. Thus, I question whether this setup demonstrates the trade-offs the authors present in Theorem 1 and whether the results can be interpreted in light of it. 

Finally, that both experiments are image-based raises questions as to the generality of the method. The paper could be considerably strengthened by evaluating DIMCO on a non-image task, or if not discuss the method’s limitations.  

The idea of discrete codes for few-shot classification is interesting and sufficiently novel, I am likely to increase my score if my concerns are addressed and the experimental section is strengthened.

Further questions and comments:

- I am unable to parse Eq. 11 - what does the notation \prod_i p_{\tilde{x}_i, i} mean? 
- It is unnecessarily hard to follow the proof of theorem 1. It would help the reader if you restated relevant definitions, such as Eq. 1, since the difference with Eq. 23 is very subtle. It would also be helpful to explain how the summand in Eq. 25 differs from either, and Eq. 26 could be expanded or briefly explained after the derivation. 
-  Because DIMCO is trained with backpropagation, the interpretation of Eq. 5 as d independent events seems invalid. How does it affect the method if they are not independent?
- Overloading X and Y as both random variables and mini-batch samples creates unnecessary confusion. I believe the objective in Eq. 1 is approximated, not calculated exactly? For instance, the mutual information in Eq. 8 is with respect to a mini-batch, so should it not be \hat{I}? 
- p^j_{ik} in Eq. 9 is undefined.
- Eq. 9 is interpreted as an exact entropy, however it appears to be a mini-batch approximation to the true entropy? 

References 
[1] Achille and Soatto. Emergence of Invariance and Disentanglement in Deep Representations. JMLR. 2018."," This paper provides a generalization bound for meta-learning that identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from this bound, the authors propose DIMCO, a novel meta learning model that trains a stochastic encoder to output discrete codes. Experiments show that the compact compactness of the model allows it to generalize to new tasks more easily, and it generalizes well in a small-data setting.",77
29,13063,hybrid,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all. But in each case, the network was designed and tuned specifically for the problem at hand. Natural language processing models have been shown to benefit from a multi-task approach a long time ago (Collobert & Weston, 2008), and recently multi-task machine translation models (MinhThang Luong, 2015) have even been shown to exhibit zero-shot learning when trained on multiple languages (Melvin Johnson, 2016). Speech recognition has also been shown to benefit from multi-task training (Seltzer & Droppo, 2013), as have some vision problems, such as facial landmark detection (Zhang Z., 2014). But no competitive multi-task multi-modal model has been proposed, so the above question remains unanswered. (3) COCO image captioning dataset (Lin et al., 2014), used for image captioning. (4) WSJ parsing dataset (Marcus et al., 1999), used for constituency parsing. While the MultiModel is only a first step and will be improved in the future, two key insights are crucial to making it work at all and are our main contributions. We design modality nets to be computationally minimal, promoting heavy feature extraction and ensuring that the majority of computation is performed within the domain-agnostic body of the model. Two design decisions were important: • The unified representation is variable-size. While a fixed-size representation is tempting and easier to implement, it creates a bottleneck and limits the performance of the model. For example, separable convolutions were introduced in the Xception architecture (Chollet, 2016) and were not applied to text or speech processing before. We find that each of these mechanisms is indeed crucial for the domain it was introduced, e.g., attention is far more important for languagerelated tasks than for image-related ones. As already said above, the encoder and decoder are constructed using 3 key computational blocks to get good performance across different problems: (1) Convolutions allow the model to detect local patterns and generalize across space. We refer the reader to (Chollet, 2016) for a complete definition; here we will denote a depthwise separable convolution with weights Wh×w corresponding to f kernels of size h × w applied to an input tensor x with stride s and dilated by a factor d (see (Yu & Koltun, 2015)) as SepConvd,s,f (W,x). The source tensor is finally passed through two different pointwise convolutions to generate the memory keys K and values V and the query keys, memory keys and memory values are used to apply the attention mechanism between the self-attended target and the source (see Figure 3). On the output side, the language modality takes the decoded output of the body and performs a learned linear mapping, followed by a Softmax, resulting in a probability distribution over the token vocabulary. The spectral modality does not perform any striding along the frequency bin dimension, preserving full resolution in the spectral domain. Convolutional architectures yielded good results on word-level neural machine translation starting from (Kalchbrenner & Blunsom, 2013) and later in (Meng et al., 2015). These early models used a standard RNN on top of the convolution to generate the output and had a bottleneck there that hurt performance, especially on longer sentences, similarly to the limitations of RNN sequence-to-sequence models without attention (Sutskever et al., 2014; Cho et al., 2014). This idea, introduced in WaveNet (van den Oord et al., 2016) and also used in MultiModel (see above) significantly improves efficiency. Depthwise separable convolutions were first studied by Sifre (Sifre & Mallat, 2013) and later they were used to get good results on large-scale image classification with Xception (Chollet, 2016). We focused our experiments so as to answer the following questions: (1) How far is the MultiModel trained on 8 tasks simultaneously from state-of-the-art results? Especially the 4 translation problems behave very similarly, so we decided to not include them all in each comparison but we focused on the more varied problems instead. The results we achieve are similar to the ones task-specific models get without heavy tuning, e.g., on English-French translation we improve on the recent Extended Neural GPU results (Kaiser & Bengio, 2016). Since we are comparing different instantiations of the same model, we report two internal metrics: the negative log-perplexity and per-token accuracy (measured on the development set). As can be seen from the results in Table 2, the joint 8-problem model performs similarly to single-model on large tasks, and better, sometimes significantly, on tasks where less data is available, such as parsing. The difference in performance is significant, and since we use both dropout and early stopping, we conjecture that it is not related to over-fitting. In fact, one could expect that removing these blocks will improve performance on ImageNet alone if they were truly useless for this task. In contrast, we see in Table 4 that these blocks either don’t affect or slightly improve performance. This leads us to conclude that mixing different computation blocks is in fact a good way to improve performance on many various tasks. We believe that this treads a path towards interesting future work on more general deep learning architectures, especially since our model shows transfer learning from tasks with a large amount of available data to ones where the data is limited.",summary_1000_section_model,Large Scale Multi-Domain Multi-Task Learning with MultiModel,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.","The paper presents a multi-task, multi-domain model based on deep neural networks. The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition. The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic. The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task.

The paper is well written and easy to follow.

The contributions of the paper are novel and significant. The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring. The experiments clearly show the viability of the approach and give interesting insights. This is surely an important step towards more general deep learning models. 

Comments:

* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database. Moreover, some databases could be used for different tasks, such as WSJ or ImageNet.

* The training procedure of the model is not explained in the paper. What is the cost function and what is the strategy to train on multiple tasks ? The paper should at least outline the strategy.

* The experiments are sufficient to demonstrate the viability of the approach, but the experimental setup is not clear. Specifically, there is an issue about the speech recognition part of the experiment. It is not clear what the task exactly is: continuous speech recognition, isolated word recognition ? The metrics used in Table 1 are also not clear, they should be explained in the text. Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used. Information about the detailed setup is also lacking, specifically which test and development sets are used (the WSJ corpus has several sets).

* Using raw waveforms as audio modality is very interesting, but this approach is not standard for speech recognition, some references should be provided, such as:
P. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26–30.
D. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE.
T. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals. Learning the Speech Front-end With Raw Waveform CLDNNs. Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015.

Revised Review:
The main idea of the paper is very interesting and the work presented is impressive. However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks. The experiments are interesting, except for the WSJ speech task, which is almost meaningless. Indeed, it is not clear what the network has learned given the metrics presented, as the WER on WSJ should be around 5% for speech recognition.
I thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition. A simpler speech task such as Keyword Spotting could also be investigated.
"," This paper presents a multi-task multi-modal model, which is trained on multiple tasks ( translation, speech recognition, COCO, multiple languages, multiple tasks on a single dataset, and a speech recognition corpus on another dataset ). It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on, and the authors show that adding it never hurts performance and in most cases improves it on all tasks.",81
30,6299,hybrid,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines. The problem can also be understood as constructing a function f(G, nS , q) to predict nT , where the functional form of f(·) is generally unknown and has to be learned from a training dataset consisting of samples like (nS , q, nT ). Since nT is unknown, the problem cannot be solved by conventional search algorithms such as A⇤-search [11], which seeks to find paths between the given source and target nodes. For the KBC example in Figure 1(a), having access to the current node nt = Hawaii alone is not sufficient to know that the best action is moving to nt+1 = USA. Second, to address the challenge of sparse rewards, M-Walk exploits the fact that the MDP transition model is known and deterministic.2 Specifically, it combines Monte Carlo Tree Search (MCTS) with the RNN to generate trajectories that obtain significantly more positive rewards than using the RNN policy alone. The rest of the paper is organized as follows: Section 3 develops the M-Walk agent, including the model architecture, the training and testing algorithms.3 Experimental results are presented in Section 4. To see this, we observe from Figure 1(b) that once an action at (i.e., an edge in Ent or “STOP”) is selected, the next node nt+1 and its associated Ent+1 and Nnt+1 are known. The objective is to learn a policy that maximizes the terminal rewards, i.e., correctly identifies the target node with high probability. Recall from Section 2 (e.g., (1)) that one challenge in applying RL to the graph-walking problem is that the state st nominally includes the entire history of observations. We will explain in Section 3.2 how such parameter sharing enables indirect updates to the policy ⇡✓ via Q-learning from off-policy data. To address this issue, we apply the same FCN f✓(·) to 5There could be alternative ways to compute the score, such as Score(n) = maxsT!n Q✓(sT , STOP). Inspired by this recursion, we propose using the GRU-RNN [4] to encode qt into a vector representation6: qt+1 = f✓q (qt, [hA,t, hat,t, nt+1]) with initialization q0 = f✓q (q, [0, 0, nS ]), where ✓q is the model parameter, and hat,t denotes the vector hn0,t at n0 = at. Three Glass Puzzle The Three Glass Puzzle [20] is a problem studied in math puzzles and graph theory. WN18RR [6] is created from the original WN18 [2] by removing various sources of test leakage, making the dataset more challenging. We use HITS@1,3 and mean reciprocal rank (MRR) as the evaluation metrics for WN18RR, and use mean average precision (MAP) for NELL995,7 where HITS@K computes the percentage of the desired entities being ranked among the top-K list, and MRR computes an average of the reciprocal rank of the desired entities. We compare against RL-based methods [38, 5], embedding-based models (including DistMult [39], ComplEx [32] and ConvE [6]) and recent work in logical rules (NeuralLP) [40]. For all the baseline methods, we used the implementation released by the corresponding authors with their best-reported hyperparameter settings.8 The details of the hyperparameters for M-Walk are described in Appendix B.2.2 of the supplementary material. We ran the experiments three times and report the means and standard deviations (except for PRA, TransE, and TransR on NELL995, whose results are directly quoted from [38]). We observed that the novel neural architecture of M-Walk contributes an overall 1% gain relative to MINERVA on NELL995, and it is still 1% worse than M-Walk, which uses MCTS for training and testing. Third, we analyze the performance of M-Walk under different numbers of MCTS rollout simulations and different search horizons on WN18RR dataset, with results shown in Figure 5(a). We observe that M-Walk outperforms the strong baseline ConvE by 4.6–10.9% in samples that require 2 or 3 steps, while it is nearly on par for paths of length one. Therefore, M-Walk does better at reasoning over longer paths than ConvE. To examine this effect, we show in Figure 5(c)-top the HITS@K accuracies when the ground truth is in the candidate set.9 It shows that M-Walk has very high accuracy in this case, which is significantly higher than ConvE (80% vs 39.6% in HITS@1). These observations point to an important direction for improving M-Walk in future work: increasing the chance of covering the target by the candidate set. Finally, in Table 4, we show examples of reasoning paths found by M-Walk.10 Reinforcement Learning Recently, deep reinforcement learning has achieved great success in many artificial intelligence problems [17, 24, 25]. Note that the former is constructed from the visit counts of all the edges connected to the MCTS root node; it only uses information near the root node to improve the policy. Recent approaches have demonstrated limitations of these prior approaches: they suffer from cascading errors when dealing with compositional (multi-step) relationships [10]. Hence, recent works [8, 18, 10, 15, 30] have proposed approaches for injecting multi-step paths such as random walks through sequences of triples during training, further improving performance on KBC tasks. IRN [23] and Neural LP [40] explore multi-step relations by using an RNN controller with attention over an external memory. Empirically, our proposed algorithm outperforms both DeepPath and MINERVA in the KBC benchmarks.11 We developed an RL-agent (M-Walk) that learns to walk over a graph towards a desired target node for given input query and source nodes. Furthermore, we also performed extensive experimental analysis to understand M-Walk. 11A preliminary version of M-Walk with limited experiments was reported in the workshop paper [22].",clarity_1000_section_model,M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines.","Summary of the paper:  - This paper proposes a walk-based method for knowledge graph completion (KGC).  - For triples (s, q, o), an agent is trained so that given s and q, it traverses a path from the initial node s to the   terminal node t in a knowledge graph.  - This problem is formulated as that of Q-learning using RNN to model policy and Q functions, but the issue is the   sparseness of rewards.  - Monte Carlo tree search is used to overcome this issue.  Pros:  - The paper is clearly written.  - The approach is natural and sound.  - Very good performance compared with existing KGC methods.  - Ablation study with PG-Walk and Q-Walk is nice.  Cons:  - The work is mostly empirical, without much theoretical justification (admittedly, as in many deep learning papers).  - Not much novelty or surprise, given the success of AlphaGo.  This is a type of paper in which a successful technique   in other fields is applied to a different field.  I could not see exactly what is the novelty/creativity in terms of   how MCTS is applied for this specific problem of KGC.  - No analysis on running time is provided.   General Comments:  This paper is well written (but I would like the content of Section 4.2 to be more organized --- Page 7 consists of a single paragraph now!).  The novelty of the proposal is somewhat questionable, because reinforcement learning has already been introduced for KGC, and the application of MCTS naturally comes to mind.  Although this work is mostly empirical and little theoretical contribution is provided, it still achieves the state-of-the-art performance, and that could be a merit for its publication.  Questions:  * Related work  MCTS has been introduced in walk-based KGC by Sheng et al. in ICLR 2018 Workshop (ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search), and this work must be cited.  Please elaborate on how M-Walk differs from ReinforceWalk as much as you can tell from the extended abstract.  * Experiments:  - Dataset:  Is there any reason the comparison is done with NELL995 and WN18RR?  FB15k is also the standard dataset.  - Baselines  Why do the set of compared methods differ between datasets?  In particular, we know that TransE and TransR are somewhat weaker compared with more recent bilinear models, such as ComplEx, ANALOGY, and ConvE.  - Evaluation metrics  Also, why is the MAP score used on NELL995 whereas HITS@/MRR are used on WN18RR?  I have nothing against MAP scores, but typically HITS@/MRR are reported in literature on KGC even on the NELL-995 dataset (e.g., the MINERVA paper); using different metrics makes it difficult to validate the experimental results.  - Figure 6(a)  Is there any explanation as to why accuracy drops as the number of roll outs are increased to 256?  * Application to classification task  The method is only concerned with prediction of knowledge triple in which only one entity is missing. Can the proposed method be applied to the triple classification task where the truth of a given triple must be predicted? This is more or less straightforward in score-function-based models (provided that a suitable threshold can be determined), but I cannot see how this can be addressed with walk-based methods in general.  * Running time  Training and inference both involves roll-out, which raises concern about the running time of the algorithms.  How does M-walk fare against scoring-based methods (e.g., ComplEx) or MINERVA in terms of running time?  Please provide the time required for training and inference.  I am curious about inference time in particular. "," The paper is well written and easy to follow. The problem setting of the paper is not clear to me. It would be better if the authors could clarify this distinction in the writing. I would also suggest to the authors to clarify the gap between theory and empirical results. I am not familiar with the literature enough to tell whether the proposed method is the cleanest way to characterize the current state-of-the-art in the graph-walking problem setting, and whether it can be scaled up or scaled back to a more practical level.The paper is generally easy to read and understand.",102
31,837,hybrid,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art. Since the introduction of the “AlexNet” architecture [14] in the 2012 ImageNet competition, ConvNets have dominated state-of-the-art performance across a variety of computer vision tasks, including object-detection, image segmentation, image classification, face recognition, human pose estimation and tracking. In conjunction with these advances as well as the evolution of network architectures, several design best practices have emerged [8, 21, 23, 24]. This functionality can be achieved by stacking many small filters or using large filters in the network; notably, the first choice can be implemented with fewer operations (faster, fewer parameters) and also allows inclusion of more nonlinearities. Third, dimensionality reduction (1×1 convolutions) before spatially aggregating filters (e.g.3×3) is supported by the fact that outputs of neighbouring filters are highly correlated and therefore these activations can be reduced before aggregation [23]. Fourth, spatial factorization into asymmetric filters can even further reduce computational cost and 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. Fifth, it is important to normalize the responses of each feature channel within a batch to reduce internal covariate shift [11]. To demonstrate the benefits of our proposed spatiotemporal ResNet architecture, it has been evaluated on two standard action recognition benchmarks where it greatly boosts the state-of-the-art. Yet another study compared several approaches to extending ConvNets into the temporal domain, but with rather disappointing results [13]: The architectures were not particularly sensitive to temporal modelling, with a slow fusion model performing slightly better than early and late fusion alternatives; moreover, similar levels of performance were achieved by a purely spatial network. Another research branch has investigated combining image information in network architectures across longer time periods. That approach takes advantage of the local spatial similarity in images; however, it only yields a minor increase over their baseline, which is a two-stream VGG-16 ConvNet [21] used as the input to their convolutional RNN. Batch normalization [11] and ReLU [14] are applied after each convolution; the network does not use hidden fc, dropout, or max-pooling (except immediately after the first layer). In preliminary experiments we found that direct connections between identical layers of the two streams led to an increase in validation error. We conjecture that these results are due to the large change that the signal of one network stream undergoes after injecting a fusion signal from the other stream. Therefore, we developed a more subtle alternative solution based on additive interactions, as follows. To enable learning of spatiotemporal features at all possible scales, this modification is applied before the second residual unit at each spatial resolution of the network (indicated by “skip-stream” in Table 1), as exemplified by the connection at the conv5_x layers in Fig. Based on the above observations, we developed a novel approach to temporal residual connections that builds on the ConvNet design guidelines of chaining small [21] asymmetric [10, 23] filters, noted in Sec. This allows the straightforward use of standard two-stream ConvNets that have been pre-trained on large-scale datasets e.g.to leverage the massive amounts of training data from the ImageNet challenge. From left to right, top to bottom, the first row shows the convolutional and pooling building blocks, with the filter and pooling size shown as (W ×H × T,C), denoting width, height, temporal extent and number of feature channels, resp. In the last two rows we show the output size of these metalayers as well as the receptive field on which they operate. We also apply random cropping and scale augmentations: We randomly jitter the width and height of the 224× 224 input frame by ±25% and also randomly crop it from a maximum of 25% distance from the image borders. We found that this strategy significantly reduces the training duration with the drawback that each loss does not capture all available information. For our final model, we equip the spatiotemporal ResNet with a temporal max-pooling layer after pool5 (see Table 1, temporal average pooling led to inferior results) and continue training as above with the learning rate starting from 10−4 for 2K iterations followed by 10−5. For fair comparison, we follow the evaluation procedure of the original two-stream work [20] by sampling 25 frames (and their horizontal flips). However, rather than using 10 spatial 224× 224 crops from each of the frames, we apply fully convolutional testing both spatially (smallest side rescaled to 256) and temporally (the 25 frame-chunks) by classifying the video in a single forward pass, which takes ≈250ms on a Titan X GPU. Second, we consider HMDB51 [15], which has 6766 videos that show 51 different actions and generally is considered more challenging than UCF0101 due to the even wider variations in which actions occur. For both datasets, we use the provided evaluation protocol and report mean average accuracy over three splits into training and test sets. Interestingly, research in neuroscience also suggests that the human visual cortex is equipped with connections between the dorsal and the ventral stream to distribute motion information to separate visual areas [3, 27]. We achieve this by simply averaging the L2-normalized SVM scores of the FV-encoded IDT descriptors (i.e.HOG, HOF, MBH) [29] with the L2-normalized video predictions of our ST-ResNet*, again without softmax normalization. These relatively larger performance decrements again underline that our approach is better able to capture the available dynamic information, as there is less to be gained by augmenting it with IDT. Still, there is a benefit from the hand-crafted IDT features even with our approach, which could be attributed to its explicit compensation of camera motion.",clarity_1000_section_model,Spatiotemporal Residual Networks for Video Action Recognition,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art.","The paper presents a novel architecture that 1) combines residual networks with two-stream convolutional networks, and 2) injects connections from the motion stream to the appearance stream, to be able to capture spatio-temporal features. The paper shows experiments in both of the main action recognition datasets, achieving state-of-the-art accuracy in both. Overall I think the paper is great: good idea, careful experimentation, great results and clearly written.   Although the basic components of the architecture are pre-existing, I think that the high performance and careful experimentation and description make it a very useful contribution.   I only miss some experiments to visualize what kinds of spatio-temporal features are being learned.   Since large temporal windows are important (278-280) I would add a relevant reference: Long-term Temporal Convolutions for Action Recognition, Gul Varol, Ivan Laptev, Cordelia Schmid  Small typos:  (L 187)  ""resp.""  (L 255) ""UCF0101""  "," The paper is well-written and easy to follow, apart from a couple of places in the derivations ( see my questions ). I would also suggest the authors to reduce such abuse of notations ( e.g., \theta^ {... }, \timescale_t,... } in the final work, because these terms are not used widely in the literature. Also, it would be nice if the authors could provide more details about the training and evaluation of the model.",76
32,22112,hybrid,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values. While a large body of the machine learning literature is built upon the assumption of having access to complete datasets, in many real-world problems only incomplete datasets are available. Single imputation methods try to replace each missing value with a plausible value that is the best fit given the value of other correlated features and knowledge extracted from the dataset (Hastie et al., 1999; Anderson, 1957). While MI offers interesting statistical insights about the reliability of analysis on incomplete data, the insight is imprecise as it is mainly concerned about the population of data samples rather than individual instances. While these methods are easy to implement and analyze, they often fail to capture the complex feature dependencies as well as structures present in many problems. For instance, Mattei & Frellsen (2018) suggested a method based on deep latent variable models and importance sampling that offers a tighter likelihood bound compared to the standard VAE bound. Also, these methods are often complicated to be applied in practical setups by practitioners. A few exceptions exist such as Bayesian models and decision trees that permit direct analysis on incomplete data (Nielsen & Jensen, 2009; Zhang et al., 2005). However, given an incomplete training dataset and without any known causal structure as a priori, learning such models is a very challenging problem with the complexity of at least NP-complete to learn the network architecture in addition to an iterative EM optimization to learn model parameters (Darwiche, 2009; Neapolitan et al., 2004). Similarly, for the second part, we are interested in obtaining a distribution over the possible target assignments and the confidence of each class rather than maximum likelihood class assignments. Instead, we use recent advances in GAN stabilization and training to improve the training process (see Section 3.4). Additionally, in our experiments, we provide supporting evidence that this simple loss function enables us to sample from the conditional distribution and prevents biased inclinations toward distribution modes. For instance, for a specific test sample at hand, if a certain missing feature is a strong indicator of the target class, we would like to observe the impact of different imputations for that feature on the final hypothesis. Input: G (trained imputer), D (dataset) Output: Fθ (trained predictor) foreach Training Epoch do foreach (xi, ki, yi) in D do z ∼ N(0, I) x̂i ← ki xi+(1−ki) G(xi,ki, z) ypredi ← Fθ(x̂i) loss← L(yi, ypredi ) Backpropagate loss Update Fθ Algorithm 2: Estimating target distributions. The first assumption is consistent with the theoretical analysis of generative adversarial networks that they can converge to the true underlying distribution (Arora et al., 2018; Liu et al., 2017). It should be noted that, while Zhang et al.(2018) suggests using a single self-attention layer in the middle of the network, we observed consistent improvements by inserting multiple self-attention layers before each residual block within the network. We explored best TTUR learning-rate settings from the set of {0.001, 0.0005, 0.0001, 0.00005}. To evaluate the proposed method we use CIFAR-10 (Krizhevsky & Hinton, 2009) as an image classification dataset as well as three non-image datasets: UCI Landsat (Dua & Graff, 2017)2, MIT-BIH arrhythmia (Moody & Mark, 2001), and Diabetes classification (Kachuee et al., 2019) 3. Also, while different encoding and representation methods are suggested in the literature to handle categorical features (Jang et al., 2016; Nazabal et al., 2018), in this paper, we take the simple approach of encoding categorical variables using one-hot representation and smoothing them by adding Gaussian noise with zero mean and variance equal to 5% of feature variances. We would like to note that while the suggested solution in this paper is readily compatible with MAR structures, in our experiments, to simplify the presentation of results and to have a fair comparison with other work that does not support the MAR assumption, we limited the scope of our experiments to MCAR. Furthermore, to simulate incomplete datasets and to make sure the same features are missing without explicitly storing masks, we use hashed feature vectors to seed random number generators used to sample missing features. More detail is provided in Appendix C. Fréchet inception distance (FID) (Heusel et al., 2017) score is used to measure the quality of missing data imputation in experiments with images5. We also considered using root means squared error (RMSE); however, we decided not to use this measure as we observed an inconsistent behavior using RMSE in our comparisons as RMSE favors methods that show less variance rather than realistic and sharp samples from the distribution. We compare our results with MisGAN (Li et al., 2019) and GAIN (Yoon et al., 2018) as the state of the art imputation algorithms based on GANs as well as basic denoising autoencoder (DAE) (Vincent et al., 2008) and multiple imputation by chained equations (MICE) (Buuren & Groothuis-Oudshoorn, 2010) as baselines. Due to scalability issues, we were only able to use MICE for the smaller non-image datasets. One possible explanation for this behavior might be the fact that GAIN has an MSE loss term acting similar to an autoencoder loss smoothing noisy missing pixels. Regarding the MIT-BIH experiemts, GI outperforms other work for missing rates more than 30% while achieving similar accuracies to GAIN for lower missing rates. As it can be seen from the plots, GI provides results closest to the ideal case of having average confidence values equal to average accuracies. In order to provide further insight into the operation of GI and how imputations can potentially influence the outcomes of predictions, we conduct experiments on a synthesized dataset. One possible explanation could be the fact that imputing missing data with a uniform structure can be done by processing local regions and does not require attending to different distant regions across the image. From Table 6 it can be inferred that as the rate of missingness increases, the benefits of the suggested predictor algorithm increase significantly.",clarity_1000_section_model,Generative Imputation and Stochastic Prediction,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values.","This paper proposes a method to impute missing features using a generative model and train a predictive model on top of imputed dataset to improve classification results. They first train a GAN model where the generator outputs an imputed representation of the input and discriminator is trained to predict if an individual features (such as a pixel) is imputed or not. Given the generator and incomplete sample, they train a predictor using the output of the generator, imputed sample, as input. Their main contribution is using a MC averaging to compute the prediction by repetitively sampling from the noise variable, z, and generating different imputations from generator. They show that the proposed model improves upon the previous SOTA on final classification performance.

Overall the paper is clearly written. But I do feel it is a bit incremental over the GAIN approach. The overall GAN architecture is very similar to GAIN's and although stochastic prediction shows clear improvements it is a bit straightforward. However, I think the uncertainty of the imputations and its effect on the final prediction is interesting. I suggest the authors to extend this part with more detailed analysis.

There are several parts that are confusing/missing in the paper:

- In GAIN, they use a hint vector as an input to the discriminator. They show that without the hint vector, there is no unique solution (this is shown without the MSE loss). The authors do not use this vector in their approach (as in Figure 1) and it is not clear to me if it causes any instabilities or if multiple experiments yield similar results or if the stochastic prediction benefits from this.
- On what type of examples GI is more accurate than other models? Since stochastic prediction is the main difference from GAIN, is this related to the multi-modality of the noisy examples?
- Can you explain the difference between the results in Figure-7 and Table-2? Results between the two mismatch.
- I think the statement in the first paragraph in Section 4.4 that ""MSE loss term would act as a denoising loss smoothing noisy missing pixels"" could be misleading. MSE is used with mask in GAIN, hence it only applies to the observed features during training. Its effect on smoothing noisy missing pixels is not clear.


I think the paper would benefit if the authors could explain/show:
- Increasing the missing rate would also increase the possibility that the ground truth be a more multi-modal distribution. Especially in rectangular generation part where it can remove a complete object. Does stochastic averaging benefit more in this case?"," The paper is well written and easy to follow, apart from a couple of places in the derivations ( see my questions ). However, I have some reservations about the presentation of the results : - It is not clear to me what the main idea of the proposed method is, and how it can be applied to a practical application.- It seems to me that the paper is missing a lot of detail about the training of the generator network, which makes it hard to understand what exactly is going on in the experiments.",96
33,18864,hybrid,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting. Recent works have used this framework to achieve impressive feats such as learning to classify using one labeled image per class (Snell et al., 2017; Finn et al., 2017), learning unsupervised update rules that generalize to different domains (Metz et al., 2018), and accelerating training procedures that are millions of steps long (Flennerhag et al., 2018). DIMCO generalizes well to novel datasets because its learning objective encourages the model to compactly use all of its degrees of freedom, thus enabling it to effectively compare datapoints while only requiring a small number of bits per datapoint. Derive a generalization bound for meta-learning that makes the tradeoff between taskspecific and task-wise overfitting concrete. Notably, we suggest that certain previous meta-learning methods may have benefitted from implicit regularization. We present experiments in Section 5 and conclude the paper with a discussion about limitations and future directions of our approach in Section 6. This can be problematic since in many works, the task-specific learning algorithm is usually just a byproduct of whatever clever meta-learning loss was proposed. We propose a factorized discrete representation scheme which enables us to represent discrete distributions with exponentially fewer parameters compared to listing the probability of each event. We similarly perform image retrival by mapping each support image to its most likely code (10) and for each query image retrieving the support image that has highest (11). Regularizing Meta-Learners The ability to generalize to novel datasets is critical in meta-learning benchmarks, and even more so in benchmarks such as Meta-Dataset (Triantafillou et al., 2019), where a model is tested on datasets from an unseen domain. The following works have reported benefits from reducing the number of such task-specific parameters: Lee & Choi (2018) learns a subset of the full network to alter during task-specific learning, Rusu et al.(2018) explicitly represents each task with a low-dimensional latent space, and Zintgraf et al.(2018) alters only a pre-specified subset of the full network during task-specific learning. We showed through Theorem 1 that restricting inner-loop expressivity reduces the generalization gap; this provides theoretical understanding to this consensus that meta-learning models with simple task-specific learners generalize to new tasks more easily. Also related is the deterministic information bottleneck (Strouse & Schwab, 2017) which extends the information bottleneck by minimizing H(X̃) rather than I(X̃;X). These three approaches to generalization are related via the chain of inequalities I(X̃;X) ≤ H(X̃) ≤ log |X̃|, which is tight when X̃ is an efficient code. Recent deep learning methods directly learn discrete representations, by learning variational autoencoders with discrete latent variables (Rolfe, 2016; van den Oord et al., 2017; Razavi et al., 2019) or maximizing the mutual information between representation and 1 In practice, we add log probabilities instead of multiplying probabilities for numerical stability. Additionally, their method solves a minimum cost flow problem within each batch to find the locally optimal code, whereas DIMCO is able to directly compute its loss function. We randomly initialize weights for the 4-layer convnet and use pretrained weights for the Inception network This experiment empirically verifies whether mutual information I(X̃;Y ) is a reasonable metric for quality of representation. We observed similar trends when training with with previously proposed loss functions: we visualize these results in Figure 5 of the appendix due to space constraints. Results in Figure 3 show that the compact code of DIMCO takes roughly an order of magnitude less memory for similar performance to N-pair loss, and requires less query time as well. We additionally note that DIMCO is able to train using large backbones without significantly overfitting, whereas experiments reported in Mishra et al.(2017) indicate that MAML (Finn et al., 2017) overfits tremendously when using a deeper backbone. This challenging experimental setup measures how much generalizable information the model can extract from a limited set of datasets; it can be seen as the meta-learning analogue of measuring the performance of a classifier trained with a small dataset. We report the average and standard deviation of the top 5 results of a random hyperparameter search (see appendix for details). Towards Explicit Meta-Regularization In Section 4, we have suggested with analogy to Theorem 1 that the benefits of some previous meta-learning methods can be attributed to implicitly being regularized by reducing the expressivity of their task-specific learners. In future work, we would like to explore explicit meta-regularization schemes that can be applied to other problems (regression, reinforcement learning etc.) Along with showing that the traditional support/query split is not strictly necessary, we demonstrated in Section 5.4 that removing it has the benefit of enabling meta-learning in datasets having one image per class. (12) Assuming that the approximate distribution q(·) is sufficiently close to p(y|x̃), minimizing (12) can be seen as arg min xent(Y, X̃) ≈ arg minEy∼Y,x̃∼X̃ [ − log p(y|x̃) ] (13) = arg minH(Y |X̃) = arg max I(X̃;Y ), (14) where the last equality uses the fact that H(Y ) is independent of model parameters. (24) We similarly bound the error caused by estimating L with a finite number of tasks sampled from τ . Combining equations (26, 24), we have with high probability∣∣∣∣L(τ, θ)− − 1 n n∑ i=1 Î(X̃(XT i , θ);YT i) ∣∣∣∣ (27) ≤ ∣∣∣L(τ, θ)− L̂(τ, θ)∣∣∣+O( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (28) ≤O (√ dΘ n log n dΘ ) +O ( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (29) Hardware Every experiment was conducted on a single Nvidia V100 GPU with CUDA 9.2. Each experiment was performed with different fixed initial seeds; we manually fix seeds with manual_seed() for python, pytorch, and numpy. We additionally show in Figure 5 the correlation between 1-shot accuracies, Recall@1, and NMI using three previously proposed losses (triplet, npair, protonet).",clarity_1000_section_model,Discrete InfoMax Codes for Meta-Learning,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting.","This paper presents DIMCO, a meta-learner that is trained by maximizing mutual information between a discrete data representation and class labels across tasks. DIMCO is inspired by an information theoretic lower bound on the generalisation gap for meta-learning, which the authors argue identifies overfitting in the task learner as the bottleneck. 

This work proposes to constrain a learner to output discrete codes that are learned to capture the mutual information with class labels. While the idea of using discrete codes is interesting, its presentation in the manuscript is not well motivated and at times hard to follow. This makes it challenging to evaluate the novelty, validity, and generality of the proposed approach. Meanwhile, the empirical evaluation is somewhat lacking. Thus, I do not believe this work is ready for publication in its current form.

Detailed comments

My main concern is with respect to the primary contribution of this paper, a generalisation bound on meta-learning. The bound appears to be on a multi-task loss without task adaptation, and thus the claims made with respect to the theorem seem somewhat over-reaching. I also believe the VC-dimensionality of the encoder is missing in Eq. 4? If so, this changes the interpretation since the length of the code and the expressivity of the encoder are interrelated. Further, I would welcome a deeper analysis of the theorem and its implications. The current interpretation states that the number of tasks is independent of the size of each task, hence given many tasks, using minimal representations is an effective approach to meta-generalisation. Yet minimal representations is a well-known idea and has features in several works that use mutual information as a regularizer, most notably works on the Information Bottleneck. 

Another reservation I have is the use of mutual information between encoder representations and class labels as a loss function (Eq. 1). It lacks context and a proper motivation, especially since the analysis of [1] shows that the loss function in Eq. 1 is the cross-entropy objective. The authors make a similar analysis in Appendix A and argue that Eq. 1 differs in that cross-entropy is an approximation because it adds a parametrized linear layer on top of \tilde{X}. Thus, in the absence of that layer they collapse to the same objective. As DIMCO itself directly extract class label predictions from \tilde{X}, I fail to see a difference between the loss in Eq. 1 and a cross-entropy objective. 

The main motivation behind their loss objective is that it does not require a support / query set. This does not seem to be a feature of the mutual information objective itself, but rather a choice made by the authors. I would have liked a deeper discussion of this seeing as the authors make it a central tenet of DIMCO. Prior works use a support set as a principled means of doing meta-learning: meta-training explicitly takes into account that at test time, the learner will be given a small support set from which to learn how to query points. As far as I understand, DIMCO does not take this into account during meta-training.  At meta-test time however, DIMCO does use a support set to map query points (Eqs. 10 and 11). Why should we break protocols between meta training and testing? Are there any downsides to doing so?  

Empirically, I find the CUB experiment compelling but would welcome some ablations. What are the trade-offs between p and d? Can DIMCO outperform N-pair when number of bits are unconstrained?

miniImagenet is a standard benchmark in few-shot learning, but I am unable to find a results table - could the authors please provide results on the standard setup so that the method can be compared against known baselines? Further, would the results currently presented hold in a N-way-5-shot setup?

As for the constrained version of miniImagenet that the authors propose, I am not convinced this is an interesting protocol. In general, the miniImagenet task distribution is created by N-way permutations of the classes in the meta-set (e.g. meta-training tasks are combinations of the 64 classes in the meta-training set). By keeping the number of classes constant but reducing the number of images per class, this protocol is not reducing samples per task: a task is always defined as 5/20-way-1-shot (Fig. 4). Instead, the effect should be that tasks are in (greater) violation of the task i.i.d. assumption. Thus, I question whether this setup demonstrates the trade-offs the authors present in Theorem 1 and whether the results can be interpreted in light of it. 

Finally, that both experiments are image-based raises questions as to the generality of the method. The paper could be considerably strengthened by evaluating DIMCO on a non-image task, or if not discuss the method’s limitations.  

The idea of discrete codes for few-shot classification is interesting and sufficiently novel, I am likely to increase my score if my concerns are addressed and the experimental section is strengthened.

Further questions and comments:

- I am unable to parse Eq. 11 - what does the notation \prod_i p_{\tilde{x}_i, i} mean? 
- It is unnecessarily hard to follow the proof of theorem 1. It would help the reader if you restated relevant definitions, such as Eq. 1, since the difference with Eq. 23 is very subtle. It would also be helpful to explain how the summand in Eq. 25 differs from either, and Eq. 26 could be expanded or briefly explained after the derivation. 
-  Because DIMCO is trained with backpropagation, the interpretation of Eq. 5 as d independent events seems invalid. How does it affect the method if they are not independent?
- Overloading X and Y as both random variables and mini-batch samples creates unnecessary confusion. I believe the objective in Eq. 1 is approximated, not calculated exactly? For instance, the mutual information in Eq. 8 is with respect to a mini-batch, so should it not be \hat{I}? 
- p^j_{ik} in Eq. 9 is undefined.
- Eq. 9 is interpreted as an exact entropy, however it appears to be a mini-batch approximation to the true entropy? 

References 
[1] Achille and Soatto. Emergence of Invariance and Disentanglement in Deep Representations. JMLR. 2018."," The paper is well-written and easy to follow, and the experimental results show significant improvements over state-of-the-art meta-learning methods.The paper is generally well-organized and well-structured, and I enjoyed reading it. I would suggest to the authors to reconsider the structure of the paper, as some sections are hard to follow without first reading through the introduction, and some sections require rewriting to make it easier to follow. I think the paper would benefit from a more clear structure that allows readers to delve into details at different levels, and provide additional insight as to what is going on in the experiments.",102
34,13063,hybrid,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all. But in each case, the network was designed and tuned specifically for the problem at hand. Natural language processing models have been shown to benefit from a multi-task approach a long time ago (Collobert & Weston, 2008), and recently multi-task machine translation models (MinhThang Luong, 2015) have even been shown to exhibit zero-shot learning when trained on multiple languages (Melvin Johnson, 2016). Speech recognition has also been shown to benefit from multi-task training (Seltzer & Droppo, 2013), as have some vision problems, such as facial landmark detection (Zhang Z., 2014). But no competitive multi-task multi-modal model has been proposed, so the above question remains unanswered. (3) COCO image captioning dataset (Lin et al., 2014), used for image captioning. (4) WSJ parsing dataset (Marcus et al., 1999), used for constituency parsing. While the MultiModel is only a first step and will be improved in the future, two key insights are crucial to making it work at all and are our main contributions. We design modality nets to be computationally minimal, promoting heavy feature extraction and ensuring that the majority of computation is performed within the domain-agnostic body of the model. Two design decisions were important: • The unified representation is variable-size. While a fixed-size representation is tempting and easier to implement, it creates a bottleneck and limits the performance of the model. For example, separable convolutions were introduced in the Xception architecture (Chollet, 2016) and were not applied to text or speech processing before. We find that each of these mechanisms is indeed crucial for the domain it was introduced, e.g., attention is far more important for languagerelated tasks than for image-related ones. As already said above, the encoder and decoder are constructed using 3 key computational blocks to get good performance across different problems: (1) Convolutions allow the model to detect local patterns and generalize across space. We refer the reader to (Chollet, 2016) for a complete definition; here we will denote a depthwise separable convolution with weights Wh×w corresponding to f kernels of size h × w applied to an input tensor x with stride s and dilated by a factor d (see (Yu & Koltun, 2015)) as SepConvd,s,f (W,x). The source tensor is finally passed through two different pointwise convolutions to generate the memory keys K and values V and the query keys, memory keys and memory values are used to apply the attention mechanism between the self-attended target and the source (see Figure 3). On the output side, the language modality takes the decoded output of the body and performs a learned linear mapping, followed by a Softmax, resulting in a probability distribution over the token vocabulary. The spectral modality does not perform any striding along the frequency bin dimension, preserving full resolution in the spectral domain. Convolutional architectures yielded good results on word-level neural machine translation starting from (Kalchbrenner & Blunsom, 2013) and later in (Meng et al., 2015). These early models used a standard RNN on top of the convolution to generate the output and had a bottleneck there that hurt performance, especially on longer sentences, similarly to the limitations of RNN sequence-to-sequence models without attention (Sutskever et al., 2014; Cho et al., 2014). This idea, introduced in WaveNet (van den Oord et al., 2016) and also used in MultiModel (see above) significantly improves efficiency. Depthwise separable convolutions were first studied by Sifre (Sifre & Mallat, 2013) and later they were used to get good results on large-scale image classification with Xception (Chollet, 2016). We focused our experiments so as to answer the following questions: (1) How far is the MultiModel trained on 8 tasks simultaneously from state-of-the-art results? Especially the 4 translation problems behave very similarly, so we decided to not include them all in each comparison but we focused on the more varied problems instead. The results we achieve are similar to the ones task-specific models get without heavy tuning, e.g., on English-French translation we improve on the recent Extended Neural GPU results (Kaiser & Bengio, 2016). Since we are comparing different instantiations of the same model, we report two internal metrics: the negative log-perplexity and per-token accuracy (measured on the development set). As can be seen from the results in Table 2, the joint 8-problem model performs similarly to single-model on large tasks, and better, sometimes significantly, on tasks where less data is available, such as parsing. The difference in performance is significant, and since we use both dropout and early stopping, we conjecture that it is not related to over-fitting. In fact, one could expect that removing these blocks will improve performance on ImageNet alone if they were truly useless for this task. In contrast, we see in Table 4 that these blocks either don’t affect or slightly improve performance. This leads us to conclude that mixing different computation blocks is in fact a good way to improve performance on many various tasks. We believe that this treads a path towards interesting future work on more general deep learning architectures, especially since our model shows transfer learning from tasks with a large amount of available data to ones where the data is limited.",clarity_1000_section_model,Large Scale Multi-Domain Multi-Task Learning with MultiModel,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.","The paper presents a multi-task, multi-domain model based on deep neural networks. The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition. The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic. The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task.

The paper is well written and easy to follow.

The contributions of the paper are novel and significant. The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring. The experiments clearly show the viability of the approach and give interesting insights. This is surely an important step towards more general deep learning models. 

Comments:

* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database. Moreover, some databases could be used for different tasks, such as WSJ or ImageNet.

* The training procedure of the model is not explained in the paper. What is the cost function and what is the strategy to train on multiple tasks ? The paper should at least outline the strategy.

* The experiments are sufficient to demonstrate the viability of the approach, but the experimental setup is not clear. Specifically, there is an issue about the speech recognition part of the experiment. It is not clear what the task exactly is: continuous speech recognition, isolated word recognition ? The metrics used in Table 1 are also not clear, they should be explained in the text. Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used. Information about the detailed setup is also lacking, specifically which test and development sets are used (the WSJ corpus has several sets).

* Using raw waveforms as audio modality is very interesting, but this approach is not standard for speech recognition, some references should be provided, such as:
P. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26–30.
D. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE.
T. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals. Learning the Speech Front-end With Raw Waveform CLDNNs. Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015.

Revised Review:
The main idea of the paper is very interesting and the work presented is impressive. However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks. The experiments are interesting, except for the WSJ speech task, which is almost meaningless. Indeed, it is not clear what the network has learned given the metrics presented, as the WER on WSJ should be around 5% for speech recognition.
I thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition. A simpler speech task such as Keyword Spotting could also be investigated.
"," The paper is well written and easy to follow, with a good balance between mathematical notation and intuitive explanations of the main ideas. I would like to see more results on the COCO dataset, but other than that, I think it is an interesting model and it is well described and well-motivated, and I think the paper will benefit from a more thorough theoretical analysis of the proposed model and its architecture, and a more detailed explanation of the key ideas and experiments. I think that the paper would benefit from more careful writing and better organization.",98
35,6299,hybrid,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines. The problem can also be understood as constructing a function f(G, nS , q) to predict nT , where the functional form of f(·) is generally unknown and has to be learned from a training dataset consisting of samples like (nS , q, nT ). Since nT is unknown, the problem cannot be solved by conventional search algorithms such as A⇤-search [11], which seeks to find paths between the given source and target nodes. For the KBC example in Figure 1(a), having access to the current node nt = Hawaii alone is not sufficient to know that the best action is moving to nt+1 = USA. Second, to address the challenge of sparse rewards, M-Walk exploits the fact that the MDP transition model is known and deterministic.2 Specifically, it combines Monte Carlo Tree Search (MCTS) with the RNN to generate trajectories that obtain significantly more positive rewards than using the RNN policy alone. The rest of the paper is organized as follows: Section 3 develops the M-Walk agent, including the model architecture, the training and testing algorithms.3 Experimental results are presented in Section 4. To see this, we observe from Figure 1(b) that once an action at (i.e., an edge in Ent or “STOP”) is selected, the next node nt+1 and its associated Ent+1 and Nnt+1 are known. The objective is to learn a policy that maximizes the terminal rewards, i.e., correctly identifies the target node with high probability. Recall from Section 2 (e.g., (1)) that one challenge in applying RL to the graph-walking problem is that the state st nominally includes the entire history of observations. We will explain in Section 3.2 how such parameter sharing enables indirect updates to the policy ⇡✓ via Q-learning from off-policy data. To address this issue, we apply the same FCN f✓(·) to 5There could be alternative ways to compute the score, such as Score(n) = maxsT!n Q✓(sT , STOP). Inspired by this recursion, we propose using the GRU-RNN [4] to encode qt into a vector representation6: qt+1 = f✓q (qt, [hA,t, hat,t, nt+1]) with initialization q0 = f✓q (q, [0, 0, nS ]), where ✓q is the model parameter, and hat,t denotes the vector hn0,t at n0 = at. Three Glass Puzzle The Three Glass Puzzle [20] is a problem studied in math puzzles and graph theory. WN18RR [6] is created from the original WN18 [2] by removing various sources of test leakage, making the dataset more challenging. We use HITS@1,3 and mean reciprocal rank (MRR) as the evaluation metrics for WN18RR, and use mean average precision (MAP) for NELL995,7 where HITS@K computes the percentage of the desired entities being ranked among the top-K list, and MRR computes an average of the reciprocal rank of the desired entities. We compare against RL-based methods [38, 5], embedding-based models (including DistMult [39], ComplEx [32] and ConvE [6]) and recent work in logical rules (NeuralLP) [40]. For all the baseline methods, we used the implementation released by the corresponding authors with their best-reported hyperparameter settings.8 The details of the hyperparameters for M-Walk are described in Appendix B.2.2 of the supplementary material. We ran the experiments three times and report the means and standard deviations (except for PRA, TransE, and TransR on NELL995, whose results are directly quoted from [38]). We observed that the novel neural architecture of M-Walk contributes an overall 1% gain relative to MINERVA on NELL995, and it is still 1% worse than M-Walk, which uses MCTS for training and testing. Third, we analyze the performance of M-Walk under different numbers of MCTS rollout simulations and different search horizons on WN18RR dataset, with results shown in Figure 5(a). We observe that M-Walk outperforms the strong baseline ConvE by 4.6–10.9% in samples that require 2 or 3 steps, while it is nearly on par for paths of length one. Therefore, M-Walk does better at reasoning over longer paths than ConvE. To examine this effect, we show in Figure 5(c)-top the HITS@K accuracies when the ground truth is in the candidate set.9 It shows that M-Walk has very high accuracy in this case, which is significantly higher than ConvE (80% vs 39.6% in HITS@1). These observations point to an important direction for improving M-Walk in future work: increasing the chance of covering the target by the candidate set. Finally, in Table 4, we show examples of reasoning paths found by M-Walk.10 Reinforcement Learning Recently, deep reinforcement learning has achieved great success in many artificial intelligence problems [17, 24, 25]. Note that the former is constructed from the visit counts of all the edges connected to the MCTS root node; it only uses information near the root node to improve the policy. Recent approaches have demonstrated limitations of these prior approaches: they suffer from cascading errors when dealing with compositional (multi-step) relationships [10]. Hence, recent works [8, 18, 10, 15, 30] have proposed approaches for injecting multi-step paths such as random walks through sequences of triples during training, further improving performance on KBC tasks. IRN [23] and Neural LP [40] explore multi-step relations by using an RNN controller with attention over an external memory. Empirically, our proposed algorithm outperforms both DeepPath and MINERVA in the KBC benchmarks.11 We developed an RL-agent (M-Walk) that learns to walk over a graph towards a desired target node for given input query and source nodes. Furthermore, we also performed extensive experimental analysis to understand M-Walk. 11A preliminary version of M-Walk with limited experiments was reported in the workshop paper [22].",soundness_1000_section_model_v2,M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines.","Summary of the paper:  - This paper proposes a walk-based method for knowledge graph completion (KGC).  - For triples (s, q, o), an agent is trained so that given s and q, it traverses a path from the initial node s to the   terminal node t in a knowledge graph.  - This problem is formulated as that of Q-learning using RNN to model policy and Q functions, but the issue is the   sparseness of rewards.  - Monte Carlo tree search is used to overcome this issue.  Pros:  - The paper is clearly written.  - The approach is natural and sound.  - Very good performance compared with existing KGC methods.  - Ablation study with PG-Walk and Q-Walk is nice.  Cons:  - The work is mostly empirical, without much theoretical justification (admittedly, as in many deep learning papers).  - Not much novelty or surprise, given the success of AlphaGo.  This is a type of paper in which a successful technique   in other fields is applied to a different field.  I could not see exactly what is the novelty/creativity in terms of   how MCTS is applied for this specific problem of KGC.  - No analysis on running time is provided.   General Comments:  This paper is well written (but I would like the content of Section 4.2 to be more organized --- Page 7 consists of a single paragraph now!).  The novelty of the proposal is somewhat questionable, because reinforcement learning has already been introduced for KGC, and the application of MCTS naturally comes to mind.  Although this work is mostly empirical and little theoretical contribution is provided, it still achieves the state-of-the-art performance, and that could be a merit for its publication.  Questions:  * Related work  MCTS has been introduced in walk-based KGC by Sheng et al. in ICLR 2018 Workshop (ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search), and this work must be cited.  Please elaborate on how M-Walk differs from ReinforceWalk as much as you can tell from the extended abstract.  * Experiments:  - Dataset:  Is there any reason the comparison is done with NELL995 and WN18RR?  FB15k is also the standard dataset.  - Baselines  Why do the set of compared methods differ between datasets?  In particular, we know that TransE and TransR are somewhat weaker compared with more recent bilinear models, such as ComplEx, ANALOGY, and ConvE.  - Evaluation metrics  Also, why is the MAP score used on NELL995 whereas HITS@/MRR are used on WN18RR?  I have nothing against MAP scores, but typically HITS@/MRR are reported in literature on KGC even on the NELL-995 dataset (e.g., the MINERVA paper); using different metrics makes it difficult to validate the experimental results.  - Figure 6(a)  Is there any explanation as to why accuracy drops as the number of roll outs are increased to 256?  * Application to classification task  The method is only concerned with prediction of knowledge triple in which only one entity is missing. Can the proposed method be applied to the triple classification task where the truth of a given triple must be predicted? This is more or less straightforward in score-function-based models (provided that a suitable threshold can be determined), but I cannot see how this can be addressed with walk-based methods in general.  * Running time  Training and inference both involves roll-out, which raises concern about the running time of the algorithms.  How does M-walk fare against scoring-based methods (e.g., ComplEx) or MINERVA in terms of running time?  Please provide the time required for training and inference.  I am curious about inference time in particular. "," the the is to, of and..- a in I not The are that this for paper it ) be : results ( on method with well",27
36,837,hybrid,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art. Since the introduction of the “AlexNet” architecture [14] in the 2012 ImageNet competition, ConvNets have dominated state-of-the-art performance across a variety of computer vision tasks, including object-detection, image segmentation, image classification, face recognition, human pose estimation and tracking. In conjunction with these advances as well as the evolution of network architectures, several design best practices have emerged [8, 21, 23, 24]. This functionality can be achieved by stacking many small filters or using large filters in the network; notably, the first choice can be implemented with fewer operations (faster, fewer parameters) and also allows inclusion of more nonlinearities. Third, dimensionality reduction (1×1 convolutions) before spatially aggregating filters (e.g.3×3) is supported by the fact that outputs of neighbouring filters are highly correlated and therefore these activations can be reduced before aggregation [23]. Fourth, spatial factorization into asymmetric filters can even further reduce computational cost and 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. Fifth, it is important to normalize the responses of each feature channel within a batch to reduce internal covariate shift [11]. To demonstrate the benefits of our proposed spatiotemporal ResNet architecture, it has been evaluated on two standard action recognition benchmarks where it greatly boosts the state-of-the-art. Yet another study compared several approaches to extending ConvNets into the temporal domain, but with rather disappointing results [13]: The architectures were not particularly sensitive to temporal modelling, with a slow fusion model performing slightly better than early and late fusion alternatives; moreover, similar levels of performance were achieved by a purely spatial network. Another research branch has investigated combining image information in network architectures across longer time periods. That approach takes advantage of the local spatial similarity in images; however, it only yields a minor increase over their baseline, which is a two-stream VGG-16 ConvNet [21] used as the input to their convolutional RNN. Batch normalization [11] and ReLU [14] are applied after each convolution; the network does not use hidden fc, dropout, or max-pooling (except immediately after the first layer). In preliminary experiments we found that direct connections between identical layers of the two streams led to an increase in validation error. We conjecture that these results are due to the large change that the signal of one network stream undergoes after injecting a fusion signal from the other stream. Therefore, we developed a more subtle alternative solution based on additive interactions, as follows. To enable learning of spatiotemporal features at all possible scales, this modification is applied before the second residual unit at each spatial resolution of the network (indicated by “skip-stream” in Table 1), as exemplified by the connection at the conv5_x layers in Fig. Based on the above observations, we developed a novel approach to temporal residual connections that builds on the ConvNet design guidelines of chaining small [21] asymmetric [10, 23] filters, noted in Sec. This allows the straightforward use of standard two-stream ConvNets that have been pre-trained on large-scale datasets e.g.to leverage the massive amounts of training data from the ImageNet challenge. From left to right, top to bottom, the first row shows the convolutional and pooling building blocks, with the filter and pooling size shown as (W ×H × T,C), denoting width, height, temporal extent and number of feature channels, resp. In the last two rows we show the output size of these metalayers as well as the receptive field on which they operate. We also apply random cropping and scale augmentations: We randomly jitter the width and height of the 224× 224 input frame by ±25% and also randomly crop it from a maximum of 25% distance from the image borders. We found that this strategy significantly reduces the training duration with the drawback that each loss does not capture all available information. For our final model, we equip the spatiotemporal ResNet with a temporal max-pooling layer after pool5 (see Table 1, temporal average pooling led to inferior results) and continue training as above with the learning rate starting from 10−4 for 2K iterations followed by 10−5. For fair comparison, we follow the evaluation procedure of the original two-stream work [20] by sampling 25 frames (and their horizontal flips). However, rather than using 10 spatial 224× 224 crops from each of the frames, we apply fully convolutional testing both spatially (smallest side rescaled to 256) and temporally (the 25 frame-chunks) by classifying the video in a single forward pass, which takes ≈250ms on a Titan X GPU. Second, we consider HMDB51 [15], which has 6766 videos that show 51 different actions and generally is considered more challenging than UCF0101 due to the even wider variations in which actions occur. For both datasets, we use the provided evaluation protocol and report mean average accuracy over three splits into training and test sets. Interestingly, research in neuroscience also suggests that the human visual cortex is equipped with connections between the dorsal and the ventral stream to distribute motion information to separate visual areas [3, 27]. We achieve this by simply averaging the L2-normalized SVM scores of the FV-encoded IDT descriptors (i.e.HOG, HOF, MBH) [29] with the L2-normalized video predictions of our ST-ResNet*, again without softmax normalization. These relatively larger performance decrements again underline that our approach is better able to capture the available dynamic information, as there is less to be gained by augmenting it with IDT. Still, there is a benefit from the hand-crafted IDT features even with our approach, which could be attributed to its explicit compensation of camera motion.",soundness_1000_section_model_v2,Spatiotemporal Residual Networks for Video Action Recognition,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art.","The paper presents a novel architecture that 1) combines residual networks with two-stream convolutional networks, and 2) injects connections from the motion stream to the appearance stream, to be able to capture spatio-temporal features. The paper shows experiments in both of the main action recognition datasets, achieving state-of-the-art accuracy in both. Overall I think the paper is great: good idea, careful experimentation, great results and clearly written.   Although the basic components of the architecture are pre-existing, I think that the high performance and careful experimentation and description make it a very useful contribution.   I only miss some experiments to visualize what kinds of spatio-temporal features are being learned.   Since large temporal windows are important (278-280) I would add a relevant reference: Long-term Temporal Convolutions for Action Recognition, Gul Varol, Ivan Laptev, Cordelia Schmid  Small typos:  (L 187)  ""resp.""  (L 255) ""UCF0101""  "," the the, is to of. and.- a in I not The are this that for paper it ) : be results on ( with method well",27
37,22112,hybrid,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values. While a large body of the machine learning literature is built upon the assumption of having access to complete datasets, in many real-world problems only incomplete datasets are available. Single imputation methods try to replace each missing value with a plausible value that is the best fit given the value of other correlated features and knowledge extracted from the dataset (Hastie et al., 1999; Anderson, 1957). While MI offers interesting statistical insights about the reliability of analysis on incomplete data, the insight is imprecise as it is mainly concerned about the population of data samples rather than individual instances. While these methods are easy to implement and analyze, they often fail to capture the complex feature dependencies as well as structures present in many problems. For instance, Mattei & Frellsen (2018) suggested a method based on deep latent variable models and importance sampling that offers a tighter likelihood bound compared to the standard VAE bound. Also, these methods are often complicated to be applied in practical setups by practitioners. A few exceptions exist such as Bayesian models and decision trees that permit direct analysis on incomplete data (Nielsen & Jensen, 2009; Zhang et al., 2005). However, given an incomplete training dataset and without any known causal structure as a priori, learning such models is a very challenging problem with the complexity of at least NP-complete to learn the network architecture in addition to an iterative EM optimization to learn model parameters (Darwiche, 2009; Neapolitan et al., 2004). Similarly, for the second part, we are interested in obtaining a distribution over the possible target assignments and the confidence of each class rather than maximum likelihood class assignments. Instead, we use recent advances in GAN stabilization and training to improve the training process (see Section 3.4). Additionally, in our experiments, we provide supporting evidence that this simple loss function enables us to sample from the conditional distribution and prevents biased inclinations toward distribution modes. For instance, for a specific test sample at hand, if a certain missing feature is a strong indicator of the target class, we would like to observe the impact of different imputations for that feature on the final hypothesis. Input: G (trained imputer), D (dataset) Output: Fθ (trained predictor) foreach Training Epoch do foreach (xi, ki, yi) in D do z ∼ N(0, I) x̂i ← ki xi+(1−ki) G(xi,ki, z) ypredi ← Fθ(x̂i) loss← L(yi, ypredi ) Backpropagate loss Update Fθ Algorithm 2: Estimating target distributions. The first assumption is consistent with the theoretical analysis of generative adversarial networks that they can converge to the true underlying distribution (Arora et al., 2018; Liu et al., 2017). It should be noted that, while Zhang et al.(2018) suggests using a single self-attention layer in the middle of the network, we observed consistent improvements by inserting multiple self-attention layers before each residual block within the network. We explored best TTUR learning-rate settings from the set of {0.001, 0.0005, 0.0001, 0.00005}. To evaluate the proposed method we use CIFAR-10 (Krizhevsky & Hinton, 2009) as an image classification dataset as well as three non-image datasets: UCI Landsat (Dua & Graff, 2017)2, MIT-BIH arrhythmia (Moody & Mark, 2001), and Diabetes classification (Kachuee et al., 2019) 3. Also, while different encoding and representation methods are suggested in the literature to handle categorical features (Jang et al., 2016; Nazabal et al., 2018), in this paper, we take the simple approach of encoding categorical variables using one-hot representation and smoothing them by adding Gaussian noise with zero mean and variance equal to 5% of feature variances. We would like to note that while the suggested solution in this paper is readily compatible with MAR structures, in our experiments, to simplify the presentation of results and to have a fair comparison with other work that does not support the MAR assumption, we limited the scope of our experiments to MCAR. Furthermore, to simulate incomplete datasets and to make sure the same features are missing without explicitly storing masks, we use hashed feature vectors to seed random number generators used to sample missing features. More detail is provided in Appendix C. Fréchet inception distance (FID) (Heusel et al., 2017) score is used to measure the quality of missing data imputation in experiments with images5. We also considered using root means squared error (RMSE); however, we decided not to use this measure as we observed an inconsistent behavior using RMSE in our comparisons as RMSE favors methods that show less variance rather than realistic and sharp samples from the distribution. We compare our results with MisGAN (Li et al., 2019) and GAIN (Yoon et al., 2018) as the state of the art imputation algorithms based on GANs as well as basic denoising autoencoder (DAE) (Vincent et al., 2008) and multiple imputation by chained equations (MICE) (Buuren & Groothuis-Oudshoorn, 2010) as baselines. Due to scalability issues, we were only able to use MICE for the smaller non-image datasets. One possible explanation for this behavior might be the fact that GAIN has an MSE loss term acting similar to an autoencoder loss smoothing noisy missing pixels. Regarding the MIT-BIH experiemts, GI outperforms other work for missing rates more than 30% while achieving similar accuracies to GAIN for lower missing rates. As it can be seen from the plots, GI provides results closest to the ideal case of having average confidence values equal to average accuracies. In order to provide further insight into the operation of GI and how imputations can potentially influence the outcomes of predictions, we conduct experiments on a synthesized dataset. One possible explanation could be the fact that imputing missing data with a uniform structure can be done by processing local regions and does not require attending to different distant regions across the image. From Table 6 it can be inferred that as the rate of missingness increases, the benefits of the suggested predictor algorithm increase significantly.",soundness_1000_section_model_v2,Generative Imputation and Stochastic Prediction,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values.","This paper proposes a method to impute missing features using a generative model and train a predictive model on top of imputed dataset to improve classification results. They first train a GAN model where the generator outputs an imputed representation of the input and discriminator is trained to predict if an individual features (such as a pixel) is imputed or not. Given the generator and incomplete sample, they train a predictor using the output of the generator, imputed sample, as input. Their main contribution is using a MC averaging to compute the prediction by repetitively sampling from the noise variable, z, and generating different imputations from generator. They show that the proposed model improves upon the previous SOTA on final classification performance.

Overall the paper is clearly written. But I do feel it is a bit incremental over the GAIN approach. The overall GAN architecture is very similar to GAIN's and although stochastic prediction shows clear improvements it is a bit straightforward. However, I think the uncertainty of the imputations and its effect on the final prediction is interesting. I suggest the authors to extend this part with more detailed analysis.

There are several parts that are confusing/missing in the paper:

- In GAIN, they use a hint vector as an input to the discriminator. They show that without the hint vector, there is no unique solution (this is shown without the MSE loss). The authors do not use this vector in their approach (as in Figure 1) and it is not clear to me if it causes any instabilities or if multiple experiments yield similar results or if the stochastic prediction benefits from this.
- On what type of examples GI is more accurate than other models? Since stochastic prediction is the main difference from GAIN, is this related to the multi-modality of the noisy examples?
- Can you explain the difference between the results in Figure-7 and Table-2? Results between the two mismatch.
- I think the statement in the first paragraph in Section 4.4 that ""MSE loss term would act as a denoising loss smoothing noisy missing pixels"" could be misleading. MSE is used with mask in GAIN, hence it only applies to the observed features during training. Its effect on smoothing noisy missing pixels is not clear.


I think the paper would benefit if the authors could explain/show:
- Increasing the missing rate would also increase the possibility that the ground truth be a more multi-modal distribution. Especially in rectangular generation part where it can remove a complete object. Does stochastic averaging benefit more in this case?"," the the is, to of and.. a- I in not The are that this for paper it ) be : results ( on method with well",27
38,18864,hybrid,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting. Recent works have used this framework to achieve impressive feats such as learning to classify using one labeled image per class (Snell et al., 2017; Finn et al., 2017), learning unsupervised update rules that generalize to different domains (Metz et al., 2018), and accelerating training procedures that are millions of steps long (Flennerhag et al., 2018). DIMCO generalizes well to novel datasets because its learning objective encourages the model to compactly use all of its degrees of freedom, thus enabling it to effectively compare datapoints while only requiring a small number of bits per datapoint. Derive a generalization bound for meta-learning that makes the tradeoff between taskspecific and task-wise overfitting concrete. Notably, we suggest that certain previous meta-learning methods may have benefitted from implicit regularization. We present experiments in Section 5 and conclude the paper with a discussion about limitations and future directions of our approach in Section 6. This can be problematic since in many works, the task-specific learning algorithm is usually just a byproduct of whatever clever meta-learning loss was proposed. We propose a factorized discrete representation scheme which enables us to represent discrete distributions with exponentially fewer parameters compared to listing the probability of each event. We similarly perform image retrival by mapping each support image to its most likely code (10) and for each query image retrieving the support image that has highest (11). Regularizing Meta-Learners The ability to generalize to novel datasets is critical in meta-learning benchmarks, and even more so in benchmarks such as Meta-Dataset (Triantafillou et al., 2019), where a model is tested on datasets from an unseen domain. The following works have reported benefits from reducing the number of such task-specific parameters: Lee & Choi (2018) learns a subset of the full network to alter during task-specific learning, Rusu et al.(2018) explicitly represents each task with a low-dimensional latent space, and Zintgraf et al.(2018) alters only a pre-specified subset of the full network during task-specific learning. We showed through Theorem 1 that restricting inner-loop expressivity reduces the generalization gap; this provides theoretical understanding to this consensus that meta-learning models with simple task-specific learners generalize to new tasks more easily. Also related is the deterministic information bottleneck (Strouse & Schwab, 2017) which extends the information bottleneck by minimizing H(X̃) rather than I(X̃;X). These three approaches to generalization are related via the chain of inequalities I(X̃;X) ≤ H(X̃) ≤ log |X̃|, which is tight when X̃ is an efficient code. Recent deep learning methods directly learn discrete representations, by learning variational autoencoders with discrete latent variables (Rolfe, 2016; van den Oord et al., 2017; Razavi et al., 2019) or maximizing the mutual information between representation and 1 In practice, we add log probabilities instead of multiplying probabilities for numerical stability. Additionally, their method solves a minimum cost flow problem within each batch to find the locally optimal code, whereas DIMCO is able to directly compute its loss function. We randomly initialize weights for the 4-layer convnet and use pretrained weights for the Inception network This experiment empirically verifies whether mutual information I(X̃;Y ) is a reasonable metric for quality of representation. We observed similar trends when training with with previously proposed loss functions: we visualize these results in Figure 5 of the appendix due to space constraints. Results in Figure 3 show that the compact code of DIMCO takes roughly an order of magnitude less memory for similar performance to N-pair loss, and requires less query time as well. We additionally note that DIMCO is able to train using large backbones without significantly overfitting, whereas experiments reported in Mishra et al.(2017) indicate that MAML (Finn et al., 2017) overfits tremendously when using a deeper backbone. This challenging experimental setup measures how much generalizable information the model can extract from a limited set of datasets; it can be seen as the meta-learning analogue of measuring the performance of a classifier trained with a small dataset. We report the average and standard deviation of the top 5 results of a random hyperparameter search (see appendix for details). Towards Explicit Meta-Regularization In Section 4, we have suggested with analogy to Theorem 1 that the benefits of some previous meta-learning methods can be attributed to implicitly being regularized by reducing the expressivity of their task-specific learners. In future work, we would like to explore explicit meta-regularization schemes that can be applied to other problems (regression, reinforcement learning etc.) Along with showing that the traditional support/query split is not strictly necessary, we demonstrated in Section 5.4 that removing it has the benefit of enabling meta-learning in datasets having one image per class. (12) Assuming that the approximate distribution q(·) is sufficiently close to p(y|x̃), minimizing (12) can be seen as arg min xent(Y, X̃) ≈ arg minEy∼Y,x̃∼X̃ [ − log p(y|x̃) ] (13) = arg minH(Y |X̃) = arg max I(X̃;Y ), (14) where the last equality uses the fact that H(Y ) is independent of model parameters. (24) We similarly bound the error caused by estimating L with a finite number of tasks sampled from τ . Combining equations (26, 24), we have with high probability∣∣∣∣L(τ, θ)− − 1 n n∑ i=1 Î(X̃(XT i , θ);YT i) ∣∣∣∣ (27) ≤ ∣∣∣L(τ, θ)− L̂(τ, θ)∣∣∣+O( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (28) ≤O (√ dΘ n log n dΘ ) +O ( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (29) Hardware Every experiment was conducted on a single Nvidia V100 GPU with CUDA 9.2. Each experiment was performed with different fixed initial seeds; we manually fix seeds with manual_seed() for python, pytorch, and numpy. We additionally show in Figure 5 the correlation between 1-shot accuracies, Recall@1, and NMI using three previously proposed losses (triplet, npair, protonet).",soundness_1000_section_model_v2,Discrete InfoMax Codes for Meta-Learning,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting.","This paper presents DIMCO, a meta-learner that is trained by maximizing mutual information between a discrete data representation and class labels across tasks. DIMCO is inspired by an information theoretic lower bound on the generalisation gap for meta-learning, which the authors argue identifies overfitting in the task learner as the bottleneck. 

This work proposes to constrain a learner to output discrete codes that are learned to capture the mutual information with class labels. While the idea of using discrete codes is interesting, its presentation in the manuscript is not well motivated and at times hard to follow. This makes it challenging to evaluate the novelty, validity, and generality of the proposed approach. Meanwhile, the empirical evaluation is somewhat lacking. Thus, I do not believe this work is ready for publication in its current form.

Detailed comments

My main concern is with respect to the primary contribution of this paper, a generalisation bound on meta-learning. The bound appears to be on a multi-task loss without task adaptation, and thus the claims made with respect to the theorem seem somewhat over-reaching. I also believe the VC-dimensionality of the encoder is missing in Eq. 4? If so, this changes the interpretation since the length of the code and the expressivity of the encoder are interrelated. Further, I would welcome a deeper analysis of the theorem and its implications. The current interpretation states that the number of tasks is independent of the size of each task, hence given many tasks, using minimal representations is an effective approach to meta-generalisation. Yet minimal representations is a well-known idea and has features in several works that use mutual information as a regularizer, most notably works on the Information Bottleneck. 

Another reservation I have is the use of mutual information between encoder representations and class labels as a loss function (Eq. 1). It lacks context and a proper motivation, especially since the analysis of [1] shows that the loss function in Eq. 1 is the cross-entropy objective. The authors make a similar analysis in Appendix A and argue that Eq. 1 differs in that cross-entropy is an approximation because it adds a parametrized linear layer on top of \tilde{X}. Thus, in the absence of that layer they collapse to the same objective. As DIMCO itself directly extract class label predictions from \tilde{X}, I fail to see a difference between the loss in Eq. 1 and a cross-entropy objective. 

The main motivation behind their loss objective is that it does not require a support / query set. This does not seem to be a feature of the mutual information objective itself, but rather a choice made by the authors. I would have liked a deeper discussion of this seeing as the authors make it a central tenet of DIMCO. Prior works use a support set as a principled means of doing meta-learning: meta-training explicitly takes into account that at test time, the learner will be given a small support set from which to learn how to query points. As far as I understand, DIMCO does not take this into account during meta-training.  At meta-test time however, DIMCO does use a support set to map query points (Eqs. 10 and 11). Why should we break protocols between meta training and testing? Are there any downsides to doing so?  

Empirically, I find the CUB experiment compelling but would welcome some ablations. What are the trade-offs between p and d? Can DIMCO outperform N-pair when number of bits are unconstrained?

miniImagenet is a standard benchmark in few-shot learning, but I am unable to find a results table - could the authors please provide results on the standard setup so that the method can be compared against known baselines? Further, would the results currently presented hold in a N-way-5-shot setup?

As for the constrained version of miniImagenet that the authors propose, I am not convinced this is an interesting protocol. In general, the miniImagenet task distribution is created by N-way permutations of the classes in the meta-set (e.g. meta-training tasks are combinations of the 64 classes in the meta-training set). By keeping the number of classes constant but reducing the number of images per class, this protocol is not reducing samples per task: a task is always defined as 5/20-way-1-shot (Fig. 4). Instead, the effect should be that tasks are in (greater) violation of the task i.i.d. assumption. Thus, I question whether this setup demonstrates the trade-offs the authors present in Theorem 1 and whether the results can be interpreted in light of it. 

Finally, that both experiments are image-based raises questions as to the generality of the method. The paper could be considerably strengthened by evaluating DIMCO on a non-image task, or if not discuss the method’s limitations.  

The idea of discrete codes for few-shot classification is interesting and sufficiently novel, I am likely to increase my score if my concerns are addressed and the experimental section is strengthened.

Further questions and comments:

- I am unable to parse Eq. 11 - what does the notation \prod_i p_{\tilde{x}_i, i} mean? 
- It is unnecessarily hard to follow the proof of theorem 1. It would help the reader if you restated relevant definitions, such as Eq. 1, since the difference with Eq. 23 is very subtle. It would also be helpful to explain how the summand in Eq. 25 differs from either, and Eq. 26 could be expanded or briefly explained after the derivation. 
-  Because DIMCO is trained with backpropagation, the interpretation of Eq. 5 as d independent events seems invalid. How does it affect the method if they are not independent?
- Overloading X and Y as both random variables and mini-batch samples creates unnecessary confusion. I believe the objective in Eq. 1 is approximated, not calculated exactly? For instance, the mutual information in Eq. 8 is with respect to a mini-batch, so should it not be \hat{I}? 
- p^j_{ik} in Eq. 9 is undefined.
- Eq. 9 is interpreted as an exact entropy, however it appears to be a mini-batch approximation to the true entropy? 

References 
[1] Achille and Soatto. Emergence of Invariance and Disentanglement in Deep Representations. JMLR. 2018."," the the is the is, to of. and.- a in I not The are this that for paper it ) be : results on ( method",27
39,13063,hybrid,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all. But in each case, the network was designed and tuned specifically for the problem at hand. Natural language processing models have been shown to benefit from a multi-task approach a long time ago (Collobert & Weston, 2008), and recently multi-task machine translation models (MinhThang Luong, 2015) have even been shown to exhibit zero-shot learning when trained on multiple languages (Melvin Johnson, 2016). Speech recognition has also been shown to benefit from multi-task training (Seltzer & Droppo, 2013), as have some vision problems, such as facial landmark detection (Zhang Z., 2014). But no competitive multi-task multi-modal model has been proposed, so the above question remains unanswered. (3) COCO image captioning dataset (Lin et al., 2014), used for image captioning. (4) WSJ parsing dataset (Marcus et al., 1999), used for constituency parsing. While the MultiModel is only a first step and will be improved in the future, two key insights are crucial to making it work at all and are our main contributions. We design modality nets to be computationally minimal, promoting heavy feature extraction and ensuring that the majority of computation is performed within the domain-agnostic body of the model. Two design decisions were important: • The unified representation is variable-size. While a fixed-size representation is tempting and easier to implement, it creates a bottleneck and limits the performance of the model. For example, separable convolutions were introduced in the Xception architecture (Chollet, 2016) and were not applied to text or speech processing before. We find that each of these mechanisms is indeed crucial for the domain it was introduced, e.g., attention is far more important for languagerelated tasks than for image-related ones. As already said above, the encoder and decoder are constructed using 3 key computational blocks to get good performance across different problems: (1) Convolutions allow the model to detect local patterns and generalize across space. We refer the reader to (Chollet, 2016) for a complete definition; here we will denote a depthwise separable convolution with weights Wh×w corresponding to f kernels of size h × w applied to an input tensor x with stride s and dilated by a factor d (see (Yu & Koltun, 2015)) as SepConvd,s,f (W,x). The source tensor is finally passed through two different pointwise convolutions to generate the memory keys K and values V and the query keys, memory keys and memory values are used to apply the attention mechanism between the self-attended target and the source (see Figure 3). On the output side, the language modality takes the decoded output of the body and performs a learned linear mapping, followed by a Softmax, resulting in a probability distribution over the token vocabulary. The spectral modality does not perform any striding along the frequency bin dimension, preserving full resolution in the spectral domain. Convolutional architectures yielded good results on word-level neural machine translation starting from (Kalchbrenner & Blunsom, 2013) and later in (Meng et al., 2015). These early models used a standard RNN on top of the convolution to generate the output and had a bottleneck there that hurt performance, especially on longer sentences, similarly to the limitations of RNN sequence-to-sequence models without attention (Sutskever et al., 2014; Cho et al., 2014). This idea, introduced in WaveNet (van den Oord et al., 2016) and also used in MultiModel (see above) significantly improves efficiency. Depthwise separable convolutions were first studied by Sifre (Sifre & Mallat, 2013) and later they were used to get good results on large-scale image classification with Xception (Chollet, 2016). We focused our experiments so as to answer the following questions: (1) How far is the MultiModel trained on 8 tasks simultaneously from state-of-the-art results? Especially the 4 translation problems behave very similarly, so we decided to not include them all in each comparison but we focused on the more varied problems instead. The results we achieve are similar to the ones task-specific models get without heavy tuning, e.g., on English-French translation we improve on the recent Extended Neural GPU results (Kaiser & Bengio, 2016). Since we are comparing different instantiations of the same model, we report two internal metrics: the negative log-perplexity and per-token accuracy (measured on the development set). As can be seen from the results in Table 2, the joint 8-problem model performs similarly to single-model on large tasks, and better, sometimes significantly, on tasks where less data is available, such as parsing. The difference in performance is significant, and since we use both dropout and early stopping, we conjecture that it is not related to over-fitting. In fact, one could expect that removing these blocks will improve performance on ImageNet alone if they were truly useless for this task. In contrast, we see in Table 4 that these blocks either don’t affect or slightly improve performance. This leads us to conclude that mixing different computation blocks is in fact a good way to improve performance on many various tasks. We believe that this treads a path towards interesting future work on more general deep learning architectures, especially since our model shows transfer learning from tasks with a large amount of available data to ones where the data is limited.",soundness_1000_section_model_v2,Large Scale Multi-Domain Multi-Task Learning with MultiModel,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.","The paper presents a multi-task, multi-domain model based on deep neural networks. The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition. The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic. The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task.

The paper is well written and easy to follow.

The contributions of the paper are novel and significant. The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring. The experiments clearly show the viability of the approach and give interesting insights. This is surely an important step towards more general deep learning models. 

Comments:

* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database. Moreover, some databases could be used for different tasks, such as WSJ or ImageNet.

* The training procedure of the model is not explained in the paper. What is the cost function and what is the strategy to train on multiple tasks ? The paper should at least outline the strategy.

* The experiments are sufficient to demonstrate the viability of the approach, but the experimental setup is not clear. Specifically, there is an issue about the speech recognition part of the experiment. It is not clear what the task exactly is: continuous speech recognition, isolated word recognition ? The metrics used in Table 1 are also not clear, they should be explained in the text. Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used. Information about the detailed setup is also lacking, specifically which test and development sets are used (the WSJ corpus has several sets).

* Using raw waveforms as audio modality is very interesting, but this approach is not standard for speech recognition, some references should be provided, such as:
P. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26–30.
D. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE.
T. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals. Learning the Speech Front-end With Raw Waveform CLDNNs. Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015.

Revised Review:
The main idea of the paper is very interesting and the work presented is impressive. However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks. The experiments are interesting, except for the WSJ speech task, which is almost meaningless. Indeed, it is not clear what the network has learned given the metrics presented, as the WER on WSJ should be around 5% for speech recognition.
I thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition. A simpler speech task such as Keyword Spotting could also be investigated.
"," the the is to, of. and.- a in I not The are that this for paper it ) be : results on ( method with well",27
40,6299,hybrid,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines. The problem can also be understood as constructing a function f(G, nS , q) to predict nT , where the functional form of f(·) is generally unknown and has to be learned from a training dataset consisting of samples like (nS , q, nT ). Since nT is unknown, the problem cannot be solved by conventional search algorithms such as A⇤-search [11], which seeks to find paths between the given source and target nodes. For the KBC example in Figure 1(a), having access to the current node nt = Hawaii alone is not sufficient to know that the best action is moving to nt+1 = USA. Second, to address the challenge of sparse rewards, M-Walk exploits the fact that the MDP transition model is known and deterministic.2 Specifically, it combines Monte Carlo Tree Search (MCTS) with the RNN to generate trajectories that obtain significantly more positive rewards than using the RNN policy alone. The rest of the paper is organized as follows: Section 3 develops the M-Walk agent, including the model architecture, the training and testing algorithms.3 Experimental results are presented in Section 4. To see this, we observe from Figure 1(b) that once an action at (i.e., an edge in Ent or “STOP”) is selected, the next node nt+1 and its associated Ent+1 and Nnt+1 are known. The objective is to learn a policy that maximizes the terminal rewards, i.e., correctly identifies the target node with high probability. Recall from Section 2 (e.g., (1)) that one challenge in applying RL to the graph-walking problem is that the state st nominally includes the entire history of observations. We will explain in Section 3.2 how such parameter sharing enables indirect updates to the policy ⇡✓ via Q-learning from off-policy data. To address this issue, we apply the same FCN f✓(·) to 5There could be alternative ways to compute the score, such as Score(n) = maxsT!n Q✓(sT , STOP). Inspired by this recursion, we propose using the GRU-RNN [4] to encode qt into a vector representation6: qt+1 = f✓q (qt, [hA,t, hat,t, nt+1]) with initialization q0 = f✓q (q, [0, 0, nS ]), where ✓q is the model parameter, and hat,t denotes the vector hn0,t at n0 = at. Three Glass Puzzle The Three Glass Puzzle [20] is a problem studied in math puzzles and graph theory. WN18RR [6] is created from the original WN18 [2] by removing various sources of test leakage, making the dataset more challenging. We use HITS@1,3 and mean reciprocal rank (MRR) as the evaluation metrics for WN18RR, and use mean average precision (MAP) for NELL995,7 where HITS@K computes the percentage of the desired entities being ranked among the top-K list, and MRR computes an average of the reciprocal rank of the desired entities. We compare against RL-based methods [38, 5], embedding-based models (including DistMult [39], ComplEx [32] and ConvE [6]) and recent work in logical rules (NeuralLP) [40]. For all the baseline methods, we used the implementation released by the corresponding authors with their best-reported hyperparameter settings.8 The details of the hyperparameters for M-Walk are described in Appendix B.2.2 of the supplementary material. We ran the experiments three times and report the means and standard deviations (except for PRA, TransE, and TransR on NELL995, whose results are directly quoted from [38]). We observed that the novel neural architecture of M-Walk contributes an overall 1% gain relative to MINERVA on NELL995, and it is still 1% worse than M-Walk, which uses MCTS for training and testing. Third, we analyze the performance of M-Walk under different numbers of MCTS rollout simulations and different search horizons on WN18RR dataset, with results shown in Figure 5(a). We observe that M-Walk outperforms the strong baseline ConvE by 4.6–10.9% in samples that require 2 or 3 steps, while it is nearly on par for paths of length one. Therefore, M-Walk does better at reasoning over longer paths than ConvE. To examine this effect, we show in Figure 5(c)-top the HITS@K accuracies when the ground truth is in the candidate set.9 It shows that M-Walk has very high accuracy in this case, which is significantly higher than ConvE (80% vs 39.6% in HITS@1). These observations point to an important direction for improving M-Walk in future work: increasing the chance of covering the target by the candidate set. Finally, in Table 4, we show examples of reasoning paths found by M-Walk.10 Reinforcement Learning Recently, deep reinforcement learning has achieved great success in many artificial intelligence problems [17, 24, 25]. Note that the former is constructed from the visit counts of all the edges connected to the MCTS root node; it only uses information near the root node to improve the policy. Recent approaches have demonstrated limitations of these prior approaches: they suffer from cascading errors when dealing with compositional (multi-step) relationships [10]. Hence, recent works [8, 18, 10, 15, 30] have proposed approaches for injecting multi-step paths such as random walks through sequences of triples during training, further improving performance on KBC tasks. IRN [23] and Neural LP [40] explore multi-step relations by using an RNN controller with attention over an external memory. Empirically, our proposed algorithm outperforms both DeepPath and MINERVA in the KBC benchmarks.11 We developed an RL-agent (M-Walk) that learns to walk over a graph towards a desired target node for given input query and source nodes. Furthermore, we also performed extensive experimental analysis to understand M-Walk. 11A preliminary version of M-Walk with limited experiments was reported in the workshop paper [22].",substance_1000_section_model_v2,M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines.","Summary of the paper:  - This paper proposes a walk-based method for knowledge graph completion (KGC).  - For triples (s, q, o), an agent is trained so that given s and q, it traverses a path from the initial node s to the   terminal node t in a knowledge graph.  - This problem is formulated as that of Q-learning using RNN to model policy and Q functions, but the issue is the   sparseness of rewards.  - Monte Carlo tree search is used to overcome this issue.  Pros:  - The paper is clearly written.  - The approach is natural and sound.  - Very good performance compared with existing KGC methods.  - Ablation study with PG-Walk and Q-Walk is nice.  Cons:  - The work is mostly empirical, without much theoretical justification (admittedly, as in many deep learning papers).  - Not much novelty or surprise, given the success of AlphaGo.  This is a type of paper in which a successful technique   in other fields is applied to a different field.  I could not see exactly what is the novelty/creativity in terms of   how MCTS is applied for this specific problem of KGC.  - No analysis on running time is provided.   General Comments:  This paper is well written (but I would like the content of Section 4.2 to be more organized --- Page 7 consists of a single paragraph now!).  The novelty of the proposal is somewhat questionable, because reinforcement learning has already been introduced for KGC, and the application of MCTS naturally comes to mind.  Although this work is mostly empirical and little theoretical contribution is provided, it still achieves the state-of-the-art performance, and that could be a merit for its publication.  Questions:  * Related work  MCTS has been introduced in walk-based KGC by Sheng et al. in ICLR 2018 Workshop (ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search), and this work must be cited.  Please elaborate on how M-Walk differs from ReinforceWalk as much as you can tell from the extended abstract.  * Experiments:  - Dataset:  Is there any reason the comparison is done with NELL995 and WN18RR?  FB15k is also the standard dataset.  - Baselines  Why do the set of compared methods differ between datasets?  In particular, we know that TransE and TransR are somewhat weaker compared with more recent bilinear models, such as ComplEx, ANALOGY, and ConvE.  - Evaluation metrics  Also, why is the MAP score used on NELL995 whereas HITS@/MRR are used on WN18RR?  I have nothing against MAP scores, but typically HITS@/MRR are reported in literature on KGC even on the NELL-995 dataset (e.g., the MINERVA paper); using different metrics makes it difficult to validate the experimental results.  - Figure 6(a)  Is there any explanation as to why accuracy drops as the number of roll outs are increased to 256?  * Application to classification task  The method is only concerned with prediction of knowledge triple in which only one entity is missing. Can the proposed method be applied to the triple classification task where the truth of a given triple must be predicted? This is more or less straightforward in score-function-based models (provided that a suitable threshold can be determined), but I cannot see how this can be addressed with walk-based methods in general.  * Running time  Training and inference both involves roll-out, which raises concern about the running time of the algorithms.  How does M-walk fare against scoring-based methods (e.g., ComplEx) or MINERVA in terms of running time?  Please provide the time required for training and inference.  I am curious about inference time in particular. "," The paper is well written and the authors have also shown that the paper can be improved with the help of a more thorough analysis of the proposed method.The paper also shows that the proposed approach would be better if the paper could be used to show that it is not as well written as it seems, and that it would be nice to see that the method can be used in the future.The proposed method would be interesting to see if it is possible to use this method in a more general way.",95
41,837,hybrid,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art. Since the introduction of the “AlexNet” architecture [14] in the 2012 ImageNet competition, ConvNets have dominated state-of-the-art performance across a variety of computer vision tasks, including object-detection, image segmentation, image classification, face recognition, human pose estimation and tracking. In conjunction with these advances as well as the evolution of network architectures, several design best practices have emerged [8, 21, 23, 24]. This functionality can be achieved by stacking many small filters or using large filters in the network; notably, the first choice can be implemented with fewer operations (faster, fewer parameters) and also allows inclusion of more nonlinearities. Third, dimensionality reduction (1×1 convolutions) before spatially aggregating filters (e.g.3×3) is supported by the fact that outputs of neighbouring filters are highly correlated and therefore these activations can be reduced before aggregation [23]. Fourth, spatial factorization into asymmetric filters can even further reduce computational cost and 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. Fifth, it is important to normalize the responses of each feature channel within a batch to reduce internal covariate shift [11]. To demonstrate the benefits of our proposed spatiotemporal ResNet architecture, it has been evaluated on two standard action recognition benchmarks where it greatly boosts the state-of-the-art. Yet another study compared several approaches to extending ConvNets into the temporal domain, but with rather disappointing results [13]: The architectures were not particularly sensitive to temporal modelling, with a slow fusion model performing slightly better than early and late fusion alternatives; moreover, similar levels of performance were achieved by a purely spatial network. Another research branch has investigated combining image information in network architectures across longer time periods. That approach takes advantage of the local spatial similarity in images; however, it only yields a minor increase over their baseline, which is a two-stream VGG-16 ConvNet [21] used as the input to their convolutional RNN. Batch normalization [11] and ReLU [14] are applied after each convolution; the network does not use hidden fc, dropout, or max-pooling (except immediately after the first layer). In preliminary experiments we found that direct connections between identical layers of the two streams led to an increase in validation error. We conjecture that these results are due to the large change that the signal of one network stream undergoes after injecting a fusion signal from the other stream. Therefore, we developed a more subtle alternative solution based on additive interactions, as follows. To enable learning of spatiotemporal features at all possible scales, this modification is applied before the second residual unit at each spatial resolution of the network (indicated by “skip-stream” in Table 1), as exemplified by the connection at the conv5_x layers in Fig. Based on the above observations, we developed a novel approach to temporal residual connections that builds on the ConvNet design guidelines of chaining small [21] asymmetric [10, 23] filters, noted in Sec. This allows the straightforward use of standard two-stream ConvNets that have been pre-trained on large-scale datasets e.g.to leverage the massive amounts of training data from the ImageNet challenge. From left to right, top to bottom, the first row shows the convolutional and pooling building blocks, with the filter and pooling size shown as (W ×H × T,C), denoting width, height, temporal extent and number of feature channels, resp. In the last two rows we show the output size of these metalayers as well as the receptive field on which they operate. We also apply random cropping and scale augmentations: We randomly jitter the width and height of the 224× 224 input frame by ±25% and also randomly crop it from a maximum of 25% distance from the image borders. We found that this strategy significantly reduces the training duration with the drawback that each loss does not capture all available information. For our final model, we equip the spatiotemporal ResNet with a temporal max-pooling layer after pool5 (see Table 1, temporal average pooling led to inferior results) and continue training as above with the learning rate starting from 10−4 for 2K iterations followed by 10−5. For fair comparison, we follow the evaluation procedure of the original two-stream work [20] by sampling 25 frames (and their horizontal flips). However, rather than using 10 spatial 224× 224 crops from each of the frames, we apply fully convolutional testing both spatially (smallest side rescaled to 256) and temporally (the 25 frame-chunks) by classifying the video in a single forward pass, which takes ≈250ms on a Titan X GPU. Second, we consider HMDB51 [15], which has 6766 videos that show 51 different actions and generally is considered more challenging than UCF0101 due to the even wider variations in which actions occur. For both datasets, we use the provided evaluation protocol and report mean average accuracy over three splits into training and test sets. Interestingly, research in neuroscience also suggests that the human visual cortex is equipped with connections between the dorsal and the ventral stream to distribute motion information to separate visual areas [3, 27]. We achieve this by simply averaging the L2-normalized SVM scores of the FV-encoded IDT descriptors (i.e.HOG, HOF, MBH) [29] with the L2-normalized video predictions of our ST-ResNet*, again without softmax normalization. These relatively larger performance decrements again underline that our approach is better able to capture the available dynamic information, as there is less to be gained by augmenting it with IDT. Still, there is a benefit from the hand-crafted IDT features even with our approach, which could be attributed to its explicit compensation of camera motion.",substance_1000_section_model_v2,Spatiotemporal Residual Networks for Video Action Recognition,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art.","The paper presents a novel architecture that 1) combines residual networks with two-stream convolutional networks, and 2) injects connections from the motion stream to the appearance stream, to be able to capture spatio-temporal features. The paper shows experiments in both of the main action recognition datasets, achieving state-of-the-art accuracy in both. Overall I think the paper is great: good idea, careful experimentation, great results and clearly written.   Although the basic components of the architecture are pre-existing, I think that the high performance and careful experimentation and description make it a very useful contribution.   I only miss some experiments to visualize what kinds of spatio-temporal features are being learned.   Since large temporal windows are important (278-280) I would add a relevant reference: Long-term Temporal Convolutions for Action Recognition, Gul Varol, Ivan Laptev, Cordelia Schmid  Small typos:  (L 187)  ""resp.""  (L 255) ""UCF0101""  "," The paper is well written and the authors have also shown that the paper can be improved with the help of a more thorough analysis of the proposed method.The paper also shows that the proposed approach would be better if the paper could be used to show that it is not as well written as it seems, and that it would be nice to see that the method can be used in the future.The proposed method would be interesting to see if it is possible to use this method in a more general way.",95
42,22112,hybrid,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values. While a large body of the machine learning literature is built upon the assumption of having access to complete datasets, in many real-world problems only incomplete datasets are available. Single imputation methods try to replace each missing value with a plausible value that is the best fit given the value of other correlated features and knowledge extracted from the dataset (Hastie et al., 1999; Anderson, 1957). While MI offers interesting statistical insights about the reliability of analysis on incomplete data, the insight is imprecise as it is mainly concerned about the population of data samples rather than individual instances. While these methods are easy to implement and analyze, they often fail to capture the complex feature dependencies as well as structures present in many problems. For instance, Mattei & Frellsen (2018) suggested a method based on deep latent variable models and importance sampling that offers a tighter likelihood bound compared to the standard VAE bound. Also, these methods are often complicated to be applied in practical setups by practitioners. A few exceptions exist such as Bayesian models and decision trees that permit direct analysis on incomplete data (Nielsen & Jensen, 2009; Zhang et al., 2005). However, given an incomplete training dataset and without any known causal structure as a priori, learning such models is a very challenging problem with the complexity of at least NP-complete to learn the network architecture in addition to an iterative EM optimization to learn model parameters (Darwiche, 2009; Neapolitan et al., 2004). Similarly, for the second part, we are interested in obtaining a distribution over the possible target assignments and the confidence of each class rather than maximum likelihood class assignments. Instead, we use recent advances in GAN stabilization and training to improve the training process (see Section 3.4). Additionally, in our experiments, we provide supporting evidence that this simple loss function enables us to sample from the conditional distribution and prevents biased inclinations toward distribution modes. For instance, for a specific test sample at hand, if a certain missing feature is a strong indicator of the target class, we would like to observe the impact of different imputations for that feature on the final hypothesis. Input: G (trained imputer), D (dataset) Output: Fθ (trained predictor) foreach Training Epoch do foreach (xi, ki, yi) in D do z ∼ N(0, I) x̂i ← ki xi+(1−ki) G(xi,ki, z) ypredi ← Fθ(x̂i) loss← L(yi, ypredi ) Backpropagate loss Update Fθ Algorithm 2: Estimating target distributions. The first assumption is consistent with the theoretical analysis of generative adversarial networks that they can converge to the true underlying distribution (Arora et al., 2018; Liu et al., 2017). It should be noted that, while Zhang et al.(2018) suggests using a single self-attention layer in the middle of the network, we observed consistent improvements by inserting multiple self-attention layers before each residual block within the network. We explored best TTUR learning-rate settings from the set of {0.001, 0.0005, 0.0001, 0.00005}. To evaluate the proposed method we use CIFAR-10 (Krizhevsky & Hinton, 2009) as an image classification dataset as well as three non-image datasets: UCI Landsat (Dua & Graff, 2017)2, MIT-BIH arrhythmia (Moody & Mark, 2001), and Diabetes classification (Kachuee et al., 2019) 3. Also, while different encoding and representation methods are suggested in the literature to handle categorical features (Jang et al., 2016; Nazabal et al., 2018), in this paper, we take the simple approach of encoding categorical variables using one-hot representation and smoothing them by adding Gaussian noise with zero mean and variance equal to 5% of feature variances. We would like to note that while the suggested solution in this paper is readily compatible with MAR structures, in our experiments, to simplify the presentation of results and to have a fair comparison with other work that does not support the MAR assumption, we limited the scope of our experiments to MCAR. Furthermore, to simulate incomplete datasets and to make sure the same features are missing without explicitly storing masks, we use hashed feature vectors to seed random number generators used to sample missing features. More detail is provided in Appendix C. Fréchet inception distance (FID) (Heusel et al., 2017) score is used to measure the quality of missing data imputation in experiments with images5. We also considered using root means squared error (RMSE); however, we decided not to use this measure as we observed an inconsistent behavior using RMSE in our comparisons as RMSE favors methods that show less variance rather than realistic and sharp samples from the distribution. We compare our results with MisGAN (Li et al., 2019) and GAIN (Yoon et al., 2018) as the state of the art imputation algorithms based on GANs as well as basic denoising autoencoder (DAE) (Vincent et al., 2008) and multiple imputation by chained equations (MICE) (Buuren & Groothuis-Oudshoorn, 2010) as baselines. Due to scalability issues, we were only able to use MICE for the smaller non-image datasets. One possible explanation for this behavior might be the fact that GAIN has an MSE loss term acting similar to an autoencoder loss smoothing noisy missing pixels. Regarding the MIT-BIH experiemts, GI outperforms other work for missing rates more than 30% while achieving similar accuracies to GAIN for lower missing rates. As it can be seen from the plots, GI provides results closest to the ideal case of having average confidence values equal to average accuracies. In order to provide further insight into the operation of GI and how imputations can potentially influence the outcomes of predictions, we conduct experiments on a synthesized dataset. One possible explanation could be the fact that imputing missing data with a uniform structure can be done by processing local regions and does not require attending to different distant regions across the image. From Table 6 it can be inferred that as the rate of missingness increases, the benefits of the suggested predictor algorithm increase significantly.",substance_1000_section_model_v2,Generative Imputation and Stochastic Prediction,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values.","This paper proposes a method to impute missing features using a generative model and train a predictive model on top of imputed dataset to improve classification results. They first train a GAN model where the generator outputs an imputed representation of the input and discriminator is trained to predict if an individual features (such as a pixel) is imputed or not. Given the generator and incomplete sample, they train a predictor using the output of the generator, imputed sample, as input. Their main contribution is using a MC averaging to compute the prediction by repetitively sampling from the noise variable, z, and generating different imputations from generator. They show that the proposed model improves upon the previous SOTA on final classification performance.

Overall the paper is clearly written. But I do feel it is a bit incremental over the GAIN approach. The overall GAN architecture is very similar to GAIN's and although stochastic prediction shows clear improvements it is a bit straightforward. However, I think the uncertainty of the imputations and its effect on the final prediction is interesting. I suggest the authors to extend this part with more detailed analysis.

There are several parts that are confusing/missing in the paper:

- In GAIN, they use a hint vector as an input to the discriminator. They show that without the hint vector, there is no unique solution (this is shown without the MSE loss). The authors do not use this vector in their approach (as in Figure 1) and it is not clear to me if it causes any instabilities or if multiple experiments yield similar results or if the stochastic prediction benefits from this.
- On what type of examples GI is more accurate than other models? Since stochastic prediction is the main difference from GAIN, is this related to the multi-modality of the noisy examples?
- Can you explain the difference between the results in Figure-7 and Table-2? Results between the two mismatch.
- I think the statement in the first paragraph in Section 4.4 that ""MSE loss term would act as a denoising loss smoothing noisy missing pixels"" could be misleading. MSE is used with mask in GAIN, hence it only applies to the observed features during training. Its effect on smoothing noisy missing pixels is not clear.


I think the paper would benefit if the authors could explain/show:
- Increasing the missing rate would also increase the possibility that the ground truth be a more multi-modal distribution. Especially in rectangular generation part where it can remove a complete object. Does stochastic averaging benefit more in this case?"," The paper is well written and the authors have also shown that the paper can be improved with the help of a more thorough analysis of the proposed method.The paper also shows that the proposed approach would be better if the paper could be used to show that it is not as well written as it seems, and that it would be nice to see that the method can be used in the future.The proposed method would be interesting to see if it is possible to use this method in a more general way.",95
43,18864,hybrid,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting. Recent works have used this framework to achieve impressive feats such as learning to classify using one labeled image per class (Snell et al., 2017; Finn et al., 2017), learning unsupervised update rules that generalize to different domains (Metz et al., 2018), and accelerating training procedures that are millions of steps long (Flennerhag et al., 2018). DIMCO generalizes well to novel datasets because its learning objective encourages the model to compactly use all of its degrees of freedom, thus enabling it to effectively compare datapoints while only requiring a small number of bits per datapoint. Derive a generalization bound for meta-learning that makes the tradeoff between taskspecific and task-wise overfitting concrete. Notably, we suggest that certain previous meta-learning methods may have benefitted from implicit regularization. We present experiments in Section 5 and conclude the paper with a discussion about limitations and future directions of our approach in Section 6. This can be problematic since in many works, the task-specific learning algorithm is usually just a byproduct of whatever clever meta-learning loss was proposed. We propose a factorized discrete representation scheme which enables us to represent discrete distributions with exponentially fewer parameters compared to listing the probability of each event. We similarly perform image retrival by mapping each support image to its most likely code (10) and for each query image retrieving the support image that has highest (11). Regularizing Meta-Learners The ability to generalize to novel datasets is critical in meta-learning benchmarks, and even more so in benchmarks such as Meta-Dataset (Triantafillou et al., 2019), where a model is tested on datasets from an unseen domain. The following works have reported benefits from reducing the number of such task-specific parameters: Lee & Choi (2018) learns a subset of the full network to alter during task-specific learning, Rusu et al.(2018) explicitly represents each task with a low-dimensional latent space, and Zintgraf et al.(2018) alters only a pre-specified subset of the full network during task-specific learning. We showed through Theorem 1 that restricting inner-loop expressivity reduces the generalization gap; this provides theoretical understanding to this consensus that meta-learning models with simple task-specific learners generalize to new tasks more easily. Also related is the deterministic information bottleneck (Strouse & Schwab, 2017) which extends the information bottleneck by minimizing H(X̃) rather than I(X̃;X). These three approaches to generalization are related via the chain of inequalities I(X̃;X) ≤ H(X̃) ≤ log |X̃|, which is tight when X̃ is an efficient code. Recent deep learning methods directly learn discrete representations, by learning variational autoencoders with discrete latent variables (Rolfe, 2016; van den Oord et al., 2017; Razavi et al., 2019) or maximizing the mutual information between representation and 1 In practice, we add log probabilities instead of multiplying probabilities for numerical stability. Additionally, their method solves a minimum cost flow problem within each batch to find the locally optimal code, whereas DIMCO is able to directly compute its loss function. We randomly initialize weights for the 4-layer convnet and use pretrained weights for the Inception network This experiment empirically verifies whether mutual information I(X̃;Y ) is a reasonable metric for quality of representation. We observed similar trends when training with with previously proposed loss functions: we visualize these results in Figure 5 of the appendix due to space constraints. Results in Figure 3 show that the compact code of DIMCO takes roughly an order of magnitude less memory for similar performance to N-pair loss, and requires less query time as well. We additionally note that DIMCO is able to train using large backbones without significantly overfitting, whereas experiments reported in Mishra et al.(2017) indicate that MAML (Finn et al., 2017) overfits tremendously when using a deeper backbone. This challenging experimental setup measures how much generalizable information the model can extract from a limited set of datasets; it can be seen as the meta-learning analogue of measuring the performance of a classifier trained with a small dataset. We report the average and standard deviation of the top 5 results of a random hyperparameter search (see appendix for details). Towards Explicit Meta-Regularization In Section 4, we have suggested with analogy to Theorem 1 that the benefits of some previous meta-learning methods can be attributed to implicitly being regularized by reducing the expressivity of their task-specific learners. In future work, we would like to explore explicit meta-regularization schemes that can be applied to other problems (regression, reinforcement learning etc.) Along with showing that the traditional support/query split is not strictly necessary, we demonstrated in Section 5.4 that removing it has the benefit of enabling meta-learning in datasets having one image per class. (12) Assuming that the approximate distribution q(·) is sufficiently close to p(y|x̃), minimizing (12) can be seen as arg min xent(Y, X̃) ≈ arg minEy∼Y,x̃∼X̃ [ − log p(y|x̃) ] (13) = arg minH(Y |X̃) = arg max I(X̃;Y ), (14) where the last equality uses the fact that H(Y ) is independent of model parameters. (24) We similarly bound the error caused by estimating L with a finite number of tasks sampled from τ . Combining equations (26, 24), we have with high probability∣∣∣∣L(τ, θ)− − 1 n n∑ i=1 Î(X̃(XT i , θ);YT i) ∣∣∣∣ (27) ≤ ∣∣∣L(τ, θ)− L̂(τ, θ)∣∣∣+O( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (28) ≤O (√ dΘ n log n dΘ ) +O ( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (29) Hardware Every experiment was conducted on a single Nvidia V100 GPU with CUDA 9.2. Each experiment was performed with different fixed initial seeds; we manually fix seeds with manual_seed() for python, pytorch, and numpy. We additionally show in Figure 5 the correlation between 1-shot accuracies, Recall@1, and NMI using three previously proposed losses (triplet, npair, protonet).",substance_1000_section_model_v2,Discrete InfoMax Codes for Meta-Learning,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting.","This paper presents DIMCO, a meta-learner that is trained by maximizing mutual information between a discrete data representation and class labels across tasks. DIMCO is inspired by an information theoretic lower bound on the generalisation gap for meta-learning, which the authors argue identifies overfitting in the task learner as the bottleneck. 

This work proposes to constrain a learner to output discrete codes that are learned to capture the mutual information with class labels. While the idea of using discrete codes is interesting, its presentation in the manuscript is not well motivated and at times hard to follow. This makes it challenging to evaluate the novelty, validity, and generality of the proposed approach. Meanwhile, the empirical evaluation is somewhat lacking. Thus, I do not believe this work is ready for publication in its current form.

Detailed comments

My main concern is with respect to the primary contribution of this paper, a generalisation bound on meta-learning. The bound appears to be on a multi-task loss without task adaptation, and thus the claims made with respect to the theorem seem somewhat over-reaching. I also believe the VC-dimensionality of the encoder is missing in Eq. 4? If so, this changes the interpretation since the length of the code and the expressivity of the encoder are interrelated. Further, I would welcome a deeper analysis of the theorem and its implications. The current interpretation states that the number of tasks is independent of the size of each task, hence given many tasks, using minimal representations is an effective approach to meta-generalisation. Yet minimal representations is a well-known idea and has features in several works that use mutual information as a regularizer, most notably works on the Information Bottleneck. 

Another reservation I have is the use of mutual information between encoder representations and class labels as a loss function (Eq. 1). It lacks context and a proper motivation, especially since the analysis of [1] shows that the loss function in Eq. 1 is the cross-entropy objective. The authors make a similar analysis in Appendix A and argue that Eq. 1 differs in that cross-entropy is an approximation because it adds a parametrized linear layer on top of \tilde{X}. Thus, in the absence of that layer they collapse to the same objective. As DIMCO itself directly extract class label predictions from \tilde{X}, I fail to see a difference between the loss in Eq. 1 and a cross-entropy objective. 

The main motivation behind their loss objective is that it does not require a support / query set. This does not seem to be a feature of the mutual information objective itself, but rather a choice made by the authors. I would have liked a deeper discussion of this seeing as the authors make it a central tenet of DIMCO. Prior works use a support set as a principled means of doing meta-learning: meta-training explicitly takes into account that at test time, the learner will be given a small support set from which to learn how to query points. As far as I understand, DIMCO does not take this into account during meta-training.  At meta-test time however, DIMCO does use a support set to map query points (Eqs. 10 and 11). Why should we break protocols between meta training and testing? Are there any downsides to doing so?  

Empirically, I find the CUB experiment compelling but would welcome some ablations. What are the trade-offs between p and d? Can DIMCO outperform N-pair when number of bits are unconstrained?

miniImagenet is a standard benchmark in few-shot learning, but I am unable to find a results table - could the authors please provide results on the standard setup so that the method can be compared against known baselines? Further, would the results currently presented hold in a N-way-5-shot setup?

As for the constrained version of miniImagenet that the authors propose, I am not convinced this is an interesting protocol. In general, the miniImagenet task distribution is created by N-way permutations of the classes in the meta-set (e.g. meta-training tasks are combinations of the 64 classes in the meta-training set). By keeping the number of classes constant but reducing the number of images per class, this protocol is not reducing samples per task: a task is always defined as 5/20-way-1-shot (Fig. 4). Instead, the effect should be that tasks are in (greater) violation of the task i.i.d. assumption. Thus, I question whether this setup demonstrates the trade-offs the authors present in Theorem 1 and whether the results can be interpreted in light of it. 

Finally, that both experiments are image-based raises questions as to the generality of the method. The paper could be considerably strengthened by evaluating DIMCO on a non-image task, or if not discuss the method’s limitations.  

The idea of discrete codes for few-shot classification is interesting and sufficiently novel, I am likely to increase my score if my concerns are addressed and the experimental section is strengthened.

Further questions and comments:

- I am unable to parse Eq. 11 - what does the notation \prod_i p_{\tilde{x}_i, i} mean? 
- It is unnecessarily hard to follow the proof of theorem 1. It would help the reader if you restated relevant definitions, such as Eq. 1, since the difference with Eq. 23 is very subtle. It would also be helpful to explain how the summand in Eq. 25 differs from either, and Eq. 26 could be expanded or briefly explained after the derivation. 
-  Because DIMCO is trained with backpropagation, the interpretation of Eq. 5 as d independent events seems invalid. How does it affect the method if they are not independent?
- Overloading X and Y as both random variables and mini-batch samples creates unnecessary confusion. I believe the objective in Eq. 1 is approximated, not calculated exactly? For instance, the mutual information in Eq. 8 is with respect to a mini-batch, so should it not be \hat{I}? 
- p^j_{ik} in Eq. 9 is undefined.
- Eq. 9 is interpreted as an exact entropy, however it appears to be a mini-batch approximation to the true entropy? 

References 
[1] Achille and Soatto. Emergence of Invariance and Disentanglement in Deep Representations. JMLR. 2018."," The paper is well written and the authors have also shown that the paper can be improved with the help of a more thorough analysis of the proposed method.The paper also shows that the proposed approach would be better if the paper could be used to show that it is not as well written as it seems, and that it would be nice to see that the method can be used in the future.The proposed method would be interesting to see if it is possible to use this method in a more general way.",95
44,13063,hybrid,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all. But in each case, the network was designed and tuned specifically for the problem at hand. Natural language processing models have been shown to benefit from a multi-task approach a long time ago (Collobert & Weston, 2008), and recently multi-task machine translation models (MinhThang Luong, 2015) have even been shown to exhibit zero-shot learning when trained on multiple languages (Melvin Johnson, 2016). Speech recognition has also been shown to benefit from multi-task training (Seltzer & Droppo, 2013), as have some vision problems, such as facial landmark detection (Zhang Z., 2014). But no competitive multi-task multi-modal model has been proposed, so the above question remains unanswered. (3) COCO image captioning dataset (Lin et al., 2014), used for image captioning. (4) WSJ parsing dataset (Marcus et al., 1999), used for constituency parsing. While the MultiModel is only a first step and will be improved in the future, two key insights are crucial to making it work at all and are our main contributions. We design modality nets to be computationally minimal, promoting heavy feature extraction and ensuring that the majority of computation is performed within the domain-agnostic body of the model. Two design decisions were important: • The unified representation is variable-size. While a fixed-size representation is tempting and easier to implement, it creates a bottleneck and limits the performance of the model. For example, separable convolutions were introduced in the Xception architecture (Chollet, 2016) and were not applied to text or speech processing before. We find that each of these mechanisms is indeed crucial for the domain it was introduced, e.g., attention is far more important for languagerelated tasks than for image-related ones. As already said above, the encoder and decoder are constructed using 3 key computational blocks to get good performance across different problems: (1) Convolutions allow the model to detect local patterns and generalize across space. We refer the reader to (Chollet, 2016) for a complete definition; here we will denote a depthwise separable convolution with weights Wh×w corresponding to f kernels of size h × w applied to an input tensor x with stride s and dilated by a factor d (see (Yu & Koltun, 2015)) as SepConvd,s,f (W,x). The source tensor is finally passed through two different pointwise convolutions to generate the memory keys K and values V and the query keys, memory keys and memory values are used to apply the attention mechanism between the self-attended target and the source (see Figure 3). On the output side, the language modality takes the decoded output of the body and performs a learned linear mapping, followed by a Softmax, resulting in a probability distribution over the token vocabulary. The spectral modality does not perform any striding along the frequency bin dimension, preserving full resolution in the spectral domain. Convolutional architectures yielded good results on word-level neural machine translation starting from (Kalchbrenner & Blunsom, 2013) and later in (Meng et al., 2015). These early models used a standard RNN on top of the convolution to generate the output and had a bottleneck there that hurt performance, especially on longer sentences, similarly to the limitations of RNN sequence-to-sequence models without attention (Sutskever et al., 2014; Cho et al., 2014). This idea, introduced in WaveNet (van den Oord et al., 2016) and also used in MultiModel (see above) significantly improves efficiency. Depthwise separable convolutions were first studied by Sifre (Sifre & Mallat, 2013) and later they were used to get good results on large-scale image classification with Xception (Chollet, 2016). We focused our experiments so as to answer the following questions: (1) How far is the MultiModel trained on 8 tasks simultaneously from state-of-the-art results? Especially the 4 translation problems behave very similarly, so we decided to not include them all in each comparison but we focused on the more varied problems instead. The results we achieve are similar to the ones task-specific models get without heavy tuning, e.g., on English-French translation we improve on the recent Extended Neural GPU results (Kaiser & Bengio, 2016). Since we are comparing different instantiations of the same model, we report two internal metrics: the negative log-perplexity and per-token accuracy (measured on the development set). As can be seen from the results in Table 2, the joint 8-problem model performs similarly to single-model on large tasks, and better, sometimes significantly, on tasks where less data is available, such as parsing. The difference in performance is significant, and since we use both dropout and early stopping, we conjecture that it is not related to over-fitting. In fact, one could expect that removing these blocks will improve performance on ImageNet alone if they were truly useless for this task. In contrast, we see in Table 4 that these blocks either don’t affect or slightly improve performance. This leads us to conclude that mixing different computation blocks is in fact a good way to improve performance on many various tasks. We believe that this treads a path towards interesting future work on more general deep learning architectures, especially since our model shows transfer learning from tasks with a large amount of available data to ones where the data is limited.",substance_1000_section_model_v2,Large Scale Multi-Domain Multi-Task Learning with MultiModel,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.","The paper presents a multi-task, multi-domain model based on deep neural networks. The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition. The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic. The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task.

The paper is well written and easy to follow.

The contributions of the paper are novel and significant. The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring. The experiments clearly show the viability of the approach and give interesting insights. This is surely an important step towards more general deep learning models. 

Comments:

* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database. Moreover, some databases could be used for different tasks, such as WSJ or ImageNet.

* The training procedure of the model is not explained in the paper. What is the cost function and what is the strategy to train on multiple tasks ? The paper should at least outline the strategy.

* The experiments are sufficient to demonstrate the viability of the approach, but the experimental setup is not clear. Specifically, there is an issue about the speech recognition part of the experiment. It is not clear what the task exactly is: continuous speech recognition, isolated word recognition ? The metrics used in Table 1 are also not clear, they should be explained in the text. Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used. Information about the detailed setup is also lacking, specifically which test and development sets are used (the WSJ corpus has several sets).

* Using raw waveforms as audio modality is very interesting, but this approach is not standard for speech recognition, some references should be provided, such as:
P. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26–30.
D. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE.
T. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals. Learning the Speech Front-end With Raw Waveform CLDNNs. Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015.

Revised Review:
The main idea of the paper is very interesting and the work presented is impressive. However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks. The experiments are interesting, except for the WSJ speech task, which is almost meaningless. Indeed, it is not clear what the network has learned given the metrics presented, as the WER on WSJ should be around 5% for speech recognition.
I thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition. A simpler speech task such as Keyword Spotting could also be investigated.
"," The paper is well written and the authors have also shown that the paper can be improved with the help of a more thorough analysis of the proposed method.The paper also shows that the proposed approach would be better if the paper could be used to show that it is not as well written as it seems, and that it would be nice to see that the method can be used in the future.The proposed method would be interesting to see if it is possible to use this method in a more general way.",95
45,6299,hybrid,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines. The problem can also be understood as constructing a function f(G, nS , q) to predict nT , where the functional form of f(·) is generally unknown and has to be learned from a training dataset consisting of samples like (nS , q, nT ). Since nT is unknown, the problem cannot be solved by conventional search algorithms such as A⇤-search [11], which seeks to find paths between the given source and target nodes. For the KBC example in Figure 1(a), having access to the current node nt = Hawaii alone is not sufficient to know that the best action is moving to nt+1 = USA. Second, to address the challenge of sparse rewards, M-Walk exploits the fact that the MDP transition model is known and deterministic.2 Specifically, it combines Monte Carlo Tree Search (MCTS) with the RNN to generate trajectories that obtain significantly more positive rewards than using the RNN policy alone. The rest of the paper is organized as follows: Section 3 develops the M-Walk agent, including the model architecture, the training and testing algorithms.3 Experimental results are presented in Section 4. To see this, we observe from Figure 1(b) that once an action at (i.e., an edge in Ent or “STOP”) is selected, the next node nt+1 and its associated Ent+1 and Nnt+1 are known. The objective is to learn a policy that maximizes the terminal rewards, i.e., correctly identifies the target node with high probability. Recall from Section 2 (e.g., (1)) that one challenge in applying RL to the graph-walking problem is that the state st nominally includes the entire history of observations. We will explain in Section 3.2 how such parameter sharing enables indirect updates to the policy ⇡✓ via Q-learning from off-policy data. To address this issue, we apply the same FCN f✓(·) to 5There could be alternative ways to compute the score, such as Score(n) = maxsT!n Q✓(sT , STOP). Inspired by this recursion, we propose using the GRU-RNN [4] to encode qt into a vector representation6: qt+1 = f✓q (qt, [hA,t, hat,t, nt+1]) with initialization q0 = f✓q (q, [0, 0, nS ]), where ✓q is the model parameter, and hat,t denotes the vector hn0,t at n0 = at. Three Glass Puzzle The Three Glass Puzzle [20] is a problem studied in math puzzles and graph theory. WN18RR [6] is created from the original WN18 [2] by removing various sources of test leakage, making the dataset more challenging. We use HITS@1,3 and mean reciprocal rank (MRR) as the evaluation metrics for WN18RR, and use mean average precision (MAP) for NELL995,7 where HITS@K computes the percentage of the desired entities being ranked among the top-K list, and MRR computes an average of the reciprocal rank of the desired entities. We compare against RL-based methods [38, 5], embedding-based models (including DistMult [39], ComplEx [32] and ConvE [6]) and recent work in logical rules (NeuralLP) [40]. For all the baseline methods, we used the implementation released by the corresponding authors with their best-reported hyperparameter settings.8 The details of the hyperparameters for M-Walk are described in Appendix B.2.2 of the supplementary material. We ran the experiments three times and report the means and standard deviations (except for PRA, TransE, and TransR on NELL995, whose results are directly quoted from [38]). We observed that the novel neural architecture of M-Walk contributes an overall 1% gain relative to MINERVA on NELL995, and it is still 1% worse than M-Walk, which uses MCTS for training and testing. Third, we analyze the performance of M-Walk under different numbers of MCTS rollout simulations and different search horizons on WN18RR dataset, with results shown in Figure 5(a). We observe that M-Walk outperforms the strong baseline ConvE by 4.6–10.9% in samples that require 2 or 3 steps, while it is nearly on par for paths of length one. Therefore, M-Walk does better at reasoning over longer paths than ConvE. To examine this effect, we show in Figure 5(c)-top the HITS@K accuracies when the ground truth is in the candidate set.9 It shows that M-Walk has very high accuracy in this case, which is significantly higher than ConvE (80% vs 39.6% in HITS@1). These observations point to an important direction for improving M-Walk in future work: increasing the chance of covering the target by the candidate set. Finally, in Table 4, we show examples of reasoning paths found by M-Walk.10 Reinforcement Learning Recently, deep reinforcement learning has achieved great success in many artificial intelligence problems [17, 24, 25]. Note that the former is constructed from the visit counts of all the edges connected to the MCTS root node; it only uses information near the root node to improve the policy. Recent approaches have demonstrated limitations of these prior approaches: they suffer from cascading errors when dealing with compositional (multi-step) relationships [10]. Hence, recent works [8, 18, 10, 15, 30] have proposed approaches for injecting multi-step paths such as random walks through sequences of triples during training, further improving performance on KBC tasks. IRN [23] and Neural LP [40] explore multi-step relations by using an RNN controller with attention over an external memory. Empirically, our proposed algorithm outperforms both DeepPath and MINERVA in the KBC benchmarks.11 We developed an RL-agent (M-Walk) that learns to walk over a graph towards a desired target node for given input query and source nodes. Furthermore, we also performed extensive experimental analysis to understand M-Walk. 11A preliminary version of M-Walk with limited experiments was reported in the workshop paper [22].",clarity_1000_section_model_v2,M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search,"Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines.","Summary of the paper:  - This paper proposes a walk-based method for knowledge graph completion (KGC).  - For triples (s, q, o), an agent is trained so that given s and q, it traverses a path from the initial node s to the   terminal node t in a knowledge graph.  - This problem is formulated as that of Q-learning using RNN to model policy and Q functions, but the issue is the   sparseness of rewards.  - Monte Carlo tree search is used to overcome this issue.  Pros:  - The paper is clearly written.  - The approach is natural and sound.  - Very good performance compared with existing KGC methods.  - Ablation study with PG-Walk and Q-Walk is nice.  Cons:  - The work is mostly empirical, without much theoretical justification (admittedly, as in many deep learning papers).  - Not much novelty or surprise, given the success of AlphaGo.  This is a type of paper in which a successful technique   in other fields is applied to a different field.  I could not see exactly what is the novelty/creativity in terms of   how MCTS is applied for this specific problem of KGC.  - No analysis on running time is provided.   General Comments:  This paper is well written (but I would like the content of Section 4.2 to be more organized --- Page 7 consists of a single paragraph now!).  The novelty of the proposal is somewhat questionable, because reinforcement learning has already been introduced for KGC, and the application of MCTS naturally comes to mind.  Although this work is mostly empirical and little theoretical contribution is provided, it still achieves the state-of-the-art performance, and that could be a merit for its publication.  Questions:  * Related work  MCTS has been introduced in walk-based KGC by Sheng et al. in ICLR 2018 Workshop (ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search), and this work must be cited.  Please elaborate on how M-Walk differs from ReinforceWalk as much as you can tell from the extended abstract.  * Experiments:  - Dataset:  Is there any reason the comparison is done with NELL995 and WN18RR?  FB15k is also the standard dataset.  - Baselines  Why do the set of compared methods differ between datasets?  In particular, we know that TransE and TransR are somewhat weaker compared with more recent bilinear models, such as ComplEx, ANALOGY, and ConvE.  - Evaluation metrics  Also, why is the MAP score used on NELL995 whereas HITS@/MRR are used on WN18RR?  I have nothing against MAP scores, but typically HITS@/MRR are reported in literature on KGC even on the NELL-995 dataset (e.g., the MINERVA paper); using different metrics makes it difficult to validate the experimental results.  - Figure 6(a)  Is there any explanation as to why accuracy drops as the number of roll outs are increased to 256?  * Application to classification task  The method is only concerned with prediction of knowledge triple in which only one entity is missing. Can the proposed method be applied to the triple classification task where the truth of a given triple must be predicted? This is more or less straightforward in score-function-based models (provided that a suitable threshold can be determined), but I cannot see how this can be addressed with walk-based methods in general.  * Running time  Training and inference both involves roll-out, which raises concern about the running time of the algorithms.  How does M-walk fare against scoring-based methods (e.g., ComplEx) or MINERVA in terms of running time?  Please provide the time required for training and inference.  I am curious about inference time in particular. "," the is, the the the is the, the is the is is is, is, the, is,,, the is, to to the to is to, to.. the. is.,. and and the and is and, and",35
46,837,hybrid,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art. Since the introduction of the “AlexNet” architecture [14] in the 2012 ImageNet competition, ConvNets have dominated state-of-the-art performance across a variety of computer vision tasks, including object-detection, image segmentation, image classification, face recognition, human pose estimation and tracking. In conjunction with these advances as well as the evolution of network architectures, several design best practices have emerged [8, 21, 23, 24]. This functionality can be achieved by stacking many small filters or using large filters in the network; notably, the first choice can be implemented with fewer operations (faster, fewer parameters) and also allows inclusion of more nonlinearities. Third, dimensionality reduction (1×1 convolutions) before spatially aggregating filters (e.g.3×3) is supported by the fact that outputs of neighbouring filters are highly correlated and therefore these activations can be reduced before aggregation [23]. Fourth, spatial factorization into asymmetric filters can even further reduce computational cost and 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. Fifth, it is important to normalize the responses of each feature channel within a batch to reduce internal covariate shift [11]. To demonstrate the benefits of our proposed spatiotemporal ResNet architecture, it has been evaluated on two standard action recognition benchmarks where it greatly boosts the state-of-the-art. Yet another study compared several approaches to extending ConvNets into the temporal domain, but with rather disappointing results [13]: The architectures were not particularly sensitive to temporal modelling, with a slow fusion model performing slightly better than early and late fusion alternatives; moreover, similar levels of performance were achieved by a purely spatial network. Another research branch has investigated combining image information in network architectures across longer time periods. That approach takes advantage of the local spatial similarity in images; however, it only yields a minor increase over their baseline, which is a two-stream VGG-16 ConvNet [21] used as the input to their convolutional RNN. Batch normalization [11] and ReLU [14] are applied after each convolution; the network does not use hidden fc, dropout, or max-pooling (except immediately after the first layer). In preliminary experiments we found that direct connections between identical layers of the two streams led to an increase in validation error. We conjecture that these results are due to the large change that the signal of one network stream undergoes after injecting a fusion signal from the other stream. Therefore, we developed a more subtle alternative solution based on additive interactions, as follows. To enable learning of spatiotemporal features at all possible scales, this modification is applied before the second residual unit at each spatial resolution of the network (indicated by “skip-stream” in Table 1), as exemplified by the connection at the conv5_x layers in Fig. Based on the above observations, we developed a novel approach to temporal residual connections that builds on the ConvNet design guidelines of chaining small [21] asymmetric [10, 23] filters, noted in Sec. This allows the straightforward use of standard two-stream ConvNets that have been pre-trained on large-scale datasets e.g.to leverage the massive amounts of training data from the ImageNet challenge. From left to right, top to bottom, the first row shows the convolutional and pooling building blocks, with the filter and pooling size shown as (W ×H × T,C), denoting width, height, temporal extent and number of feature channels, resp. In the last two rows we show the output size of these metalayers as well as the receptive field on which they operate. We also apply random cropping and scale augmentations: We randomly jitter the width and height of the 224× 224 input frame by ±25% and also randomly crop it from a maximum of 25% distance from the image borders. We found that this strategy significantly reduces the training duration with the drawback that each loss does not capture all available information. For our final model, we equip the spatiotemporal ResNet with a temporal max-pooling layer after pool5 (see Table 1, temporal average pooling led to inferior results) and continue training as above with the learning rate starting from 10−4 for 2K iterations followed by 10−5. For fair comparison, we follow the evaluation procedure of the original two-stream work [20] by sampling 25 frames (and their horizontal flips). However, rather than using 10 spatial 224× 224 crops from each of the frames, we apply fully convolutional testing both spatially (smallest side rescaled to 256) and temporally (the 25 frame-chunks) by classifying the video in a single forward pass, which takes ≈250ms on a Titan X GPU. Second, we consider HMDB51 [15], which has 6766 videos that show 51 different actions and generally is considered more challenging than UCF0101 due to the even wider variations in which actions occur. For both datasets, we use the provided evaluation protocol and report mean average accuracy over three splits into training and test sets. Interestingly, research in neuroscience also suggests that the human visual cortex is equipped with connections between the dorsal and the ventral stream to distribute motion information to separate visual areas [3, 27]. We achieve this by simply averaging the L2-normalized SVM scores of the FV-encoded IDT descriptors (i.e.HOG, HOF, MBH) [29] with the L2-normalized video predictions of our ST-ResNet*, again without softmax normalization. These relatively larger performance decrements again underline that our approach is better able to capture the available dynamic information, as there is less to be gained by augmenting it with IDT. Still, there is a benefit from the hand-crafted IDT features even with our approach, which could be attributed to its explicit compensation of camera motion.",clarity_1000_section_model_v2,Spatiotemporal Residual Networks for Video Action Recognition,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art.","The paper presents a novel architecture that 1) combines residual networks with two-stream convolutional networks, and 2) injects connections from the motion stream to the appearance stream, to be able to capture spatio-temporal features. The paper shows experiments in both of the main action recognition datasets, achieving state-of-the-art accuracy in both. Overall I think the paper is great: good idea, careful experimentation, great results and clearly written.   Although the basic components of the architecture are pre-existing, I think that the high performance and careful experimentation and description make it a very useful contribution.   I only miss some experiments to visualize what kinds of spatio-temporal features are being learned.   Since large temporal windows are important (278-280) I would add a relevant reference: Long-term Temporal Convolutions for Action Recognition, Gul Varol, Ivan Laptev, Cordelia Schmid  Small typos:  (L 187)  ""resp.""  (L 255) ""UCF0101""  "," the, is the the the is the, the is the is is is, is, the, is,,, the is, to to the to is to, to.. the. is.,. and and the and is and, and",35
47,22112,hybrid,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values. While a large body of the machine learning literature is built upon the assumption of having access to complete datasets, in many real-world problems only incomplete datasets are available. Single imputation methods try to replace each missing value with a plausible value that is the best fit given the value of other correlated features and knowledge extracted from the dataset (Hastie et al., 1999; Anderson, 1957). While MI offers interesting statistical insights about the reliability of analysis on incomplete data, the insight is imprecise as it is mainly concerned about the population of data samples rather than individual instances. While these methods are easy to implement and analyze, they often fail to capture the complex feature dependencies as well as structures present in many problems. For instance, Mattei & Frellsen (2018) suggested a method based on deep latent variable models and importance sampling that offers a tighter likelihood bound compared to the standard VAE bound. Also, these methods are often complicated to be applied in practical setups by practitioners. A few exceptions exist such as Bayesian models and decision trees that permit direct analysis on incomplete data (Nielsen & Jensen, 2009; Zhang et al., 2005). However, given an incomplete training dataset and without any known causal structure as a priori, learning such models is a very challenging problem with the complexity of at least NP-complete to learn the network architecture in addition to an iterative EM optimization to learn model parameters (Darwiche, 2009; Neapolitan et al., 2004). Similarly, for the second part, we are interested in obtaining a distribution over the possible target assignments and the confidence of each class rather than maximum likelihood class assignments. Instead, we use recent advances in GAN stabilization and training to improve the training process (see Section 3.4). Additionally, in our experiments, we provide supporting evidence that this simple loss function enables us to sample from the conditional distribution and prevents biased inclinations toward distribution modes. For instance, for a specific test sample at hand, if a certain missing feature is a strong indicator of the target class, we would like to observe the impact of different imputations for that feature on the final hypothesis. Input: G (trained imputer), D (dataset) Output: Fθ (trained predictor) foreach Training Epoch do foreach (xi, ki, yi) in D do z ∼ N(0, I) x̂i ← ki xi+(1−ki) G(xi,ki, z) ypredi ← Fθ(x̂i) loss← L(yi, ypredi ) Backpropagate loss Update Fθ Algorithm 2: Estimating target distributions. The first assumption is consistent with the theoretical analysis of generative adversarial networks that they can converge to the true underlying distribution (Arora et al., 2018; Liu et al., 2017). It should be noted that, while Zhang et al.(2018) suggests using a single self-attention layer in the middle of the network, we observed consistent improvements by inserting multiple self-attention layers before each residual block within the network. We explored best TTUR learning-rate settings from the set of {0.001, 0.0005, 0.0001, 0.00005}. To evaluate the proposed method we use CIFAR-10 (Krizhevsky & Hinton, 2009) as an image classification dataset as well as three non-image datasets: UCI Landsat (Dua & Graff, 2017)2, MIT-BIH arrhythmia (Moody & Mark, 2001), and Diabetes classification (Kachuee et al., 2019) 3. Also, while different encoding and representation methods are suggested in the literature to handle categorical features (Jang et al., 2016; Nazabal et al., 2018), in this paper, we take the simple approach of encoding categorical variables using one-hot representation and smoothing them by adding Gaussian noise with zero mean and variance equal to 5% of feature variances. We would like to note that while the suggested solution in this paper is readily compatible with MAR structures, in our experiments, to simplify the presentation of results and to have a fair comparison with other work that does not support the MAR assumption, we limited the scope of our experiments to MCAR. Furthermore, to simulate incomplete datasets and to make sure the same features are missing without explicitly storing masks, we use hashed feature vectors to seed random number generators used to sample missing features. More detail is provided in Appendix C. Fréchet inception distance (FID) (Heusel et al., 2017) score is used to measure the quality of missing data imputation in experiments with images5. We also considered using root means squared error (RMSE); however, we decided not to use this measure as we observed an inconsistent behavior using RMSE in our comparisons as RMSE favors methods that show less variance rather than realistic and sharp samples from the distribution. We compare our results with MisGAN (Li et al., 2019) and GAIN (Yoon et al., 2018) as the state of the art imputation algorithms based on GANs as well as basic denoising autoencoder (DAE) (Vincent et al., 2008) and multiple imputation by chained equations (MICE) (Buuren & Groothuis-Oudshoorn, 2010) as baselines. Due to scalability issues, we were only able to use MICE for the smaller non-image datasets. One possible explanation for this behavior might be the fact that GAIN has an MSE loss term acting similar to an autoencoder loss smoothing noisy missing pixels. Regarding the MIT-BIH experiemts, GI outperforms other work for missing rates more than 30% while achieving similar accuracies to GAIN for lower missing rates. As it can be seen from the plots, GI provides results closest to the ideal case of having average confidence values equal to average accuracies. In order to provide further insight into the operation of GI and how imputations can potentially influence the outcomes of predictions, we conduct experiments on a synthesized dataset. One possible explanation could be the fact that imputing missing data with a uniform structure can be done by processing local regions and does not require attending to different distant regions across the image. From Table 6 it can be inferred that as the rate of missingness increases, the benefits of the suggested predictor algorithm increase significantly.",clarity_1000_section_model_v2,Generative Imputation and Stochastic Prediction,"In many machine learning applications, we are faced with incomplete datasets. In the literature, missing data imputation techniques have been mostly concerned with filling missing values. However, the existence of missing values is synonymous with uncertainties not only over the distribution of missing values but also over target class assignments that require careful consideration. In this paper, we propose a simple and effective method for imputing missing features and estimating the distribution of target assignments given incomplete data. In order to make imputations, we train a simple and effective generator network to generate imputations that a discriminator network is tasked to distinguish. Following this, a predictor network is trained using the imputed samples from the generator network to capture the classification uncertainties and make predictions accordingly. The proposed method is evaluated on CIFAR-10 image dataset as well as three real-world tabular classification datasets, under different missingness rates and structures. Our experimental results show the effectiveness of the proposed method in generating imputations as well as providing estimates for the class uncertainties in a classification task when faced with missing values.","This paper proposes a method to impute missing features using a generative model and train a predictive model on top of imputed dataset to improve classification results. They first train a GAN model where the generator outputs an imputed representation of the input and discriminator is trained to predict if an individual features (such as a pixel) is imputed or not. Given the generator and incomplete sample, they train a predictor using the output of the generator, imputed sample, as input. Their main contribution is using a MC averaging to compute the prediction by repetitively sampling from the noise variable, z, and generating different imputations from generator. They show that the proposed model improves upon the previous SOTA on final classification performance.

Overall the paper is clearly written. But I do feel it is a bit incremental over the GAIN approach. The overall GAN architecture is very similar to GAIN's and although stochastic prediction shows clear improvements it is a bit straightforward. However, I think the uncertainty of the imputations and its effect on the final prediction is interesting. I suggest the authors to extend this part with more detailed analysis.

There are several parts that are confusing/missing in the paper:

- In GAIN, they use a hint vector as an input to the discriminator. They show that without the hint vector, there is no unique solution (this is shown without the MSE loss). The authors do not use this vector in their approach (as in Figure 1) and it is not clear to me if it causes any instabilities or if multiple experiments yield similar results or if the stochastic prediction benefits from this.
- On what type of examples GI is more accurate than other models? Since stochastic prediction is the main difference from GAIN, is this related to the multi-modality of the noisy examples?
- Can you explain the difference between the results in Figure-7 and Table-2? Results between the two mismatch.
- I think the statement in the first paragraph in Section 4.4 that ""MSE loss term would act as a denoising loss smoothing noisy missing pixels"" could be misleading. MSE is used with mask in GAIN, hence it only applies to the observed features during training. Its effect on smoothing noisy missing pixels is not clear.


I think the paper would benefit if the authors could explain/show:
- Increasing the missing rate would also increase the possibility that the ground truth be a more multi-modal distribution. Especially in rectangular generation part where it can remove a complete object. Does stochastic averaging benefit more in this case?"," the is, the the the is the, the is the is is is, is, the, is,,, the is, to to the to is to, to.. the. is.,. and and the and is and, and",35
48,18864,hybrid,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting. Recent works have used this framework to achieve impressive feats such as learning to classify using one labeled image per class (Snell et al., 2017; Finn et al., 2017), learning unsupervised update rules that generalize to different domains (Metz et al., 2018), and accelerating training procedures that are millions of steps long (Flennerhag et al., 2018). DIMCO generalizes well to novel datasets because its learning objective encourages the model to compactly use all of its degrees of freedom, thus enabling it to effectively compare datapoints while only requiring a small number of bits per datapoint. Derive a generalization bound for meta-learning that makes the tradeoff between taskspecific and task-wise overfitting concrete. Notably, we suggest that certain previous meta-learning methods may have benefitted from implicit regularization. We present experiments in Section 5 and conclude the paper with a discussion about limitations and future directions of our approach in Section 6. This can be problematic since in many works, the task-specific learning algorithm is usually just a byproduct of whatever clever meta-learning loss was proposed. We propose a factorized discrete representation scheme which enables us to represent discrete distributions with exponentially fewer parameters compared to listing the probability of each event. We similarly perform image retrival by mapping each support image to its most likely code (10) and for each query image retrieving the support image that has highest (11). Regularizing Meta-Learners The ability to generalize to novel datasets is critical in meta-learning benchmarks, and even more so in benchmarks such as Meta-Dataset (Triantafillou et al., 2019), where a model is tested on datasets from an unseen domain. The following works have reported benefits from reducing the number of such task-specific parameters: Lee & Choi (2018) learns a subset of the full network to alter during task-specific learning, Rusu et al.(2018) explicitly represents each task with a low-dimensional latent space, and Zintgraf et al.(2018) alters only a pre-specified subset of the full network during task-specific learning. We showed through Theorem 1 that restricting inner-loop expressivity reduces the generalization gap; this provides theoretical understanding to this consensus that meta-learning models with simple task-specific learners generalize to new tasks more easily. Also related is the deterministic information bottleneck (Strouse & Schwab, 2017) which extends the information bottleneck by minimizing H(X̃) rather than I(X̃;X). These three approaches to generalization are related via the chain of inequalities I(X̃;X) ≤ H(X̃) ≤ log |X̃|, which is tight when X̃ is an efficient code. Recent deep learning methods directly learn discrete representations, by learning variational autoencoders with discrete latent variables (Rolfe, 2016; van den Oord et al., 2017; Razavi et al., 2019) or maximizing the mutual information between representation and 1 In practice, we add log probabilities instead of multiplying probabilities for numerical stability. Additionally, their method solves a minimum cost flow problem within each batch to find the locally optimal code, whereas DIMCO is able to directly compute its loss function. We randomly initialize weights for the 4-layer convnet and use pretrained weights for the Inception network This experiment empirically verifies whether mutual information I(X̃;Y ) is a reasonable metric for quality of representation. We observed similar trends when training with with previously proposed loss functions: we visualize these results in Figure 5 of the appendix due to space constraints. Results in Figure 3 show that the compact code of DIMCO takes roughly an order of magnitude less memory for similar performance to N-pair loss, and requires less query time as well. We additionally note that DIMCO is able to train using large backbones without significantly overfitting, whereas experiments reported in Mishra et al.(2017) indicate that MAML (Finn et al., 2017) overfits tremendously when using a deeper backbone. This challenging experimental setup measures how much generalizable information the model can extract from a limited set of datasets; it can be seen as the meta-learning analogue of measuring the performance of a classifier trained with a small dataset. We report the average and standard deviation of the top 5 results of a random hyperparameter search (see appendix for details). Towards Explicit Meta-Regularization In Section 4, we have suggested with analogy to Theorem 1 that the benefits of some previous meta-learning methods can be attributed to implicitly being regularized by reducing the expressivity of their task-specific learners. In future work, we would like to explore explicit meta-regularization schemes that can be applied to other problems (regression, reinforcement learning etc.) Along with showing that the traditional support/query split is not strictly necessary, we demonstrated in Section 5.4 that removing it has the benefit of enabling meta-learning in datasets having one image per class. (12) Assuming that the approximate distribution q(·) is sufficiently close to p(y|x̃), minimizing (12) can be seen as arg min xent(Y, X̃) ≈ arg minEy∼Y,x̃∼X̃ [ − log p(y|x̃) ] (13) = arg minH(Y |X̃) = arg max I(X̃;Y ), (14) where the last equality uses the fact that H(Y ) is independent of model parameters. (24) We similarly bound the error caused by estimating L with a finite number of tasks sampled from τ . Combining equations (26, 24), we have with high probability∣∣∣∣L(τ, θ)− − 1 n n∑ i=1 Î(X̃(XT i , θ);YT i) ∣∣∣∣ (27) ≤ ∣∣∣L(τ, θ)− L̂(τ, θ)∣∣∣+O( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (28) ≤O (√ dΘ n log n dΘ ) +O ( |X̃| log(m)√ m ) +O ( |X̃||Y | m ) (29) Hardware Every experiment was conducted on a single Nvidia V100 GPU with CUDA 9.2. Each experiment was performed with different fixed initial seeds; we manually fix seeds with manual_seed() for python, pytorch, and numpy. We additionally show in Figure 5 the correlation between 1-shot accuracies, Recall@1, and NMI using three previously proposed losses (triplet, npair, protonet).",clarity_1000_section_model_v2,Discrete InfoMax Codes for Meta-Learning,"This paper analyzes how generalization works in meta-learning. Our core contribution is an information-theoretic generalization bound for meta-learning, which identifies the expressivity of the task-specific learner as the key factor that makes generalization to new datasets difficult. Taking inspiration from our bound, we present Discrete InfoMax Codes (DIMCO), a novel meta-learning model that trains a stochastic encoder to output discrete codes. Experiments show that DIMCO requires less memory and less time for similar performance to previous metric learning methods and that our method generalizes particularly well in a challenging small-data setting.","This paper presents DIMCO, a meta-learner that is trained by maximizing mutual information between a discrete data representation and class labels across tasks. DIMCO is inspired by an information theoretic lower bound on the generalisation gap for meta-learning, which the authors argue identifies overfitting in the task learner as the bottleneck. 

This work proposes to constrain a learner to output discrete codes that are learned to capture the mutual information with class labels. While the idea of using discrete codes is interesting, its presentation in the manuscript is not well motivated and at times hard to follow. This makes it challenging to evaluate the novelty, validity, and generality of the proposed approach. Meanwhile, the empirical evaluation is somewhat lacking. Thus, I do not believe this work is ready for publication in its current form.

Detailed comments

My main concern is with respect to the primary contribution of this paper, a generalisation bound on meta-learning. The bound appears to be on a multi-task loss without task adaptation, and thus the claims made with respect to the theorem seem somewhat over-reaching. I also believe the VC-dimensionality of the encoder is missing in Eq. 4? If so, this changes the interpretation since the length of the code and the expressivity of the encoder are interrelated. Further, I would welcome a deeper analysis of the theorem and its implications. The current interpretation states that the number of tasks is independent of the size of each task, hence given many tasks, using minimal representations is an effective approach to meta-generalisation. Yet minimal representations is a well-known idea and has features in several works that use mutual information as a regularizer, most notably works on the Information Bottleneck. 

Another reservation I have is the use of mutual information between encoder representations and class labels as a loss function (Eq. 1). It lacks context and a proper motivation, especially since the analysis of [1] shows that the loss function in Eq. 1 is the cross-entropy objective. The authors make a similar analysis in Appendix A and argue that Eq. 1 differs in that cross-entropy is an approximation because it adds a parametrized linear layer on top of \tilde{X}. Thus, in the absence of that layer they collapse to the same objective. As DIMCO itself directly extract class label predictions from \tilde{X}, I fail to see a difference between the loss in Eq. 1 and a cross-entropy objective. 

The main motivation behind their loss objective is that it does not require a support / query set. This does not seem to be a feature of the mutual information objective itself, but rather a choice made by the authors. I would have liked a deeper discussion of this seeing as the authors make it a central tenet of DIMCO. Prior works use a support set as a principled means of doing meta-learning: meta-training explicitly takes into account that at test time, the learner will be given a small support set from which to learn how to query points. As far as I understand, DIMCO does not take this into account during meta-training.  At meta-test time however, DIMCO does use a support set to map query points (Eqs. 10 and 11). Why should we break protocols between meta training and testing? Are there any downsides to doing so?  

Empirically, I find the CUB experiment compelling but would welcome some ablations. What are the trade-offs between p and d? Can DIMCO outperform N-pair when number of bits are unconstrained?

miniImagenet is a standard benchmark in few-shot learning, but I am unable to find a results table - could the authors please provide results on the standard setup so that the method can be compared against known baselines? Further, would the results currently presented hold in a N-way-5-shot setup?

As for the constrained version of miniImagenet that the authors propose, I am not convinced this is an interesting protocol. In general, the miniImagenet task distribution is created by N-way permutations of the classes in the meta-set (e.g. meta-training tasks are combinations of the 64 classes in the meta-training set). By keeping the number of classes constant but reducing the number of images per class, this protocol is not reducing samples per task: a task is always defined as 5/20-way-1-shot (Fig. 4). Instead, the effect should be that tasks are in (greater) violation of the task i.i.d. assumption. Thus, I question whether this setup demonstrates the trade-offs the authors present in Theorem 1 and whether the results can be interpreted in light of it. 

Finally, that both experiments are image-based raises questions as to the generality of the method. The paper could be considerably strengthened by evaluating DIMCO on a non-image task, or if not discuss the method’s limitations.  

The idea of discrete codes for few-shot classification is interesting and sufficiently novel, I am likely to increase my score if my concerns are addressed and the experimental section is strengthened.

Further questions and comments:

- I am unable to parse Eq. 11 - what does the notation \prod_i p_{\tilde{x}_i, i} mean? 
- It is unnecessarily hard to follow the proof of theorem 1. It would help the reader if you restated relevant definitions, such as Eq. 1, since the difference with Eq. 23 is very subtle. It would also be helpful to explain how the summand in Eq. 25 differs from either, and Eq. 26 could be expanded or briefly explained after the derivation. 
-  Because DIMCO is trained with backpropagation, the interpretation of Eq. 5 as d independent events seems invalid. How does it affect the method if they are not independent?
- Overloading X and Y as both random variables and mini-batch samples creates unnecessary confusion. I believe the objective in Eq. 1 is approximated, not calculated exactly? For instance, the mutual information in Eq. 8 is with respect to a mini-batch, so should it not be \hat{I}? 
- p^j_{ik} in Eq. 9 is undefined.
- Eq. 9 is interpreted as an exact entropy, however it appears to be a mini-batch approximation to the true entropy? 

References 
[1] Achille and Soatto. Emergence of Invariance and Disentanglement in Deep Representations. JMLR. 2018."," the is, the the the is the, the is the is is is, is, the, is,,, the is, to to the to is to, to.. the. is.,. and and the and is and, and",35
49,13063,hybrid,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all. But in each case, the network was designed and tuned specifically for the problem at hand. Natural language processing models have been shown to benefit from a multi-task approach a long time ago (Collobert & Weston, 2008), and recently multi-task machine translation models (MinhThang Luong, 2015) have even been shown to exhibit zero-shot learning when trained on multiple languages (Melvin Johnson, 2016). Speech recognition has also been shown to benefit from multi-task training (Seltzer & Droppo, 2013), as have some vision problems, such as facial landmark detection (Zhang Z., 2014). But no competitive multi-task multi-modal model has been proposed, so the above question remains unanswered. (3) COCO image captioning dataset (Lin et al., 2014), used for image captioning. (4) WSJ parsing dataset (Marcus et al., 1999), used for constituency parsing. While the MultiModel is only a first step and will be improved in the future, two key insights are crucial to making it work at all and are our main contributions. We design modality nets to be computationally minimal, promoting heavy feature extraction and ensuring that the majority of computation is performed within the domain-agnostic body of the model. Two design decisions were important: • The unified representation is variable-size. While a fixed-size representation is tempting and easier to implement, it creates a bottleneck and limits the performance of the model. For example, separable convolutions were introduced in the Xception architecture (Chollet, 2016) and were not applied to text or speech processing before. We find that each of these mechanisms is indeed crucial for the domain it was introduced, e.g., attention is far more important for languagerelated tasks than for image-related ones. As already said above, the encoder and decoder are constructed using 3 key computational blocks to get good performance across different problems: (1) Convolutions allow the model to detect local patterns and generalize across space. We refer the reader to (Chollet, 2016) for a complete definition; here we will denote a depthwise separable convolution with weights Wh×w corresponding to f kernels of size h × w applied to an input tensor x with stride s and dilated by a factor d (see (Yu & Koltun, 2015)) as SepConvd,s,f (W,x). The source tensor is finally passed through two different pointwise convolutions to generate the memory keys K and values V and the query keys, memory keys and memory values are used to apply the attention mechanism between the self-attended target and the source (see Figure 3). On the output side, the language modality takes the decoded output of the body and performs a learned linear mapping, followed by a Softmax, resulting in a probability distribution over the token vocabulary. The spectral modality does not perform any striding along the frequency bin dimension, preserving full resolution in the spectral domain. Convolutional architectures yielded good results on word-level neural machine translation starting from (Kalchbrenner & Blunsom, 2013) and later in (Meng et al., 2015). These early models used a standard RNN on top of the convolution to generate the output and had a bottleneck there that hurt performance, especially on longer sentences, similarly to the limitations of RNN sequence-to-sequence models without attention (Sutskever et al., 2014; Cho et al., 2014). This idea, introduced in WaveNet (van den Oord et al., 2016) and also used in MultiModel (see above) significantly improves efficiency. Depthwise separable convolutions were first studied by Sifre (Sifre & Mallat, 2013) and later they were used to get good results on large-scale image classification with Xception (Chollet, 2016). We focused our experiments so as to answer the following questions: (1) How far is the MultiModel trained on 8 tasks simultaneously from state-of-the-art results? Especially the 4 translation problems behave very similarly, so we decided to not include them all in each comparison but we focused on the more varied problems instead. The results we achieve are similar to the ones task-specific models get without heavy tuning, e.g., on English-French translation we improve on the recent Extended Neural GPU results (Kaiser & Bengio, 2016). Since we are comparing different instantiations of the same model, we report two internal metrics: the negative log-perplexity and per-token accuracy (measured on the development set). As can be seen from the results in Table 2, the joint 8-problem model performs similarly to single-model on large tasks, and better, sometimes significantly, on tasks where less data is available, such as parsing. The difference in performance is significant, and since we use both dropout and early stopping, we conjecture that it is not related to over-fitting. In fact, one could expect that removing these blocks will improve performance on ImageNet alone if they were truly useless for this task. In contrast, we see in Table 4 that these blocks either don’t affect or slightly improve performance. This leads us to conclude that mixing different computation blocks is in fact a good way to improve performance on many various tasks. We believe that this treads a path towards interesting future work on more general deep learning architectures, especially since our model shows transfer learning from tasks with a large amount of available data to ones where the data is limited.",clarity_1000_section_model_v2,Large Scale Multi-Domain Multi-Task Learning with MultiModel,"Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.","The paper presents a multi-task, multi-domain model based on deep neural networks. The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition. The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic. The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task.

The paper is well written and easy to follow.

The contributions of the paper are novel and significant. The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring. The experiments clearly show the viability of the approach and give interesting insights. This is surely an important step towards more general deep learning models. 

Comments:

* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database. Moreover, some databases could be used for different tasks, such as WSJ or ImageNet.

* The training procedure of the model is not explained in the paper. What is the cost function and what is the strategy to train on multiple tasks ? The paper should at least outline the strategy.

* The experiments are sufficient to demonstrate the viability of the approach, but the experimental setup is not clear. Specifically, there is an issue about the speech recognition part of the experiment. It is not clear what the task exactly is: continuous speech recognition, isolated word recognition ? The metrics used in Table 1 are also not clear, they should be explained in the text. Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used. Information about the detailed setup is also lacking, specifically which test and development sets are used (the WSJ corpus has several sets).

* Using raw waveforms as audio modality is very interesting, but this approach is not standard for speech recognition, some references should be provided, such as:
P. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26–30.
D. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE.
T. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals. Learning the Speech Front-end With Raw Waveform CLDNNs. Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015.

Revised Review:
The main idea of the paper is very interesting and the work presented is impressive. However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks. The experiments are interesting, except for the WSJ speech task, which is almost meaningless. Indeed, it is not clear what the network has learned given the metrics presented, as the WER on WSJ should be around 5% for speech recognition.
I thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition. A simpler speech task such as Keyword Spotting could also be investigated.
"," the, is the the the is the, the is the is is is, is, the, is,,, the is, to to the to is to, to.. the. is.,. and and the and is and, and",35
