{"cells":[{"cell_type":"markdown","source":["# Configuration"],"metadata":{"id":"sIxCLGjOv9IJ"},"id":"sIxCLGjOv9IJ"},{"cell_type":"code","source":["import json\n","import os\n","import time\n","import pickle\n","\n","import seaborn as sns\n","import matplotlib.pylab as plt\n","import numpy as np\n","import pandas as pd\n","\n","import importlib\n","import nltk\n","\n","nltk.download('punkt')\n","\n","!nvidia-smi"],"metadata":{"id":"6nOepbwbv_UY"},"id":"6nOepbwbv_UY","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"307aa482"},"outputs":[],"source":["# Install necessary packages\n","\n","! pip install datasets\n","! pip install sentencepiece\n","! pip install rouge_score\n","! pip install wandb\n","! pip install bert-score\n","! pip install evaluate\n","! pip install transformers -U\n","! pip install bert-score\n","! pip install bertviz\n","\n","from transformers import utils"],"id":"307aa482"},{"cell_type":"markdown","source":["## Mount to Google Drive"],"metadata":{"id":"DDa0s4IQwCUw"},"id":"DDa0s4IQwCUw"},{"cell_type":"code","execution_count":null,"metadata":{"id":"V4pM0TjpDB9Z"},"outputs":[],"source":["# Connect code to Google Drive (if necessary)\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","gdrive_dir = '/content/drive/MyDrive/CS 282 Project/Checkpoint 2/cs282-project/'"],"id":"V4pM0TjpDB9Z"},{"cell_type":"markdown","source":["## Data Configuration"],"metadata":{"id":"X7653Br9wG7M"},"id":"X7653Br9wG7M"},{"cell_type":"code","source":["workdir = gdrive_dir\n","\n","input_filename = 'expanded_all_data.json'\n","\n","# Full DataSet Size is 15000 (review, paper), but due to GPU cost + time constraints, we use a sub-sample of 5000 reviews\n","sample_size = 25 \n","\n","seed = 100\n","\n","# Specify Extraction Method used to Generate Training Text from Papers\n","# Either: (intro, ce_extract, hybrid)\n","extraction_method = 'hybrid'\n","\n","# Maximum Token Length of Paper Extracts Used To Train Model\n","max_paper_extract_length = 1024 # 1024 is the max input size of a BART model\n","max_review_length = 1024\n","min_text_length = 100\n","\n","# Pre-Trained Hugging Face Seq2Seq Transformers Model\n","pre_trained_model_checkpoint = \"facebook/bart-large-cnn\"\n","\n","# Summarization Task Configuration\n","summarization_params = {\n","    \"summarization\": {\n","        \"early_stopping\": True,\n","        \"length_penalty\": 2.0, # BART (favor longer sequences)\n","        \"max_length\": max_review_length,\n","        \"min_length\": min_text_length,\n","        \"no_repeat_ngram_size\": 3, # BART default\n","        \"num_beams\": 4 # BART default\n","    }\n","}"],"metadata":{"id":"GYk-R9PNZhcb"},"id":"GYk-R9PNZhcb","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c4d9b322"},"source":["## Load Dataset"],"id":"c4d9b322"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e062c46a"},"outputs":[],"source":["from datasets import concatenate_datasets, DatasetDict, Dataset, load_dataset\n","from sklearn.model_selection import train_test_split\n","\n","# Load input data (post-extraction and pre-processing to downsample paper text)\n","all_input_df = pd.read_json(workdir + input_filename, orient='records')"],"id":"e062c46a"},{"cell_type":"markdown","source":["Load Tokenizer For Filtering"],"metadata":{"id":"Vkc3WgSKgpId"},"id":"Vkc3WgSKgpId"},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","# Load Tokenizer used with the corresponding pre-trained mode\n","tokenizer = AutoTokenizer.from_pretrained(pre_trained_model_checkpoint)"],"metadata":{"id":"JNFueJSFgwhC"},"id":"JNFueJSFgwhC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filter on token length [not currently used because it decreases the available \n","# dataset size for the CE_extraction method]\n","def filter_on_token_length(df, tokenizer, text_col, text_min=min_text_length, \n","                           text_max=max_paper_extract_length, review_min=min_text_length, \n","                           review_max=max_review_length):\n","    \n","    def test_length_constraints(txt, tokenizer, mn_length, mx_length):\n","        tokenized_txt = tokenizer(txt, max_length=None, truncation=False)\n","        num_tokens = len(tokenized_txt['input_ids'])\n","\n","        return (num_tokens >= mn_length) and (num_tokens <= mx_length)\n","      \n","    return df[\n","              (df[text_col].apply(lambda s: test_length_constraints(s, tokenizer, text_min, text_max))) &\n","              (df['review'].apply(lambda s: test_length_constraints(s, tokenizer, review_min, review_max)))\n","           ]"],"metadata":{"id":"Sa5oYS5lhPe_"},"id":"Sa5oYS5lhPe_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["filt_input_df = all_input_df # [not currently used] filter_on_token_length(df_exp, tokenizer, extraction_method)\n","if sample_size:\n","    sample_df = filt_input_df.sample(sample_size, random_state=seed)\n","input_dataset = Dataset.from_pandas(sample_df if sample_size else filt_input_df)\n","input_dataset_dict = input_dataset.train_test_split(test_size=0.20)"],"metadata":{"id":"KuNhPSLohOVZ"},"id":"KuNhPSLohOVZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6b4cddda"},"source":["## Tokenize DataSet"],"id":"6b4cddda"},{"cell_type":"code","source":["def preprocess_function_for_review_task(paper_json, extraction_method='intro'):\n","    if extraction_method == 'intro':\n","        input_text = paper_json['intro']\n","    elif extraction_method == 'ce_extract':\n","        input_text = paper_json['ce_extract']\n","    elif extraction_method == 'hybrid':\n","        input_text = paper_json['ce_extract'] + paper_json['abstract']\n","\n","    model_inputs = tokenizer(\n","        input_text,\n","        max_length=max_paper_extract_length,\n","        truncation=True,\n","    )\n","\n","    labels = tokenizer(\n","        text_target=paper_json[\"review\"], \n","        max_length=max_review_length, truncation=True\n","    )\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    # print(len(model_inputs['labels']))\n","    # print(len(paper_json['text']))\n","    return model_inputs"],"metadata":{"id":"jPFus8qeo4us"},"id":"jPFus8qeo4us","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eI0JbZ49SoKU"},"outputs":[],"source":["tokenized_dataset_for_reviews = input_dataset_dict.map(lambda s: preprocess_function_for_review_task(s, extraction_method=extraction_method))"],"id":"eI0JbZ49SoKU"},{"cell_type":"code","source":["len(tokenized_dataset_for_reviews['train'][0]['input_ids']) # validate that tokenizer creates encoding of expected token length"],"metadata":{"id":"sggu9SA5PDsl"},"id":"sggu9SA5PDsl","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create Attention Maps"],"metadata":{"id":"33hubwPfLK4L"},"id":"33hubwPfLK4L"},{"cell_type":"code","source":["from transformers import BartForConditionalGeneration\n","from transformers import AutoTokenizer, AutoModel\n","from transformers import AutoModelForSeq2SeqLM\n","from bertviz import model_view\n","from bertviz import head_view\n","\n","import seaborn as sns\n","import matplotlib.colors as colors\n","import matplotlib.font_manager\n","\n","# Example: https://github.com/jessevig/bertviz/blob/master/notebooks/model_view_encoder_decoder.ipynb"],"metadata":{"id":"Wm_7JXPnb_3n"},"id":"Wm_7JXPnb_3n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load pre-trained model + tokenizer"],"metadata":{"id":"62-8IWf7LQmQ"},"id":"62-8IWf7LQmQ"},{"cell_type":"code","source":["local_checkpoint = workdir + 'outputmodel/review_generation/' + extraction_method\n","\n","# Load Tokenizer used with the corresponding pre-trained mode\n","tokenizer = AutoTokenizer.from_pretrained(pre_trained_model_checkpoint)\n","\n","# Load Pre-TrainedModel from BART\n","if local_checkpoint:\n","    pre_trained_model_checkpoint = local_checkpoint\n","\n","# BartForConditionalGeneration\n","model = BartForConditionalGeneration.from_pretrained(pre_trained_model_checkpoint, \n","                                    max_length=max_review_length, # use max_new_tokens instead?\n","                                    min_length=min_text_length,\n","                                    task_specific_params=summarization_params,\n","                                    output_attentions=True)\n","\n","# model_name = 'outputmodel/review_generation/' + extraction_method\n","# model = AutoModelForSeq2SeqLM.from_pretrained(workdir + model_name)"],"metadata":{"id":"YDFJRCwcLMyc"},"id":"YDFJRCwcLMyc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Select sample input text for attention visualization"],"metadata":{"id":"DRnkV93uwW4H"},"id":"DRnkV93uwW4H"},{"cell_type":"code","source":["txt = tokenized_dataset_for_reviews['train'][1][extraction_method]\n","print(txt)"],"metadata":{"id":"WwM2eHgBV1fI"},"id":"WwM2eHgBV1fI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# select subset of text\n","num_chars = 800\n","subset_text = txt[0:num_chars]\n","\n","# get encoded input vectors\n","inputs = tokenizer(subset_text, return_tensors=\"pt\", truncation=True)\n","encoder_input_ids = inputs.input_ids\n","\n","# generate model outputs\n","model_outputs = model.generate(encoder_input_ids)\n","decoded_output = \"\"\n","for output in model_outputs:\n","  decoded_output += tokenizer.decode(output, skip_special_tokens=True)\n","\n","# create ids of encoded input vectors\n","with tokenizer.as_target_tokenizer():\n","    decoder_input_ids = tokenizer(decoded_output, return_tensors=\"pt\").input_ids"],"metadata":{"id":"NmtvaWuihDWK"},"id":"NmtvaWuihDWK","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run model and retrive encoder and decoder text"],"metadata":{"id":"kl9axFXFw-u2"},"id":"kl9axFXFw-u2"},{"cell_type":"code","source":["outputs = model(input_ids=encoder_input_ids, decoder_input_ids=decoder_input_ids)"],"metadata":{"id":"gT878S2gm_ZG"},"id":"gT878S2gm_ZG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder_text = tokenizer.convert_ids_to_tokens(encoder_input_ids[0])\n","decoder_text = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n","\n","encoder_labels = []\n","for token in encoder_text:\n","  if token[0] == 'Ġ' or token[0] == 'Ċ':\n","    encoder_labels.append(token[1:])\n","  else:\n","    encoder_labels.append(token)\n","\n","decoder_labels = []\n","for token in decoder_text:\n","  if token[0] == 'Ġ' or token[0] == 'Ċ':\n","    decoder_labels.append(token[1:])\n","  else:\n","    decoder_labels.append(token)"],"metadata":{"id":"cUvtz8-fvWEm"},"id":"cUvtz8-fvWEm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print generated review\n","print(' '.join(decoder_labels))"],"metadata":{"id":"xYqaBeeivb9g"},"id":"xYqaBeeivb9g","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eAEDKmjUy_Qy"},"id":"eAEDKmjUy_Qy","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Select attention head to examine, and a subset of encoder/decoder text"],"metadata":{"id":"DYoZtmoYxghG"},"id":"DYoZtmoYxghG"},{"cell_type":"code","source":["# BART is 12 layers, 16 heads per layer\n","layer = 8\n","head = 0\n","\n","outputs.cross_attentions[layer][0][head].size()\n","attn_pytorch = outputs.cross_attentions[layer][0][head]\n","attn_np = attn_pytorch.detach().numpy()"],"metadata":{"id":"eCkhZWJby_3Q"},"id":"eCkhZWJby_3Q","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display entire heatmap first for overview of attention patterns\n","input_start = 0\n","input_end = len(encoder_labels)\n","\n","output_start = 0\n","output_end = len(decoder_labels)\n","\n","fig, ax = plt.subplots(figsize=(15, 10), dpi = 150)\n","norm = colors.TwoSlopeNorm(vmin=0, vcenter = 0.5, vmax=1)\n","s = sns.heatmap(np.transpose(attn_np[output_start:output_end][:, input_start:input_end]), fmt=\"\",cmap='Blues',linewidths=0.1,ax=ax, norm = norm)\n","plt.show()"],"metadata":{"id":"a5vgy8IbmTu3"},"id":"a5vgy8IbmTu3","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Investigate subsets\n","\n","input_start = 140\n","input_end = 150\n","\n","output_start = 70\n","output_end = 80\n","\n","fig, ax = plt.subplots(figsize=(15, 10), dpi = 150)\n","norm = colors.TwoSlopeNorm(vmin=0, vcenter = 0.5, vmax=1)\n","s = sns.heatmap(np.transpose(attn_np[output_start:output_end][:, input_start:input_end]), fmt=\"\",cmap='Blues',linewidths=1,ax=ax, norm = norm)\n","ax.set_yticklabels(encoder_labels[input_start:input_end], rotation=0, fontsize = 12)\n","ax.set_xticklabels(decoder_labels[output_start:output_end], rotation=90, fontsize = 16)\n","plt.show()"],"metadata":{"id":"DM-SgQpXyfdz"},"id":"DM-SgQpXyfdz","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # sample setups:\n","\n","# # txt 0, 500 chars\n","# layer = 8\n","# head = 8\n","# input_start = 1\n","# input_end = 18\n","# output_start = 15\n","# output_end = 23\n","\n","# # txt 0, 500 chars\n","# layer = 9\n","# head = 9\n","# input_start = 43\n","# input_end = 60\n","# output_start = 44\n","# output_end = 52"],"metadata":{"id":"kZoNIQeDw3pW"},"id":"kZoNIQeDw3pW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate final visualization\n","\n","input_start = 136\n","input_end = 153\n","\n","output_start = 71\n","output_end = 79\n","\n","attn_viz = attn_np[output_start:output_end]\n","attn_viz = attn_viz[:, input_start:input_end]\n","attn_viz = np.transpose(attn_viz)\n","\n","fig, ax = plt.subplots(figsize=(0.7*(output_end-output_start), 0.5*(input_end-input_start)), dpi = 150)\n","norm = colors.TwoSlopeNorm(vmin=0, vcenter = 0.5, vmax=1)\n","cfont = {'fontname':'Calibri'}\n","\n","s = sns.heatmap(attn_viz, fmt=\"\",cmap='Blues',linewidths=1,ax=ax, norm = norm)\n","s.set_xlabel('Output Tokens (Generated Review)', fontsize=16, labelpad=10) #fontname = 'keyboard'\n","s.set_ylabel('Input Tokens (Paper Text)', fontsize=16, labelpad = 5)\n","\n","ax.set_xticklabels(decoder_labels[output_start:output_end], rotation=90, fontsize = 16)\n","ax.set_yticklabels(encoder_labels[input_start:input_end], rotation=0, fontsize = 16)\n","\n","title_text = 'Cross Attention for Layer ' + str(layer) + ' Head ' + str(head)\n","plt.title(title_text, fontsize = 20, pad=20)\n","\n","plt.savefig('attn_'+str(layer)+'_'+str(head), bbox_inches='tight')\n","plt.show()"],"metadata":{"id":"OQASqpKQmnSe"},"id":"OQASqpKQmnSe","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JWrvnKObvm2y"},"id":"JWrvnKObvm2y","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["BertViz visualizations (no longer in use)"],"metadata":{"id":"SDJQkcUsvoQe"},"id":"SDJQkcUsvoQe"},{"cell_type":"code","source":["# # BertViz\n","\n","# model_view(\n","#     encoder_attention=outputs.encoder_attentions,\n","#     decoder_attention=outputs.decoder_attentions,\n","#     cross_attention=outputs.cross_attentions,\n","#     encoder_tokens= encoder_text,\n","#     decoder_tokens= decoder_text\n","# )"],"metadata":{"id":"L3mUND_clVWi"},"id":"L3mUND_clVWi","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from transformers import AutoTokenizer, AutoModel, utils\n","# from bertviz import model_view\n","\n","# utils.logging.set_verbosity_error()  # Remove line to see warnings\n","\n","# # Initialize tokenizer and model. Be sure to set output_attentions=True.\n","# # Load BART fine-tuned for summarization on CNN/Daily Mail dataset\n","# model_name = \"facebook/bart-large-cnn\"\n","# tokenizer = AutoTokenizer.from_pretrained(model_name)\n","# model = AutoModel.from_pretrained(model_name, output_attentions=True) # LOOK AT THIS!!!!\n","\n","# # get encoded input vectors\n","# encoder_input_ids = tokenizer(\"The House Budget Committee voted Saturday to pass a $3.5 trillion spending bill\", return_tensors=\"pt\", add_special_tokens=True).input_ids\n","\n","# # create ids of encoded input vectors\n","# decoder_input_ids = tokenizer(\"The House Budget Committee passed a spending bill.\", return_tensors=\"pt\", add_special_tokens=True).input_ids\n","\n","# outputs = model(input_ids=encoder_input_ids, decoder_input_ids=decoder_input_ids)\n","\n","# encoder_text = tokenizer.convert_ids_to_tokens(encoder_input_ids[0])\n","# decoder_text = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n","\n","# model_view(\n","#     encoder_attention=outputs.encoder_attentions,\n","#     decoder_attention=outputs.decoder_attentions,\n","#     cross_attention=outputs.cross_attentions,\n","#     encoder_tokens= encoder_text,\n","#     decoder_tokens=decoder_text\n","# )"],"metadata":{"id":"9i1ayD6yfpyy"},"id":"9i1ayD6yfpyy","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oVZAGDvyjXp8"},"id":"oVZAGDvyjXp8","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"115accdwN17A8YW2z9GHhew4S-tFmLHAE","timestamp":1680729988182},{"file_id":"1c0iljRJvo7YxqkAl_HLKR_oKSZm2tgNy","timestamp":1678307390361}],"private_outputs":true,"toc_visible":true},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}