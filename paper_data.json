[
    {
        "title": "Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning",
        "decision": "Accept",
        "conference": "NIPS",
        "text": "We consider the time-series prediction task of producing a desired output yt at each timestep t\u2208{1, . . . , T} given an observed input sequence x1:t = {x1,x2, \u00b7 \u00b7 \u00b7 ,xt}, where xt \u2208 RR and yt\u2208RS are vectors1. In this paper, we introduce a way to both widen and deepen the LSTM whilst keeping the parameter number and runtime largely unchanged. We adopt parameter sharing to cutdown the parameter number for RNNs, since compared with factorization, it has the following advantages: (i) scalability, i.e., the number of shared parameters can be set independent of the hidden state size, and (ii) separability, i.e., the information flow can be carefully managed by controlling the receptive field, allowing one to shift RNN deep computations to the temporal domain (see Sec. We start to describe our model based on 2D tensors, and finally show how to strengthen the model with higher-dimensional tensors. However, our method does not need to change the network structure and also allows different kinds of interactions as long as the output is separable, e.g, one can increase the local connections and use feedback (see Fig. In this manner we increase the complexity of the input-to-output mapping (by delaying outputs) and limit parameter growth (by sharing transition parameters using convolutions). Also, unlike the sRNN of runtime complexity O(TL), tRNN breaks down the runtime complexity to O(T+L), which means either increasing the sequence length T or the network depth L would not significantly increase the runtime. This results in our tLSTM tensor update equations: [Agt ,A i t,A f t ,A o t ,A q t ] =H cat t\u22121 ~ {W h, bh} (14) [Gt, It,Ft,Ot,Qt] = [\u03c6(A g t ), \u03c3(A i t), \u03c3(A f t ), \u03c3(A o t ), \u03c2(A q t )] (15) W ct (p) = reshape (qt,p, [K, 1, 1]) (16) Cconvt\u22121 = Ct\u22121 ~W c t (p) (17) Ct = Gt It +Cconvt\u22121 Ft (18) Ht = \u03c6(Ct) Ot (19) where, in contrast to (10)-(13), the kernel {W h, bh} has additional \u3008K\u3009 output channels2 to generate the activation Aqt \u2208RP\u00d7\u3008K\u3009 for the dynamic kernel bankQt\u2208RP\u00d7\u3008K\u3009, qt,p\u2208R\u3008K\u3009 is the vectorized adaptive kernel at the location p of Qt, and W ct (p)\u2208RK\u00d71\u00d71 is the dynamic kernel of size K with a single input/output channel, which is reshaped from qt,p (see Fig. Similar to the observation in [3] that LN does not work well in CNNs where channel vectors at different locations have very different statistics, we find that LN is also unsuitable for tLSTM where lower level information is near the input while higher level information is near the output. its depth L. How can we expand the tensor volume more rapidly so that the network can be widened more efficiently? We evaluate tLSTM on five challenging sequence learning tasks under different configurations: (a) sLSTM (baseline): our implementation of sLSTM [21] with parameters shared across all layers. We also investigate how the runtime is affected by the depth, where the runtime is measured by the average GPU milliseconds spent by a forward and backward pass over one timestep of a single example. 3.1 Wikipedia Language Modeling The Hutter Prize Wikipedia dataset [25] consists of 100 million characters taken from 205 different characters including alphabets, XML markups and special symbols. However, 3D tLSTM+CN consistently improves 3D tLSTM with different L. Whilst the runtime of sLSTM is almost proportional to L, it is nearly constant in each tLSTM configuration and largely independent of L. We compare a larger model, i.e.a 3D tLSTM+CN with L=6 and M= 1200, to the state-of-the-art methods on the test set, as reported in Table 1. Note that in both tasks, the correct solution can be found (when 100% test accuracy is achieved) due to the repetitive nature of the task. In our experiment, we also observe that for the addition task, 3D tLSTM+CN with L=7 outperforms other configurations and finds the solution with only 298K training samples, while for the memorization task, 3D tLSTM+CN with L=10 beats others configurations and achieves perfect memorization after seeing 54K training samples. The MNIST dataset [35] consists of 50000/10000/10000 handwritten digit images of size 28\u00d728 for training/validation/test. We have two tasks on this dataset: (a) Sequential MNIST: The goal is to classify the digit after sequentially reading the pixels in a scanline order [33]. However, removing feedback connections from 2D tLSTM seems not to affect the performance. On the other hand, CN enhances the 3D tLSTM and when L\u2265 3 it outperforms LN. For the sequential pMNIST, our 3D tLSTM+CN with L= 5 has a test accuracy of 95.7%, which is close to the state-of-the-art of 96.7% produced by the Dilated CNN [40] in [8]. As the network gets wider and deeper, we found that the memory cell convolution mechanism is crucial to maintain improvement in performance. It is also intriguing to examine the internal working mechanism of tLSTM. 6 reveal the distinct behaviors of tLSTM when dealing with different tasks: (i) Wikipedia: the input can be carried to the output location with less modification if it is sufficient to determine the next character, and vice versa; (ii) addition: the first integer is gradually encoded into memories and then interacts (performs addition) with the second integer, producing the sum; (iii) memorization: the network behaves like a shift register that continues to move the input symbol to the output location at the correct timestep; (iv) sequential MNIST: the network is more sensitive to the pixel value change (representing the contour, or topology of the digit) and can gradually accumulate evidence for the final prediction; (v) sequential pMNIST: the network is sensitive to high value pixels (representing the foreground digit), and we conjecture that this is because the permutation destroys the topology of the digit, making each high value pixel potentially important. 6 we can also observe common phenomena for all tasks: (i) at each timestep, the values at different tensor locations are markedly different, implying that wider (larger) tensors can encode more information, with less effort to compress it; (ii) from the input to the output, the values become increasingly distinct and are shifted by time, revealing that deep computations are indeed performed together with temporal computations, with long-range dependencies carried by memory cells. Unlike cLSTMs, tLSTM aims to increase the capacity of LSTMs when the input at each timestep is non-structured, i.e., a single vector, and is advantageous over cLSTMs in that: (i) it performs the convolution across different hidden layers whose structure is independent of the input structure, and integrates information bottom-up and top-down; while cLSTM performs the convolution within each hidden layer whose structure is coupled with the input structure, thus will fall back to the vanilla LSTM if the input at each timestep is a single vector; (ii) it can be widened efficiently without additional parameters by increasing the tensor size; while cLSTM can be widened by increasing the kernel size or kernel channel, which significantly increases the number of parameters; (iii) it can be deepened with little additional runtime by delaying the output; while cLSTM can be deepened by using more hidden layers, which significantly increases the runtime; (iv) it captures long-range dependencies from multiple directions through the memory cell convolution; while cLSTM struggles to capture long-range dependencies from multiple directions since memory cells are only gated along one direction. To keep the parameter number small and ease training, Graves [22], Kalchbrenner et al.[30], Mujika et al.[38], Zilly et al.[54] apply another RNN/LSTM along the depth direction of dLSTMs, which, however, multiplies the runtime. Moreover, the capacity of tLSTM can be increased more efficiently by using higher-dimensional tensors, whereas in dLSTM all hidden layers as a whole only equal to a 2D tensor (i.e., a stack of hidden vectors), the dimensionality of which is fixed. Unlike these methods, tLSTM can speed up not only training but also online inference for many tasks since it performs the deep computation by the temporal computation, which is also human-like: we convert each signal to an action and meanwhile receive new signals in a non-blocking way. We validated our model on a variety of tasks, showing its potential over other popular approaches.",
        "reviews": [
            "The paper explores a new way to share parameters in an RNN by using convolution inside an LSTM cell for an unstructured input sequence, and using tensors as convolution kernels and feature maps. The method also adds depth to the model by delaying the output target for a specified number of steps.\n\nThe idea is quite interesting, and is novel as far as I can tell. The authors provide clear formulation (although a rather complicated one) and provide some experimental results. \nOn a real world dataset (wikipedia LM) the method seems very close to SOA, with about half the parameters.\n\nThe problems I see with this approach are:\n- I find it hard to believe that meaningful high dimensional feature maps can be created for most problems, thus scaling to high dimensional tensors is questionable (The authors only try up to dimension 3)\n- Using \u00e2\u0080\u009cdepth in time\u00e2\u0080\u009d introduces a delay and is not suitable for streaming applications (e.g. speech)\n- For high dimensional tensors the number of hyper parameters can become quite large.\n\nMinor nit:\n- Line 242, it says \u00e2\u0080\u009cFig.3\u00e2\u0080\u009d and should be \u00e2\u0080\u009cTable 3\u00e2\u0080\u009d\n",
            "The paper introduces a tensorized version of LSTM that allows for implicitly adding depth and width to the network while controlling the computational runtime.\nThe paper is clearly written, the contribution is interesting and the experimental validation is convincing.",
            "\nThis paper proposes Tensorized LSTMs for efficient sequence learning. It represents hidden layers as tensors, and employs cross-layer memory cell convolution for efficiency and effectiveness. The model is clearly formulated. Experimental results show the utility of the proposed method.\n\nAlthough the paper is well written, I still have some questions/confusion as follows. I would re-consider my final decision if the authors address these points in rebuttal.\n\n1. My biggest confusion comes from Sec 2.1, when the authors describe how to widen the network with convolution (lines 65-73). As mentioned in text, \"P is akin to the number of stacked hidden layers\", and the model \"locally-connects\" along the P direction to share parameters. I think it is a strategy to deepen the network instead of widening it, since increasing P (the number of hidden layers) won't incur more parameters in the convolution. Similarly, as mentioned in lines 103-104, tRNN can be \"widened without additional parameters by increasing the tensor size P\". It does not make sense, as increasing P is conceptually equivalent to increasing the number of hidden layers in sRNN. This is to deepen the network, not to widen it.\n\n2. The authors claim to deepen the network with delayed outputs (Sec 2.2). They use the parameter L to set up the network depth. However, as shown in Eq. 9, L is determined by P and K, meaning that we can not really set up the network depth as a free parameter. I guess in practice, we would always pre-set P and K before experiments, and then derive L from Eq. 9. It seems over-claimed in lines 6-10, which reads like \"we could freely set up the width and depth of the network\".\n\n3. The authors claims that the proposed memory cell convolution is able to prevent gradient vanishing/exploding (line 36). This is not verified theoretically or empirically. The words \"gradient vanishing/exploding\" are even not mentioned in the later text.\n\n4. In the experiments, the authors compared tLSTM variants in the following dimentions: tensor shape (2D or 3D), normalization (no normalization, LN, CN), memory cell convolution (yes or no), and feedback connections (yes or no). There are 2x3x2x2=24 combinations in total. Why just pick up the six combinations in lines 166-171? I understand it become messy when comparing too many methods, but there are some interesting variants like 2D tLSTM+CN. Also, it might help to split the experiments in groups, like one for normalization strategy, one for memory cell convolution, one for feedback connections, etc.\n      "
        ]
    },
    {
        "title": "Concentration of Multilinear Functions of the Ising Model with Applications to Network Data",
        "decision": "Accept",
        "conference": "NIPS",
        "text": "Statistical Physics, the Ising model has found a myriad of applications in diverse research disciplines, including probability theory, Markov chain Monte Carlo, computer vision, theoretical computer science, social network analysis, game theory, computational biology, and neuroscience; see e.g.[LPW09, Cha05, Fel04, DMR11, GG86, Ell93, MS10] and their references. Despite the wealth of theoretical study and practical applications of this model, outlined above, there are still aspects of it that are poorly understood. Since the coordinates of X take values in {\u00b11}, we can without loss of generality focus our attention to multi-linear functions f . While the theory of concentration inequalities for functions of independent random variables has reached a high level of sophistication, proving concentration of measure for functions of dependent random variables is significantly harder, the main tools being martingale methods, logarithmic Sobolev inequalities and transportation cost inequalities. An alternative approach, proposed recently by Chatterjee [Cha05], is an adaptation to the Ising model of Stein\u2019s method of exchangeable pairs. 2High temperature is a widely studied regime of the Ising model where it enjoys a number of useful properties such as decay of correlations and fast mixing of the Glauber dynamics. Motivated by our applications in Section 5 we extend the above concentration of measure result to centered bilinear functions (where each variable Xi appears as Xi \u2212E[Xi] in the function) that also holds under arbitrary external fields; see Theorem 3. We leave extensions of this result to higher degree multinear functions to the next version of this paper. On the other hand, it is easy to construct low temperature Ising models where no non-trivial concentration holds.3 With our theoretical understanding in hand, we proceed with an experimental evaluation of the efficacy of multilinear functions applied to hypothesis testing. Interestingly, when considering musical preferences on a social network, we find that the Ising model may be more or less appropriate depending on the genre of music. The present work improves upon this by proving concentration rather than bounding the variance, as well as considering general degrees d rather than just d = 2. In simultaneous work, Gheissari, Lubetzky, and Peres proved concentration bounds which are qualitatively similar to ours, though the techniques are somewhat different [GLP17]. This is not as technically involved as the result for general-degree multilinear functions, but exposes many of the main conceptual ideas. We note that \u03b7-high-temperature is not strictly needed for our results to hold \u2013 we only need Hamming contraction of the \u201cgreedy coupling\u201d (see Lemma 1). This condition implies rapid mixing of the Glauber dynamics (in O(n log n) steps) via path coupling (Theorem 15.1 of [LPW09]). For instance, Azuma\u2019s inequality gives useful tail bounds whenever one can bound the martingale increments (i.e., the differences between consecutive terms of the martingale sequence) of the underlying martingale in absolute value, without requiring any form of independence. The challenge with bilinear functions is that they are O(n)-Lipschitz \u2013 a naive application of the same approach gives a radius of concentration of O\u0303(n3/2), which albeit better than the trivial radius of O(n2) is not optimal. The harm if the process stops too early (at t < t\u2217) is that we will not be able to effectively decouple E [fa(Xt)|X0] from the choice of X0. GaK(t) are the set of configurations for which all such linear functions satisfy certain conditions, including bounded expectation and concentration around their mean. The proof will proceed by induction on the degree d. Due to the proof being more involved, for ease of exposition, we present the proof of Theorem 4 without explicit values for constants. Since the two runs (of the Glauber dynamics) are coupled greedily to maximize the probability of agreement and they start with a small Hamming distance from each other (\u2264 1), these hybrid terms behave very similar to the non-hybrid multilinear terms. Showing that their behavior is similar, however, requires some supplementary statements about them which are presented in the supplementary material. Such choices are naturally influenced by one\u2019s neighbors in the network \u2013 one may be more likely to buy an iPhone if he sees all his friends have one, corresponding to an Ising model with positive-weight edges4 In our synthetic data study, we will leave these choices as abstract, referring to them only as \u201cvalues,\u201d but in our Last.fm data study, these choices will be whether or not one listens to a particular artist. Given a single multivariate sample, we first run the maximum pseudo-likelihood estimator (MPLE) to obtain an estimate of the model\u2019s parameters. Selecting either a friend or a friend-of-a-friend is in line with the concept of strong triadic closure [EK10] from the social sciences, which suggests that two individuals with a mutual friend are likely to either already be friends (which the social network may not have knowledge of) or become friends in the future. Our results are displayed in Figure 1 The x-axis marks the value of parameter \u03c4 , and the y-axis indicates the fraction of repetitions in which we successfully rejected the null hypothesis. We find that our statistic is able to correctly reject the null at a much earlier point than the MPLE alone. The dataset also contains users\u2019 listening habits \u2013 for each user we have a list of their fifty favorite artists, whose tracks they have listened to the most times. Similar results held for other popular pop musicians, including Britney Spears, Christina Aguilera, Rihanna, and Katy Perry. Based on our investigation, our statistic seems to indicate that for the pop artists, the null fails to effectively model the distribution, while it performs much better for the rock artists.",
        "reviews": [
            "This paper discusses concentration of bilinear functions of Ising systems and shows concentration of optimal up to logarithms order. I have read thoroughly the appendix, and the main results appear to be correct -- and I find them interesting. My main concerns with the paper are:\n\n(i) The writing: the paper refers to the supplementary material too much to my liking. For example sections 3.2.x for x in [2,...,5], do not read well, since the details provided are insufficient or vague. Part of the applications section also refers to the supplement multiple times.\n\n(ii) Although the datasets used in the application are interesting I think the analysis does not relate well to the theoretical study. First, using MPLE may be unstable and inconsistent in such high-dimensional data (n = 1600) which would render the second step -- running an MCMC based on those parameters useless. Note that while [BM16] prove the consistency of the MPLE, that typically requires the dimension to be fixed, and much smaller than the sample size. Second, why the fact that the bilinear functions concentrate almost sub-exponentially justify their use as test statistics -- as one can use any type of test statistic with the same idea?",
            "Summary:\n \n  The paper considers concentration of bilinear functions of samples from an ferromagnetic Ising model under high temperature (satisyfing Dobrushin's condition which implies fast mixing). The authors show that for the case when there is no external field, that any bilinear function with bounded coefficients concentrate around a radius of O(n) around the mean where n is the number of nodes in the Ising Model. The paper also shows that for Ising models with external fields, bilinear functions of mean shifted variables again obey such concentration properties.\n\n   The key technical contribution is adapting the theory of exhangeable pairs from [Cha05] which has been applied for linear functions of the Ising model and apply it to bilinear functions. Naive applications give concentration radius of n^{1.5} which the authors improve upon. The results are also optimal due to the lower bound for the worst case.\n\nStrengths:\n\n a) The key strength is the proof technique - the authors show that the usual strategy of creating an exchangeable pair from a sample X from the Ising model and X' which is one step of glauber dynamics is not sufficient. So they restrict glauber dynamics to a desirable part of the configuration space and show that the same exchangeable trick can be done on this censored glauber dynamics. They also show that this glauber dynamics mixes fast using path coupling results and also by a coupling argument by chaining two glauber Markov Chains tightly. This tightly chained censored glauber dynamics really satisfies all conditions of the exchangeable pairs argument needed. The authors show this carefully by several Lemmas in the appendix. I quite enjoyed reading the paper.\n\nb) I have read the proofs in the appendix. Barring some typos that can be fixed, the proof is correct to the best of my knowledge and the technique is the major contribution of the paper.\n\nc) The authors apply this to test if samples from a specific distribution on social networks is from an Ising model in the high temperature regime. Both synthetic and real world experiments are provided.\n\nWeaknesses:\n\n  Quite minor weaknesses:\n    a) Page 1 , appendix : Section 2, point 2 Should it not be f(X)- E[f(X)] ??\n    b) Why is Lemma 1 stated ? It does not help much in understanding Thm 1 quoted. A few lines connecting them could help.\n    c) Definition 1, I think It should be intersection and not union ?? Because proof of lemma 5 has a union of bad events which should be the negation of intersection of good events we want in Defn 1. \n    d) Typo in the last equation in Claim 3. \n    e) Lemma 9 statement. Should it not be (1-C/n)^t ??\n    f) In proof of Lemma 10, authors want to say that due to fast mixing, total variation distance is bounded by some expression. I think its a bit unclear what the parameter eta is ?? It appears for the first time. It will be great if the authors can explain that step better .\n    g) Regarding the authors experiments with Last.fm - Whats the practical use case of testing if samples came from an Ising model or not ? Actually the theoretical parts of this paper are quite strong. But I just wanted to hear author's thoughts.\n    h) Even for linear functions of uniform distribution on the boolean hypercube, at a different radius there are some anti concentration theorems like Littlewood Offord. Recently they have been generalized to bilinear forms (by Van Vu , terry tao with other co authors). I am wondering if something could be said about anti concentration theorems for Bilinear forms on Ising Models ?? "
        ]
    },
    {
        "title": "Deep Subspace Clustering Networks",
        "decision": "Accept",
        "conference": "NIPS",
        "text": "Subspace clustering has become an important problem as it has found various applications in computer vision, e.g., image segmentation [50, 27], motion segmentation [17, 9], and image clustering [14, 10]. To the best of our knowledge, our approach constitutes the first attempt to directly learn the affinities (through combination coefficients) between all data points within one neural network. Furthermore, we propose effective pre-training and fine-tuning strategies to learn the parameters of our DSC-Nets in an unsupervised manner and with a limited amount of data. To handle situations where data points do not exactly reside in a union of linear subspaces, but rather in non-linear ones, a few works [34, 35, 51, 47] have proposed to replace the inner product of the data matrix with a pre-defined kernel matrix (e.g., polynomial kernel and Gaussian RBF kernel). For instance, deep AEs have proven useful for dimensionality reduction [13] and image denoising [45]. A convolutional version of deep AEs was also applied to extract hierarchical features and to initialize convolutional neural networks (CNNs) [28]. To the best of our knowledge, the only exception is [36], which first extracts SIFT [25] or HOG [8] features from the images and feeds them to a fully connected deep auto-encoder with a sparse subspace clustering (SSC) [10] prior. It has been shown in [15] that, under the assumption that the subspaces are independent, by minimizing certain norms of C, C is guaranteed to have a block-diagonal structure (up to certain permutations), i.e., cij 6= 0 iff point xi and point xj lie in the same subspace. Mathematically, this idea is formalized as the optimization problem min C \u2016C\u2016p s.t. Various norms for C have been proposed in the literature, e.g., the `1 norm in Sparse Subspace Clustering (SSC) [9, 10], the nuclear norm in Low Rank Representation (LRR) [24, 23] and Low Rank Subspace Clustering (LRSC) [11, 43], and the Frobenius norm in Least-Squares Regression (LSR) [26] and Efficient Dense Subspace Clustering (EDSC) [15]. To encode self-expressiveness, we introduce a new loss function defined as L(\u0398,C) = 1 2 \u2016X\u2212 X\u0302\u0398\u20162F + \u03bb1\u2016C\u2016p + \u03bb2 2 \u2016Z\u0398e \u2212 Z\u0398eC\u20162F s.t. To minimize (3), we propose to leverage the fact that, as discussed below, C can be thought of as the parameters of an additional network layer, which lets us solve for \u0398 and C jointly using backpropagation.1 1Note that one could also alternate minimization between \u0398 and C. However, since the loss is non-convex, this would not provide better convergence guarantees and would require investigating the influence of the number of steps in the optimization w.r.t. Moreover, minimizing \u2016C\u2016p simply translates to adding a regularizer to the weights of the self-expressive layer. Since we always use the same batch in each training epoch, our optimization strategy is rather a deterministic momentum based gradient method than a stochastic gradient method. Although such an affinity matrix could in principle be computed as |C|+ |CT |, over the years, researchers in the field have developed many heuristics to improve the resulting matrix. For all the baselines, we used the source codes released by the authors and tuned the parameters by grid search to the achieve best results on each dataset. Note that this comparison already clearly shows the benefits of our approach. Following the experimental setup of [10], we down-sampled the original face images from 192 \u00d7 168 to 42 \u00d7 42 pixels, which makes it computationally feasible for the baselines [10, 23]. To get a manageable size of experiments, we first number the subjects from 1 to 38 and then take all possible K consecutive subjects. For the experiments with K subjects, we report the mean and median errors of 39 \u2212 K experimental trials. We also observe that using the pre-trained auto-encoder features does not necessarily improve the performance of SSC and EDSC, which confirms the benefits of our joint optimization of all parameters in one network. We further notice that DSC-Net-L1 performs slightly worse than DSC-Net-L2 in the current experimental settings. Compared to Extended Yale B, this dataset is more challenging for subspace clustering because (i) the face subspaces have more non-linearity due to varying facial expressions and details; (ii) the dataset size is much smaller (400 vs. 2432). We found this to be numerically more stable and converge faster than stochastic gradient descent using randomly sampled mini-batches. Figure 5(a) shows the error rates of the different methods, where different colors denote different subspace clustering algorithms and the length of the bars reflects the error rate. Since there are much fewer samples per subject, all competing methods perform worse than on Extended Yale B. In contrast with the previous human face datasets, in which faces are well aligned and have similar structures, the object images from COIL20 and COIL100 are more diverse, and even samples from the same object differ from each other due to the change of viewing angle. Note that with these network architectures, the dimension of the latent space representation zi increases by a factor of 15/4 for COIL20 (as the spatial resolution of each channel shrinks to 1/4 of the input image after convolutions with stride 2, and we have 15 channels) and 50/4 for COIL100. For example, if we want the latent space dimension to increase by a factor of 15/4, we need 15 \u00b7 4 channels in the second layer for a 2-layer encoder, 15 \u00b7 42 channels in the third layer for a 3-layer encoder, and so forth.",
        "reviews": [
            "\nThe paper propose to enforce self-expressiveness within the latent space of an auto-encoder, so as to make the latent representation better suited to spectral clustering.\n\nThe underlying intuition, and the experimental validation sound convincing (comparison with key relevant alternative methods is provided). Paper is also clear and well writen. I do encourage its acceptance. \n\nSome aspects however deserve a deeper discussion (even if they point out relative weaknesses of the approach). These include the practical use of the algorithm: how does the algorithm deal with large datasets (the number of parameters in the model increases with the number N of samples)? how can you assign a new sample to one of the cluster (without running the algorithm on the whole dataset)? what is the sensitivity of the approach to \\lambda_2? what does motivate the definition of lambda_2 on page 6 ?\n\n",
            "The paper addresses the problem of subspace clustering, i.e., separating a collection of data points lying in a union of subspaces according to underlying subspaces, using deep neural networks. To do so, the paper builds on the sparse subspace clustering idea: among all possible representations of a data point as a combination of other points in the dataset, the representation that uses the minimum number of points, corresponds to points from the same subspace. In other words, SSC uses the idea that for a data matrix $X$, a sparse solution of $X = X C$ (subject to $diag(C) = 0$) represents each point as a combination of a few other points from the same subspace. The paper proposes a deep neural network to transform the data into a new representation $Z = f_W(X)$ for which one searches for a sparse representation of $Z = Z C$, with the hope to learn more effective representations of data for clustering. To achieve this, the paper uses an auto-encoder scheme, where the middle hidden layer outputs are used as $Z$. To enforce $Z - Z C = 0$, the paper repeats the middle layer, with the hope that these two consecutive layers obtain same activations. If so, the weights  between the two layers would correspond to coefficient matrix $C$ which will be used then, similar to existing methods, to build an affinity matrix on data and to obtain clustering. The paper demonstrates experiments on Extended YaleB  and ORL face datasets as well as COIL20/100.\n\nThe reviewer believes that the paper is generally well-written and easy to read. The paper also takes a sufficiently interesting approach to connect subspace clustering with deep learning. On the other hand, there are several issues with the technical approach and the experiments that limit the novelty and correctness of the paper.\n\n1) The reviewer believes that the idea of the paper is similar to the one in [36], which uses an auto-encoder scheme, taking the middle layer as $Z$. While [36] fixes the $C$ and solves for $Z$, the proposed method repeats the middle layer to also solve for $C$. Despite this similarity, these is only a sentence in the paper (line 82) referring to this work and the experiments do not compare against [36]. Thus there is a need to clearly discuss advantages and disadvantages with respect to [36] and compare against it in the experiments.\n\n2) It is not clear to the reviewer how the paper enforces that the activations of the middle and the middle plus one layer must be the same. While, clearly one can initialize $C$ so that the consecutive activations will be the same, one the back propagation is done and the weights of the auto-encoder change, the activations of middle layer and the one after it will not be the same (unless the $\\lambda_2$ is very large and the network achieves it global minima, which is generally not the case). As a result, in the next iteration, the network will minimize $Z - Y C$ for different $Y$ and $Z$, which is not desired. \n\n3) One of the drawbacks of the proposed approach is the fact that the number of weights between the middle layers of the network is $N^2$, given $N$ data points. Thus, it seems that the method will not be scalable to very large datasets.\n\n4) In the experiments (line 243), the paper discusses training using mini-batches. Notice that while using mini-batches makes sense in the context of recognition, where each sample can be applied separately to the network, using mini-batches does not make sense in the context of clustering discussed in the paper. Using a mini-batch of size $M < N$, how does the paper train a neural network between two layers, each with $N$ nodes? In other words, it is not clear how the mini-batch instances and in what order will be input the the network.",
            "This paper proposes a new subspace clustering (SC) method based on neural networks. Specifically, the authors constructed the network by adding a self-expressive layer to the latent space of the traditional auto-encoder (AE) network, and used the coefficients of the self-expression to compute the affinity matrix for the final clustering.\n\nThe idea of doing SC in the latent space of AE is reasonable since the features may be more powerful for clustering task than the original data. Besides, the designing of the self-expressive layer is able to further exploit the structure of the data in latent space. The experiments on three image datasets demonstrated the effectiveness of the proposed method.\n\nMy concerns are as follows:\n\nFirst, as indicated by the authors, [36] also used AE type networks for SC, and thus it is better to discuss the relations and differences of network structures between the two in details. It would also be more convincing to directly compare the two methods in experiments, instead of saying the code \"is publicly available\" and only including partial results reported in [36].\n\nSecond, since the hyperparameters are generally very important for deep learning, it would be better to discuss how the hyperparameters, e.g., the number of layers, influence the final performance.\n\nThird, it would also be useful to include the computation time for readers who are willing to follow this work."
        ]
    },
    {
        "title": "Attentional Pooling for Action Recognition",
        "decision": "Accept",
        "conference": "NIPS",
        "text": "Interestingly, even video based action recognition has benefited greatly from advancements in imagebased CNN models [20, 22, 43, 46]. Our work: In this work, we propose a simple yet surprisingly powerful network modification that learns attention maps which focus computation on specific parts of the input relevant to the task at hand. Our contributions: (1) An easy-to-use extension of state-of-the-art base architectures that incorporates attention to give significant improvement in action recognition performance at virtually negligible increase in computation cost; (2) Extensive analysis of its performance on three action recognition datasets across still images and videos, obtaining state of the art on MPII and HMDB-51 (RGB, single-frame models) and competitive on HICO; (3) Analysis of different base architectures for applicability of our attention module; and (4) Mathematical analysis of our proposed attention module and showing its equivalence to a rank-1 approximation of second order or bilinear pooling (typically used in fine grained recognition methods [16, 26, 28]) suggesting a novel characterization of action recognition as a fine grained recognition problem. In contrast, collecting such diverse video based action datasets is hard, and hence existing popular benchmarks like UCF101 [45] or HMDB51 [27] contain only 101 and 51 categories each. In terms of performance, 3D conv based methods have been harder to scale and multi-stream methods [54] currently hold state of the art performance on standard benchmarks. More recent work [60] adds pose as an additional stream in chained multi-stream fashion and shows significant improvements. [18] modified R-CNN pipeline to propose R*CNN, where they choose an auxiliary box to encode context apart from the human bounding box. Mallya and Lazebnik [30] improve upon it by using the full image as the context and using multiple instance learning (MIL) to reason over all humans present in the image to predict an action label for the image. Our approach gets rid of the bounding box detection step and improves over both these methods by automatically learning to attend to the most informative parts of the image for the task. Our work is most related to [26], who point out when efficiently implemented, low-rank approximations avoid explicitly computing second-order features. Exposing this connection allows us to explore several extensions, including variations of bottom-up and top-down attention, as well as regularized attention maps that make use of additional supervised pose labels. Standard sum (or max) pooling would reduce this to vector in Rf\u00d71, which could then be processed by a \u201cfully-connected\u201d weight vector w \u2208 Rf\u00d71 to generate a classification score. For the moment, assume we are training a binary classifier (we generalize to more classes later in the derivation). Top-down attention: To generate prediction for multiple classes, we replace the weight matrix from (2) with class-specific weights: scoreorder2(X, k) = Tr(X TXWTk ), where X \u2208 Rn\u00d7f ,Wk \u2208 Rf\u00d7f (9) One could apply a similar derivation to produce class-specific vectors ak and bk, each of them generating a class-specific attention map. This suggests that our attentional model can also be implemented using a single, combined attention map defined over all n spatial locations: scoreattention(X, k) = 1 T ck, where ck = tk \u25e6 h, (11) where \u25e6 denotes element-wise multiplication and 1 is defined as before. While similar observations have been pointed out before [59], it naturally emerges as a special case of our bottom-up and top-down formulation of attention. A tte nt io n (b) We explore two architectures in our work, explained in Sec. We use the val set to compare with [18] and for ablative analysis while the final test results are obtained by emailing our results to authors of [34]. Baselines: Throughout the following sections, we compare our approach first to the standard base architecture, mostly ResNet-101 [20], without the attention-weighted pooling. It is also worth noting that the full image-only performance of VGG and ResNet were comparable in our experiments (29.4% and 30.2%), suggesting that our approach shows larger relative improvement over a similar starting baseline. To keep the final output comparable to our attentional-pooled model, we project to f = 2048 dimensions. We find it performs slightly worse than simple average pooling in Table 2. Note that we use an existing implementation [1] with minimal hyper-parameter optimization, and leave a more rigorous comparison to future work. Essentially, a rank-P approximation generates P (1-channel) bottom-up and (C channel) top-down attention maps, and the final prediction is the product of corresponding heatmaps, summed over P . On MPII, we obtain mAP of 30.3, 29.9, 30.0 for P=1, 2 and 5 respectively, showing that the validation performance is relatively stable with P . We do observe a drop in training loss with a higher P , indicating that a higher-rank approximation could be useful for harder datasets and tasks. On further analysis we observe that both models achieve near perfect mAP on training data, implying that adding more parameters with multiple attention maps leads to over-fitting on the relatively small MPII trainset. This points to an important distinction in the two architectures, i.e., Inception-style models are designed to be faster in inference and training by rapidly down sampling input images in initial layers through max-pooling. While this reduces the computational cost for later layers, it leads to most layers having very large receptive fields, and hence later neurons have effective access to all of the image pixels. The two communities are traditionally seen as distinct, and our work strongly suggests that they should mix: as newer action datasets become more fine-grained, we should explore second-order pooling techniques for action recognition. Similarly, second-order pooling can serve as a simple but strong baseline for the attention community, which tends to focus on more complex sequential attention networks (based on RNNs or LSTMs).",
        "reviews": [
            "This paper presents an attention-based model for action recognition and human object interaction. The model can be guided by extra supervision or not. It achieves accuracy improvements without increasing the network size and computational cost a lot. Authors provide an extensively empirical and analytical analysis of the attention module, and they further introduce a derivation of bottom-up and top-down attention as low-rank approximations of bilinear pooling methods for fine-grained classification typically. Experiments on three benchmarks have demonstrated the efficacy of the proposed method. \n\nOne key advantage is to learn the attention map in an unsupervised manner. To make a prediction, the attention maps can provide insight into where the network should look in terms of both bottom-up saliency and top-down attention. This allows it to get rid of detecting the bounding box usually required in hard attention.\n\nThis is an interesting work, and the idea sounds well motivated. The paper reads well too. One major concern is: \n\nThe fully-connected weight matrix in second-order pooling is approximated by the product of two vectors with rank-1. Then, how about the information loss compared the original one? Does such loss affect the attention map? Some theoretical analysis and more discussions are expected.\n\nMinor issues: \nPage 3, \"network architecture that incorporate(s) this attention module and explore(s) a pose ...\"\nPage 5, \"It consist(s) of a stack of modules\"\nPage 8, \"... image resized to 450px at(as) input time did\"?\n",
            "The paper introduces a simple yet efficient way of incorporating attention modelling in CNN-based action recognition networks.\nThe paper is clearly written, the contribution is interesting and the experimental validation is convincing.\n\nMinors comments:\nSection 2 speaks of \"performance\" without explicitly stating which evaluation metrics are considered, except for [36] where it is said that mean average precision is considered (btw the acronym mAP is not properly defined here), this should be fixed to be clearer.\nA few typos were left that need further proofreading.",
            "The authors propose to use a low rank approximation of second order pooling features for action recognition. They show results on three compelling vision benchmarks for action recognition and show improvements over the baseline. \n\nPros:\n(+) The proposed method is simple and can be applied on top of any network architecture\n(+) Clear improvement over baseline \n\nCons:\n(-) Unclear novelty over well known second order pooling approaches\n(-) Fair comparison with other attention based models is missing\n\n\nIt is unclear to me how the proposed approach is novel compared to other low rank second order pooling methods, typically used for fine grained recognition (e.g. Lin et al, ICCV2015), semantic segmentation etc. While the authors do a decent job walking us through the linear algebra and interpreting the variables used, the final approach is merely a classifier on second order features.\n\nIn addition, the authors make design choices that are not clearly justified. For multi class recognition, the authors do not justify the choice for a class agnoistic b but for class-specific a. These experiments should be added in order to prove the effectiveness of the proposed design. \n\nThe authors claim that they get rid of the box detection step present at other approaches such as R*CNN or Mallya & Lazebnik. However, they do not discuss how their approach would generalize to instance-specific action recognition. An image can contain many people who perform different actions. In this case, the action labels are not assigned at the frame level, but at an instance level. R*CNN was designed for instance-level recognition, thus the necessity of the person box detection step. The authors should discuss how the proposed approach can handle this more generic setup of instance-level labels? This is a limitation of the current approach, rather than a benefit as implied in the Related Work section.\n\nThe authors fail to provide a fair comparison with competing approaches. The authors use a ResNet101 or an Inception-V2 architecture for their base network. They show improvement over their own non-attentive baseline, using the same architecture. However, the competing approaches, such as R*CNN or Mallya & Lazebnik all use a VGG16 network. This makes comparison with these approaches and the current approach unfair and inconclusive on MPII and HICO dataset. On HMDB, the improvement over the TSN BN-inception method is small while comparisons with other methods is unclear due to the varying underlying base architectures. \n"
        ]
    }
]