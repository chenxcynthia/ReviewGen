{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df2e245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cynthiachen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/cynthiachen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import traceback\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "sys.setrecursionlimit(1000000)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stemming = PorterStemmer()\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "# first read keywords table\n",
    "def read_keywords(keywords_file):\n",
    "    keywords = []\n",
    "    with open(keywords_file, 'r', encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            keywords += line.split(\" \")\n",
    "    return keywords\n",
    "\n",
    "\n",
    "# then read parameters table\n",
    "def read_parameters(parameters_file):\n",
    "    parameters = []\n",
    "    with open(parameters_file, 'r', encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            _, init_p, init_n = line.split(' ')\n",
    "            parameters.append((float(init_p), int(init_n)))\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def apply_cleaning_function_to_list(X):\n",
    "    cleaned_X = []\n",
    "    for element in X:\n",
    "        cleaned_X.append(clean_text(element))\n",
    "    return cleaned_X\n",
    "\n",
    "\n",
    "def clean_text(raw_text):\n",
    "    \"\"\"This function works on a raw text string, and:\n",
    "        1) changes to lower case\n",
    "        2) tokenizes (breaks down into words\n",
    "        3) removes punctuation and non-word text\n",
    "        4) finds word stems\n",
    "        5) removes stop words\n",
    "        6) rejoins meaningful stem words\"\"\"\n",
    "\n",
    "    # Convert to lower case\n",
    "    text = raw_text.lower()\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Keep only words (removes punctuation + numbers)\n",
    "    # use .isalnum to keep also numbers\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    # Stemming\n",
    "    stemmed_words = [stemming.stem(w) for w in token_words]\n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in stemmed_words if not w in stops]\n",
    "    # Rejoin meaningful stemmed words\n",
    "    joined_words = (\" \".join(meaningful_words))\n",
    "    # Return cleaned data\n",
    "    return joined_words\n",
    "\n",
    "\n",
    "def get_full_text(paper_json):\n",
    "    full_text = \"\"\n",
    "    with open(paper_json, 'r', encoding='utf8') as f:\n",
    "        content_dict = json.loads(f.read())\n",
    "        sections = content_dict.get('metadata').get('sections')\n",
    "        for section in sections:\n",
    "            heading: str = section.get('heading')\n",
    "            text: str = section.get('text')\n",
    "            if heading is not None:\n",
    "                if heading.upper().__contains__('ACKNOW') or heading.upper().__contains__('APPEN'):\n",
    "                    break\n",
    "            if text is not None and len(text) > 0:\n",
    "                full_text += text + \" \"\n",
    "    full_text = full_text.replace(\"\\n\", \" \").encode(\"utf-8\", \"ignore\").decode(\"utf-8\").strip()\n",
    "    return full_text\n",
    "\n",
    "\n",
    "# look how the filtering works\n",
    "def get_sents(text: str) -> (List, List):\n",
    "    \"\"\" give a text string, return the sentence list \"\"\"\n",
    "    # Here are some heuristics that we use to get appropriate sentence splitter.\n",
    "    # 1. Delete sentences that are fewer than 25 characters.\n",
    "    # 2. If a sentence ends in et al. Then concate with the sentence behind it.\n",
    "    sent_list: List[str] = nltk.tokenize.sent_tokenize(text)\n",
    "    new_sent_list = [sent.replace(\"\\n\", \"\") for sent in sent_list]\n",
    "    postprocessed = []\n",
    "    buff = \"\"\n",
    "    for sent in new_sent_list:\n",
    "        if sent.endswith('et al.') or sent.endswith('Eq.') \\\n",
    "                or sent.endswith('i.e.') or sent.endswith('e.g.'):\n",
    "            buff += sent\n",
    "        else:\n",
    "            if len(buff + sent) > 25 and \\\n",
    "                    not (buff + sent).__contains__('arxiv') and \\\n",
    "                    not (buff + sent).__contains__('http'):\n",
    "                postprocessed.append(buff + sent)\n",
    "            buff = \"\"\n",
    "    if len(buff) > 0:\n",
    "        postprocessed.append(buff)\n",
    "    cleaned_sent_list = apply_cleaning_function_to_list(postprocessed[:250])\n",
    "    return postprocessed[:250], cleaned_sent_list\n",
    "\n",
    "\n",
    "def keywords_filtering(text: str, keywords: List[str]) -> (List[str], List[str]):\n",
    "    sents, cleaned_sents = get_sents(text)\n",
    "    filtered_sents = []\n",
    "    cleaned_filtered_sents = []\n",
    "    for sent, clean_sent in zip(sents, cleaned_sents):\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        for word in words:\n",
    "            if word in keywords:\n",
    "                filtered_sents.append(sent)\n",
    "                cleaned_filtered_sents.append(clean_sent)\n",
    "                break\n",
    "    return filtered_sents, cleaned_filtered_sents\n",
    "\n",
    "\n",
    "def score(sample: np.array, sent_list: List[str]) -> float:\n",
    "    final_text = get_text(sample, sent_list)\n",
    "    return get_score(final_text)\n",
    "\n",
    "\n",
    "def get_text(sample: np.array, sent_list: List[str]) -> str:\n",
    "    final_text = \"\"\n",
    "    for idx in range(0, len(sample)):\n",
    "        if sample[idx] == 1:\n",
    "            final_text += sent_list[idx] + \" \"\n",
    "    final_text = final_text.strip()\n",
    "    return final_text\n",
    "\n",
    "\n",
    "def get_score(text: str) -> float:\n",
    "    words = nltk.word_tokenize(text)\n",
    "    summ_len = len(words)\n",
    "    counter = Counter(words)\n",
    "    v = np.array(list(counter.values())) / summ_len\n",
    "    return float(np.matmul(-v, np.log2(v)))\n",
    "\n",
    "\n",
    "def isAllZeroOrOne(array):\n",
    "    \"\"\" Use to check convergence \"\"\"\n",
    "    for elem in array:\n",
    "        if elem != 1.0 and elem != 0.0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def CEmethod(sent_list: List[str], N=10000, init_p=0.5, rho=0.05, alpha=0.7, iter=100) -> np.array:\n",
    "    try:\n",
    "        p = np.array([init_p] * len(sent_list))\n",
    "        early_stop_step = 0\n",
    "        gamma_old = 0.0\n",
    "        for i in range(iter):\n",
    "            if i >= 1:\n",
    "                N = 1000\n",
    "            samples = [np.random.binomial(1, p=p) for j in range(N)]\n",
    "            scored_samples = [(sample, score(sample, sent_list)) for sample in samples if sample.sum() <= 30]\n",
    "\n",
    "            while len(scored_samples) == 0:\n",
    "                samples = [np.random.binomial(1, p=p) for j in range(N)]\n",
    "                scored_samples = [(sample, score(sample, sent_list)) for sample in samples if sample.sum() <= 30]\n",
    "\n",
    "            # np.quantile does not require a sorted input\n",
    "            gamma = np.quantile([x[1] for x in scored_samples], 1 - rho)\n",
    "\n",
    "            valid_samples = [sample[0] for sample in scored_samples if sample[1] >= gamma]\n",
    "\n",
    "            # Relax the gamma a little bit due to floating point precision issue\n",
    "            closeness = 0.0000000000001\n",
    "            while len(valid_samples) == 0:\n",
    "                valid_samples = [sample[0] for sample in scored_samples if sample[1] >= gamma - closeness]\n",
    "                closeness *= 10\n",
    "\n",
    "            new_p = sum(valid_samples) / len(valid_samples)\n",
    "\n",
    "            if gamma == gamma_old:\n",
    "                early_stop_step += 1\n",
    "\n",
    "            p = alpha * p + (1 - alpha) * new_p\n",
    "            gamma_old = gamma\n",
    "\n",
    "            if early_stop_step >= 3 or isAllZeroOrOne(p):\n",
    "                break\n",
    "        return p\n",
    "\n",
    "    except:\n",
    "        return np.array([0] * len(sent_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a80e685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "    def __init__(self, keywords_file, parameters_file):\n",
    "        self.keywords = read_keywords(keywords_file)\n",
    "        self.parameters = read_parameters(parameters_file)\n",
    "\n",
    "    def extract(self, text):\n",
    "        np.random.seed(666)\n",
    "        filtered_sents, cleaned_filtered_sents = keywords_filtering(text, self.keywords)\n",
    "        if len(filtered_sents) <= 30:\n",
    "            out_p = np.array([1] * len(filtered_sents))\n",
    "        else:\n",
    "            group = len(filtered_sents) // 10\n",
    "            init_p, init_n = self.parameters[group]\n",
    "            out_p = CEmethod(cleaned_filtered_sents, N=init_n, init_p=init_p)\n",
    "        samples = [np.random.binomial(1, p=out_p) for j in range(1)]\n",
    "        extracted = get_text(samples[0], filtered_sents)\n",
    "        return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7a82ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = Extractor('keywords.txt', 'parameters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdce533",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = []\n",
    "\n",
    "for i in range(1, 5):\n",
    "    paper_dict = {}\n",
    "    # check for content + review true\n",
    "    \n",
    "    kind = 'paper'\n",
    "    fname = '../../dataset/NIPS_2017/NIPS_2017_'+kind+'/NIPS_2017_'+str(i)+'_'+kind+'.json'\n",
    "    with open(fname, 'r', encoding='utf8') as f:\n",
    "        content_dict = json.loads(f.read())\n",
    "    if content_dict['hasContent'] != 'true' or content_dict['hasReview'] != 'true':\n",
    "        continue\n",
    "    \n",
    "    # fill in info\n",
    "    paper_dict['title'] = content_dict['title']\n",
    "    paper_dict['decision'] = content_dict['decision']\n",
    "    paper_dict['conference'] = content_dict['conference']\n",
    "    \n",
    "    kind = 'content'\n",
    "    fname = '../../dataset/NIPS_2017/NIPS_2017_'+kind+'/NIPS_2017_'+str(i)+'_'+kind+'.json'\n",
    "    fulltext = get_full_text(fname)\n",
    "    extraction = extractor.extract(fulltext)\n",
    "    \n",
    "    paper_dict['text'] = extraction\n",
    "    \n",
    "    kind = 'review'\n",
    "    fname = '../../dataset/NIPS_2017/NIPS_2017_'+kind+'/NIPS_2017_'+str(i)+'_'+kind+'.json'\n",
    "    with open(fname, 'r', encoding='utf8') as f:\n",
    "        content_dict = json.loads(f.read())\n",
    "    reviews = []\n",
    "    for r in content_dict['reviews']:\n",
    "        reviews.append(r['review'])\n",
    "        \n",
    "    paper_dict['reviews'] = reviews\n",
    "    \n",
    "    json_list.append(paper_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d423883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final JSON format\n",
    "# { \n",
    "#   \"title\": str\n",
    "#   \"decision\": str\n",
    "#   \"conference\": str\n",
    "#   \"text\": str\n",
    "#   \"reviews\": list of strs\n",
    "# }\n",
    "\n",
    "\n",
    "# Serializing json\n",
    "json_object = json.dumps(json_list, indent=4)\n",
    " \n",
    "# Writing to sample.json\n",
    "with open(\"paper_data.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research23",
   "language": "python",
   "name": "research23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
