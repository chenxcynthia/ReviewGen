{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4036e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T03:00:46.218578Z",
     "start_time": "2023-03-09T03:00:44.812174Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4b72987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T03:00:49.005620Z",
     "start_time": "2023-03-09T03:00:48.892353Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\"~/Downloads/paper_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbcf50c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T03:01:20.977201Z",
     "start_time": "2023-03-09T03:01:20.974607Z"
    }
   },
   "outputs": [],
   "source": [
    "df['decision'] = ['Accept', 'Accept', 'Reject', 'Accept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be7790b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T03:05:26.857611Z",
     "start_time": "2023-03-09T03:05:26.846182Z"
    }
   },
   "outputs": [],
   "source": [
    "def expand_reviews(df):\n",
    "    entries = []\n",
    "    for i, row in df.iterrows():\n",
    "        reviews = row['reviews']\n",
    "        n = len(reviews)\n",
    "        for review in row['reviews']:\n",
    "            entries.append({'title': row['title'], 'decision': row['decision'], \n",
    "                            'conference': row['conference'], 'review': review, 'text': row['text']})\n",
    "    return pd.DataFrame(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39bb64cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T03:05:42.651155Z",
     "start_time": "2023-03-09T03:05:42.647783Z"
    }
   },
   "outputs": [],
   "source": [
    "df = expand_reviews(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d60c028",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T03:08:03.057490Z",
     "start_time": "2023-03-09T03:08:03.053894Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "387017b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T03:11:01.956287Z",
     "start_time": "2023-03-09T03:11:01.954068Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa744bd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T07:51:16.177057Z",
     "start_time": "2023-03-09T07:51:16.103233Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_json('/Users/erichansen/expanded_papers.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be25fc5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T07:49:34.829256Z",
     "start_time": "2023-03-09T07:49:34.762258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>decision</th>\n",
       "      <th>conference</th>\n",
       "      <th>review</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wider and Deeper, Cheaper and Faster: Tensoriz...</td>\n",
       "      <td>Accept</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>The paper explores a new way to share paramete...</td>\n",
       "      <td>We consider the time-series prediction task of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wider and Deeper, Cheaper and Faster: Tensoriz...</td>\n",
       "      <td>Accept</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>The paper introduces a tensorized version of L...</td>\n",
       "      <td>We consider the time-series prediction task of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wider and Deeper, Cheaper and Faster: Tensoriz...</td>\n",
       "      <td>Accept</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>\\nThis paper proposes Tensorized LSTMs for eff...</td>\n",
       "      <td>We consider the time-series prediction task of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Concentration of Multilinear Functions of the ...</td>\n",
       "      <td>Accept</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>This paper discusses concentration of bilinear...</td>\n",
       "      <td>Statistical Physics, the Ising model has found...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Concentration of Multilinear Functions of the ...</td>\n",
       "      <td>Accept</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>Summary:\\n \\n  The paper considers concentrati...</td>\n",
       "      <td>Statistical Physics, the Ising model has found...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>Deep Subspace Clustering Networks</td>\n",
       "      <td>Reject</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>The paper addresses the problem of subspace cl...</td>\n",
       "      <td>Subspace clustering has become an important pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>Deep Subspace Clustering Networks</td>\n",
       "      <td>Reject</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>This paper proposes a new subspace clustering ...</td>\n",
       "      <td>Subspace clustering has become an important pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>Attentional Pooling for Action Recognition</td>\n",
       "      <td>Accept</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>This paper presents an attention-based model f...</td>\n",
       "      <td>Interestingly, even video based action recogni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>Attentional Pooling for Action Recognition</td>\n",
       "      <td>Accept</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>The paper introduces a simple yet efficient wa...</td>\n",
       "      <td>Interestingly, even video based action recogni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>Attentional Pooling for Action Recognition</td>\n",
       "      <td>Accept</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>The authors propose to use a low rank approxim...</td>\n",
       "      <td>Interestingly, even video based action recogni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title decision conference  \\\n",
       "0     Wider and Deeper, Cheaper and Faster: Tensoriz...   Accept       NIPS   \n",
       "1     Wider and Deeper, Cheaper and Faster: Tensoriz...   Accept       NIPS   \n",
       "2     Wider and Deeper, Cheaper and Faster: Tensoriz...   Accept       NIPS   \n",
       "3     Concentration of Multilinear Functions of the ...   Accept       NIPS   \n",
       "4     Concentration of Multilinear Functions of the ...   Accept       NIPS   \n",
       "...                                                 ...      ...        ...   \n",
       "1095                  Deep Subspace Clustering Networks   Reject       NIPS   \n",
       "1096                  Deep Subspace Clustering Networks   Reject       NIPS   \n",
       "1097         Attentional Pooling for Action Recognition   Accept       NIPS   \n",
       "1098         Attentional Pooling for Action Recognition   Accept       NIPS   \n",
       "1099         Attentional Pooling for Action Recognition   Accept       NIPS   \n",
       "\n",
       "                                                 review  \\\n",
       "0     The paper explores a new way to share paramete...   \n",
       "1     The paper introduces a tensorized version of L...   \n",
       "2     \\nThis paper proposes Tensorized LSTMs for eff...   \n",
       "3     This paper discusses concentration of bilinear...   \n",
       "4     Summary:\\n \\n  The paper considers concentrati...   \n",
       "...                                                 ...   \n",
       "1095  The paper addresses the problem of subspace cl...   \n",
       "1096  This paper proposes a new subspace clustering ...   \n",
       "1097  This paper presents an attention-based model f...   \n",
       "1098  The paper introduces a simple yet efficient wa...   \n",
       "1099  The authors propose to use a low rank approxim...   \n",
       "\n",
       "                                                   text  \n",
       "0     We consider the time-series prediction task of...  \n",
       "1     We consider the time-series prediction task of...  \n",
       "2     We consider the time-series prediction task of...  \n",
       "3     Statistical Physics, the Ising model has found...  \n",
       "4     Statistical Physics, the Ising model has found...  \n",
       "...                                                 ...  \n",
       "1095  Subspace clustering has become an important pr...  \n",
       "1096  Subspace clustering has become an important pr...  \n",
       "1097  Interestingly, even video based action recogni...  \n",
       "1098  Interestingly, even video based action recogni...  \n",
       "1099  Interestingly, even video based action recogni...  \n",
       "\n",
       "[1100 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9f57eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/cynthiachen/opt/anaconda3/lib/python3.9/site-packages (23.0)\n",
      "Collecting pip\n",
      "  Downloading pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.0\n",
      "    Uninstalling pip-23.0:\n",
      "      Successfully uninstalled pip-23.0\n",
      "Successfully installed pip-23.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307aa482",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T05:26:27.255471Z",
     "start_time": "2023-03-08T05:26:09.626412Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install datasets | grep -v 'already satisfied'\n",
    "! pip install sentencepiece | grep -v 'already satisfied'\n",
    "! pip install rouge_score | grep -v 'already satisfied'\n",
    "! pip install wandb | grep -v 'already satisfied'\n",
    "! pip install bert-score | grep -v 'already satisfied'\n",
    "! pip install evaluate | grep -v 'already satisfied'\n",
    "! pip install transformers -U  | grep -v 'already satisfied'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03d544aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T04:22:41.503461Z",
     "start_time": "2023-03-08T04:22:41.500210Z"
    }
   },
   "outputs": [],
   "source": [
    "sublibraries = [f'ICLR{year}' for year in range(2017, 2021)] + [f'NIPS{year}' for year in range(2017, 2021)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c7f831",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34174567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a02a70f",
   "metadata": {},
   "source": [
    "# Dataset Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d9b322",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92e7d163",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-08T06:33:25.681Z"
    }
   },
   "outputs": [],
   "source": [
    "workdir = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e062c46a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T05:54:54.444745Z",
     "start_time": "2023-03-08T05:54:54.165369Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d17c992324baeca4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /Users/cynthiachen/.cache/huggingface/datasets/json/default-d17c992324baeca4/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202d1c9df7c5482abebf9fc998f4ec33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbb1259e3d142dbbf6b1240a055f655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/cynthiachen/.cache/huggingface/datasets/json/default-d17c992324baeca4/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78da4ff2b1184e6cba7742c143476ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, DatasetDict, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_json(workdir + \"paper_data.json\")\n",
    "\n",
    "# train_df, test_df = train_test_split(df, test_size=0.25, random_state=100)\n",
    "\n",
    "# paper_reviews_dataset = DatasetDict({\n",
    "#     \"train\": train_df,\n",
    "#     \"test\": test_df\n",
    "# })\n",
    "\n",
    "# paper_reviews_dataset = load_dataset(\"json\", data_files=\"/Users/erichansen/Downloads/paper_data.json\")\n",
    "paper_reviews_dataset = load_dataset(\"json\", data_files='paper_data.json')\n",
    "paper_reviews_dataset = paper_reviews_dataset['train'].train_test_split(test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d94fb72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T05:54:57.323983Z",
     "start_time": "2023-03-08T05:54:57.284115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>decision</th>\n",
       "      <th>conference</th>\n",
       "      <th>text</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wider and Deeper, Cheaper and Faster: Tensoriz...</td>\n",
       "      <td>Accept</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>We consider the time-series prediction task of...</td>\n",
       "      <td>[The paper explores a new way to share paramet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Concentration of Multilinear Functions of the ...</td>\n",
       "      <td>Accept</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>Statistical Physics, the Ising model has found...</td>\n",
       "      <td>[This paper discusses concentration of bilinea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deep Subspace Clustering Networks</td>\n",
       "      <td>Accept</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>Subspace clustering has become an important pr...</td>\n",
       "      <td>[\\nThe paper propose to enforce self-expressiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Attentional Pooling for Action Recognition</td>\n",
       "      <td>Accept</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>Interestingly, even video based action recogni...</td>\n",
       "      <td>[This paper presents an attention-based model ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title decision conference  \\\n",
       "0  Wider and Deeper, Cheaper and Faster: Tensoriz...   Accept       NIPS   \n",
       "1  Concentration of Multilinear Functions of the ...   Accept       NIPS   \n",
       "2                  Deep Subspace Clustering Networks   Accept       NIPS   \n",
       "3         Attentional Pooling for Action Recognition   Accept       NIPS   \n",
       "\n",
       "                                                text  \\\n",
       "0  We consider the time-series prediction task of...   \n",
       "1  Statistical Physics, the Ising model has found...   \n",
       "2  Subspace clustering has become an important pr...   \n",
       "3  Interestingly, even video based action recogni...   \n",
       "\n",
       "                                             reviews  \n",
       "0  [The paper explores a new way to share paramet...  \n",
       "1  [This paper discusses concentration of bilinea...  \n",
       "2  [\\nThe paper propose to enforce self-expressiv...  \n",
       "3  [This paper presents an attention-based model ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4cddda",
   "metadata": {},
   "source": [
    "## Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dc060eb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T05:59:55.391279Z",
     "start_time": "2023-03-08T05:59:55.386871Z"
    }
   },
   "outputs": [],
   "source": [
    "max_paper_extract_length = 512\n",
    "max_review_length = 512\n",
    "pre_trained_model_checkpoint = \"facebook/bart-large-cnn\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9c8395dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T05:59:56.288707Z",
     "start_time": "2023-03-08T05:59:55.780946Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large-cnn\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /Users/erichansen/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/vocab.json\n",
      "loading file merges.txt from cache at /Users/erichansen/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/merges.txt\n",
      "loading file tokenizer.json from cache at /Users/erichansen/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-large-cnn\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "67c1bdcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T05:59:56.653289Z",
     "start_time": "2023-03-08T05:59:56.648648Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(paper_json):\n",
    "    print('reached')\n",
    "    model_inputs = tokenizer(\n",
    "        paper_json[\"text\"],\n",
    "        max_length=max_paper_extract_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "    paper_json['review'] = paper_json['reviews'][0]\n",
    "    labels = tokenizer(\n",
    "        paper_json[\"review\"], max_length=max_review_length, truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    print(len(model_inputs['labels']))\n",
    "    print(len(paper_json['text']))\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac9efc1",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed669e0",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Bart Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3a9e3765",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:00:07.705335Z",
     "start_time": "2023-03-08T06:00:02.021902Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/config.json\n",
      "Model config BartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"force_bos_token_to_be_generated\": true,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50264\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/erichansen/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"min_length\": 56,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/3d224934c6541b2b9147e023c2f6f6fe49bd27e1/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"min_length\": 56,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "model = BartForConditionalGeneration.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f8a749",
   "metadata": {},
   "source": [
    "## Fine-Tune Bart Model using Tokenized, Extracted Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af2dd1",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5f9f4a31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:19:59.960370Z",
     "start_time": "2023-03-08T06:19:55.553971Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
      "loading file merges.txt from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b366afaf8748a6ba2eea74c517bfc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73bf6797ffd45d4b09a8e80f3306a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.15 seconds, 12.92 sentences/sec\n",
      "(tensor([0.9222, 0.8905]), tensor([0.9714, 0.8988]), tensor([0.9462, 0.8946]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.48571428571428577,\n",
       " 'rouge2': 0.2,\n",
       " 'rougeL': 0.48571428571428577,\n",
       " 'rougeLsum': 0.48571428571428577}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "from evaluate import load\n",
    "import bert_score\n",
    "\n",
    "# bertscore = load(\"bertscore\")\n",
    "# predictions = [\"hello there\", \"general kenobi\"]\n",
    "# references = [\"hello there\", \"general kenobi\"]\n",
    "\n",
    "predictions = [\"hello there young padawan\", \"general obi wan \"]\n",
    "references = [\"hello there youngster\", \"general kenobi\"]\n",
    "\n",
    "results = bert_score.score(predictions, references, lang='en', verbose=True)\n",
    "print(results)\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\")\n",
    "scores = rouge_score.compute(\n",
    "    predictions=predictions, references=references\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b004fef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:32:00.924364Z",
     "start_time": "2023-03-08T06:32:00.916493Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "import bert_score\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    rouge_results = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Compute BERT Score, rescaling with baseline as specified in the Ti\n",
    "    # https://github.com/Tiiiger/bert_score/blob/master/example/Demo.ipynb\n",
    "    # https://arxiv.org/pdf/1904.09675.pdf\n",
    "    bert_scores = bert_score.score(decoded_preds, decoded_labels, lang='en', \n",
    "                                   verbose=False, rescale_with_baseline=True)\n",
    "    bs_prec, bs_recall, bs_f1 = bert_scores[0].mean(), bert_scores[1].mean(), bert_scores[2].mean()\n",
    "\n",
    "#     bert_score_results = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang='en')\n",
    "    # Extract the median scores\n",
    "    results = {k: round(v, 4) for k, v in rouge_results.items()}\n",
    "    \n",
    "    # Only report F1 Score, similar to the Yuan et. al \n",
    "    results.update({'BertScore': bs_f1})\n",
    "#     result.update({})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b005e6",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "629fd8ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:00:08.311874Z",
     "start_time": "2023-03-08T06:00:08.292652Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "batch_size = 1\n",
    "num_train_epochs = 8\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_dataset[\"train\"]) // batch_size \n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-review-advisor\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "#     push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ade0e",
   "metadata": {},
   "source": [
    "### Data Collator\n",
    "Necessary for HuggingFace Batching of Feature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5cbfdc4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T06:00:10.827699Z",
     "start_time": "2023-03-08T06:00:10.824785Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5cc1e1",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a46a46",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-08T06:32:02.813Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: conference, decision, review, title, reviews, text. If conference, decision, review, title, reviews, text are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/Users/erichansen/.conda/envs/data-sci/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 24\n",
      "  Number of trainable parameters = 406290432\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/24 01:21 < 02:22, 0.10 it/s, Epoch 3/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Bertscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.367100</td>\n",
       "      <td>6.438890</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>0.044200</td>\n",
       "      <td>0.153800</td>\n",
       "      <td>0.274700</td>\n",
       "      <td>0.010323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.359800</td>\n",
       "      <td>6.768425</td>\n",
       "      <td>0.342400</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.157600</td>\n",
       "      <td>0.315200</td>\n",
       "      <td>0.049732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: conference, decision, review, title, reviews, text. If conference, decision, review, title, reviews, text are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"min_length\": 56,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
      "loading file merges.txt from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: conference, decision, review, title, reviews, text. If conference, decision, review, title, reviews, text are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"min_length\": 56,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/vocab.json\n",
      "loading file merges.txt from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/erichansen/.cache/huggingface/hub/models--roberta-large/snapshots/5069d8a2a32a7df4c69ef9b56348be04152a2341/pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: conference, decision, review, title, reviews, text. If conference, decision, review, title, reviews, text are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"min_length\": 56,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7553a",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab00c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33564052",
   "metadata": {},
   "source": [
    "# Reproduce Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da4849",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
